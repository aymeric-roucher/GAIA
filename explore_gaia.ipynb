{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/aymeric/.cache/huggingface/modules/datasets_modules/datasets/gaia-benchmark--GAIA/ec492fe4320ee795b1aed6bb46229c5f693226b0f1316347501c24b4baeee005 (last modified on Tue May 28 10:04:32 2024) since it couldn't be found locally at gaia-benchmark/GAIA, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "eval_ds = datasets.load_dataset(\"gaia-benchmark/GAIA\", \"2023_all\")[\"validation\"]\n",
    "eval_ds = eval_ds.rename_columns(\n",
    "    {\"Question\": \"question\", \"Final answer\": \"true_answer\", \"Level\": \"task\"}\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_file_paths(row):\n",
    "    if len(row[\"file_name\"]) > 0:\n",
    "        row[\"file_name\"] = \"data/gaia/validation/\" + row[\"file_name\"]\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "eval_ds = eval_ds.map(preprocess_file_paths)\n",
    "\n",
    "eval_df = pd.DataFrame(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task\n",
       "2    86\n",
       "1    53\n",
       "3    26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[\"task\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"extension\"] = eval_df[\"file_name\"].apply(lambda x: x.split(\".\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extension\n",
       "          127\n",
       "xlsx       13\n",
       "png         8\n",
       "mp3         3\n",
       "pdf         3\n",
       "zip         2\n",
       "jpg         2\n",
       "pdb         1\n",
       "jsonld      1\n",
       "docx        1\n",
       "txt         1\n",
       "pptx        1\n",
       "csv         1\n",
       "py          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[\"extension\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/aymeric/.cache/huggingface/modules/datasets_modules/datasets/gaia-benchmark--GAIA/ec492fe4320ee795b1aed6bb46229c5f693226b0f1316347501c24b4baeee005 (last modified on Tue May 28 10:04:32 2024) since it couldn't be found locally at gaia-benchmark/GAIA, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "\n",
    "SET = \"test\"\n",
    "\n",
    "eval_ds = datasets.load_dataset(\"gaia-benchmark/GAIA\", \"2023_all\")[SET]\n",
    "eval_ds = eval_ds.rename_columns(\n",
    "    {\"Question\": \"question\", \"Final answer\": \"true_answer\", \"Level\": \"task\"}\n",
    ")\n",
    "eval_df = pd.DataFrame(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_json(\n",
    "    f\"output_gaia/{SET}/react_code_gpt4o_23-june_planning2_newprompt5_test.jsonl\",\n",
    "    lines=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluation.gaia_scorer import question_scorer\n",
    "\n",
    "results[\"is_correct\"] = results.apply(\n",
    "    lambda x: question_scorer(x[\"prediction\"], x[\"true_answer\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0033333333333333335"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"is_correct\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Reverse this ascii picture so that the fish is facing the opposite direction:\\n&gt;&gt;$()&gt;. Return the characters (without quotes) in a comma separated list.</td>\n",
       "      <td>.,&gt;,),(,$,&gt;,&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     question  \\\n",
       "278  Reverse this ascii picture so that the fish is facing the opposite direction:\\n>>$()>. Return the characters (without quotes) in a comma separated list.   \n",
       "\n",
       "        prediction  \n",
       "278  .,>,),(,$,>,>  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.loc[results[\"is_correct\"], [\"question\", \"prediction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements(x):\n",
    "    output = x[0][\"task\"]\n",
    "    for y in x[1:]:\n",
    "        try:\n",
    "            if \"observation\" in y:\n",
    "                output += y[\"llm_output\"] + \"\\nObservation:\" + y[\"observation\"]\n",
    "            else:\n",
    "                output += y[\"llm_output\"] + \"\\Error:\" + str(y[\"error\"])\n",
    "        except:\n",
    "            pass\n",
    "    return output\n",
    "\n",
    "\n",
    "results[\"reasoning_trace\"] = results[\"intermediate_steps\"].apply(\n",
    "    lambda x: get_elements(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[[\"question\", \"prediction\", \"reasoning_trace\"]].rename(\n",
    "    columns={\"prediction\": \"model_answer\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "results = results.merge(eval_df[[\"task_id\", \"question\"]], on=\"question\")\n",
    "results = results.drop_duplicates(subset=[\"task_id\"])\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[\n",
    "    [\"task_id\", \"model_answer\"]\n",
    "]  # [\"task_id\", \"model_answer\", \"reasoning_trace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_json(\n",
    "    f\"gaia_{SET}_transformers_agents_gpt4o.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieved documents:\\n\\nat+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\\n===Document===\\nBART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+wa\\n===Document===\\nВ области NLP ( Обработка текстов на естественном языке ):\\n- [Маскированное заполнение слов с помощью BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\\n- [Распознавание сущностей с помощью Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\\n- [Генерация текста с помощью GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RetrieverTool(vectordb)(query=\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vectordb': <langchain_community.vectorstores.faiss.FAISS object at 0x3acdc95d0>, 'all_sources': ['datasets', 'evaluate', 'blog', 'hf-endpoints-documentation', 'diffusers', 'datasets-server', 'transformers', 'optimum', 'pytorch-image-models', 'gradio', 'deep-rl-class', 'course', 'peft', 'hub-docs']}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "from transformers.agents import Tool\n",
    "\n",
    "retriever_tool = Tool.from_hub(\n",
    "    repo_id=\"m-ric/langchain-retriever-tool\", vectordb=vectordb, all_sources=all_sources\n",
    ")\n",
    "\n",
    "other_tool = Tool.from_hub(repo_id=\"m-ric/hf-model-downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieved documents:\\n\\nat+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\\n===Document===\\nBART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+wa\\n===Document===\\nВ области NLP ( Обработка текстов на естественном языке ):\\n- [Маскированное заполнение слов с помощью BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\\n- [Распознавание сущностей с помощью Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\\n- [Генерация текста с помощью GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool(query=\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mPlease show me a LORA finetuning script\u001b[0m\n",
      "\u001b[33;1mCalling tool: 'retriever' with arguments: {'query': 'LORA finetuning script', 'source': \"['transformers', 'hf-endpoints-documentation']\"}\u001b[0m\n",
      "\u001b[33;1mCalling tool: 'retriever' with arguments: {'query': 'LORA finetuning script'}\u001b[0m\n",
      "\u001b[33;1mCalling tool: 'retriever' with arguments: {'query': 'LORA finetuning script', 'source': 'github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py'}\u001b[0m\n",
      "\u001b[33;1mCalling tool: 'final_answer' with arguments: {'answer': 'https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py'}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:\n",
      "https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py\n"
     ]
    }
   ],
   "source": [
    "from transformers.agents import HfApiEngine, ReactJsonAgent\n",
    "\n",
    "llm_engine = HfApiEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "\n",
    "agent = ReactJsonAgent(\n",
    "    tools=[RetrieverTool(vectordb, all_sources)], llm_engine=llm_engine\n",
    ")\n",
    "\n",
    "agent_output = agent.run(\"Please show me a LORA finetuning script\")\n",
    "\n",
    "print(\"Final output:\")\n",
    "print(agent_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lavague\n",
      "  Using cached lavague-1.1.19-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting lavague-contexts-openai<0.3.0,>=0.2.0 (from lavague)\n",
      "  Using cached lavague_contexts_openai-0.2.4-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting lavague-core<0.3.0,>=0.2.31 (from lavague)\n",
      "  Using cached lavague_core-0.2.35-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting lavague-drivers-selenium<0.3.0,>=0.2.12 (from lavague)\n",
      "  Using cached lavague_drivers_selenium-0.2.15-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting lavague-gradio<0.3.0,>=0.2.8 (from lavague)\n",
      "  Using cached lavague_gradio-0.2.8-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting llama-index-embeddings-azure-openai==0.1.11 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached llama_index_embeddings_azure_openai-0.1.11-py3-none-any.whl.metadata (804 bytes)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.9 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
      "Collecting llama-index-llms-azure-openai==0.1.10 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached llama_index_llms_azure_openai-0.1.10-py3-none-any.whl.metadata (787 bytes)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.9 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached llama_index_llms_openai-0.1.31-py3-none-any.whl.metadata (650 bytes)\n",
      "Collecting llama-index-multi-modal-llms-azure-openai==0.1.4 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached llama_index_multi_modal_llms_azure_openai-0.1.4-py3-none-any.whl.metadata (820 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.6 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.11.post1 (from llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached llama_index_core-0.10.68.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting azure-identity<2.0.0,>=1.15.0 (from llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Downloading azure_identity-1.19.0-py3-none-any.whl.metadata (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m606.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (0.27.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from lavague-core<0.3.0,>=0.2.31->lavague) (6.0.2)\n",
      "Collecting ipython<8.0.0,>=7.34.0 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached ipython-7.34.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.23.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from lavague-core<0.3.0,>=0.2.31->lavague) (4.23.0)\n",
      "Collecting langchain<0.2.0,>=0.1.20 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting llama-index==0.10.56 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached llama_index-0.10.56-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-retrievers-bm25<0.2.0,>=0.1.3 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached llama_index_retrievers_bm25-0.1.5-py3-none-any.whl.metadata (700 bytes)\n",
      "Requirement already satisfied: lxml<6.0.0,>=5.1.1 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from lavague-core<0.3.0,>=0.2.31->lavague) (5.3.0)\n",
      "Collecting lxml-html-clean<0.2.0,>=0.1.1 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached lxml_html_clean-0.1.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.8 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting seaborn<0.14.0,>=0.13.2 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting tenacity<8.4.0,>=8.2.0 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting trafilatura<2.0.0,>=1.9.0 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading trafilatura-1.12.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.11.post1 (from llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached llama_index_core-0.10.56-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_index_readers_file-0.1.33-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (0.6.7)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2024.3.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (3.3)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.35.7)\n",
      "Requirement already satisfied: pandas in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.32.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (0.9.0)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting selenium<5.0.0,>=4.18.1 (from lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
      "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio==4.39.0 (from lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (4.4.0)\n",
      "Collecting fastapi (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.1.1 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (0.23.4)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (2.1.5)\n",
      "Collecting matplotlib~=3.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.10.7)\n",
      "Requirement already satisfied: packaging in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (24.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (2.8.2)\n",
      "Collecting pydub (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading ruff-0.7.0-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: urllib3~=2.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (2.2.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached websockets-11.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (69.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.19.1)\n",
      "Requirement already satisfied: decorator in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (5.1.1)\n",
      "Collecting pickleshare (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (5.14.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (3.0.47)\n",
      "Requirement already satisfied: pygments in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (2.18.0)\n",
      "Collecting backcall (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (4.9.0)\n",
      "Requirement already satisfied: appnope in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.1.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.23.0->lavague-core<0.3.0,>=0.2.31->lavague) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.23.0->lavague-core<0.3.0,>=0.2.31->lavague) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.23.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.23.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.20.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague) (4.0.3)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague) (0.1.117)\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.9 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Downloading llama_index_llms_openai-0.1.30-py3-none-any.whl.metadata (650 bytes)\n",
      "  Downloading llama_index_llms_openai-0.1.29-py3-none-any.whl.metadata (650 bytes)\n",
      "  Downloading llama_index_llms_openai-0.1.28-py3-none-any.whl.metadata (650 bytes)\n",
      "  Downloading llama_index_llms_openai-0.1.27-py3-none-any.whl.metadata (610 bytes)\n",
      "  Downloading llama_index_llms_openai-0.1.26-py3-none-any.whl.metadata (610 bytes)\n",
      "Collecting rank-bm25<0.3.0,>=0.2.2 (from llama-index-retrievers-bm25<0.2.0,>=0.1.3->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting trio~=0.17 (from selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague) (2024.7.4)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague) (1.8.0)\n",
      "Collecting courlan>=1.2.0 (from trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading courlan-1.3.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting htmldate>=1.8.1 (from trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading htmldate-1.9.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting justext>=3.0.1 (from trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached jusText-3.0.1-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: charset-normalizer>=3.2.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague) (3.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.8)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (1.2.2)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Downloading azure_core-1.31.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (43.0.0)\n",
      "Collecting msal>=1.30.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Downloading msal-1.31.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: babel>=2.16.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from courlan>=1.2.0->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague) (2.16.0)\n",
      "Collecting tld>=0.13 (from courlan>=1.2.0->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached tld-0.13-py2.py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (3.22.0)\n",
      "Collecting dateparser>=1.1.2 (from htmldate>=1.8.1->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from htmldate>=1.8.1->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague) (2.9.0.post0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from httpx->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.15.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from jedi>=0.16->ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.8.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague) (1.33)\n",
      "Collecting packaging (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_cloud-0.1.4-py3-none-any.whl.metadata (814 bytes)\n",
      "INFO: pip is looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-program-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague) (4.2.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-readers-llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_index_readers_llama_parse-0.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_parse-0.5.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading contourpy-1.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading fonttools-4.54.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2024.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from pexpect>4.3->ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from pydantic>=2.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from pydantic>=2.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (2.20.1)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading rich-13.9.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Downloading starlette-0.41.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from azure-core>=1.31.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague) (2.6)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.17.0)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate>=1.8.1->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague) (2024.7.24)\n",
      "Collecting tzlocal (from dateparser>=1.1.2->htmldate>=1.8.1->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague) (3.0.0)\n",
      "INFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
      "  Downloading llama_parse-0.5.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.8-py3-none-any.whl.metadata (6.4 kB)\n",
      "  Downloading llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\n",
      "  Downloading llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading llama_parse-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "\u001b[33mWARNING: lxml 5.3.0 does not provide the extra 'html-clean'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting portalocker<3,>=1.4 (from msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.9.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/aymeric/venvs/gaia/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.22)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached lavague-1.1.19-py3-none-any.whl (8.4 kB)\n",
      "Downloading lavague_contexts_openai-0.2.4-py3-none-any.whl (2.5 kB)\n",
      "Downloading llama_index_embeddings_azure_openai-0.1.11-py3-none-any.whl (3.3 kB)\n",
      "Downloading llama_index_llms_azure_openai-0.1.10-py3-none-any.whl (5.1 kB)\n",
      "Using cached llama_index_multi_modal_llms_azure_openai-0.1.4-py3-none-any.whl (3.7 kB)\n",
      "Downloading lavague_core-0.2.35-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached llama_index-0.10.56-py3-none-any.whl (6.8 kB)\n",
      "Using cached llama_index_core-0.10.56-py3-none-any.whl (15.5 MB)\n",
      "Downloading lavague_drivers_selenium-0.2.15-py3-none-any.whl (12 kB)\n",
      "Using cached lavague_gradio-0.2.8-py3-none-any.whl (3.5 kB)\n",
      "Using cached gradio-4.39.0-py3-none-any.whl (12.4 MB)\n",
      "Using cached gradio_client-1.1.1-py3-none-any.whl (318 kB)\n",
      "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached ipython-7.34.0-py3-none-any.whl (793 kB)\n",
      "Using cached langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
      "Downloading llama_index_llms_openai-0.1.26-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
      "Using cached llama_index_retrievers_bm25-0.1.5-py3-none-any.whl (2.8 kB)\n",
      "Using cached lxml_html_clean-0.1.1-py3-none-any.whl (11 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading trafilatura-1.12.2-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.2/132.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading azure_identity-1.19.0-py3-none-any.whl (187 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.6/187.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading courlan-1.3.1-py3-none-any.whl (33 kB)\n",
      "Downloading htmldate-1.9.1-py3-none-any.whl (31 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached jusText-3.0.1-py2.py3-none-any.whl (837 kB)\n",
      "Using cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "Using cached langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl (9.5 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.33-py3-none-any.whl (38 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
      "Downloading matplotlib-3.9.2-cp310-cp310-macosx_11_0_arm64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
      "Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading ruff-0.7.0-py3-none-macosx_11_0_arm64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading azure_core-1.31.0-py3-none-any.whl (197 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.0-cp310-cp310-macosx_11_0_arm64.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading fonttools-4.54.1-cp310-cp310-macosx_11_0_arm64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp310-cp310-macosx_11_0_arm64.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_cloud-0.1.4-py3-none-any.whl (176 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
      "Downloading msal-1.31.0-py3-none-any.whl (113 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.1/113.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading rich-13.9.3-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.41.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-macosx_11_0_arm64.whl (982 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m982.4/982.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tld-0.13-py2.py3-none-any.whl (263 kB)\n",
      "Using cached websockets-11.0.3-cp310-cp310-macosx_11_0_arm64.whl (121 kB)\n",
      "Using cached wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-macosx_11_0_universal2.whl (271 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.2/271.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: striprtf, sortedcontainers, pydub, pickleshare, dirtyjson, backcall, wsproto, wrapt, websockets, uvicorn, tzlocal, tomlkit, tld, tenacity, shellingham, semantic-version, ruff, rank-bm25, python-multipart, pysocks, pyparsing, PyJWT, portalocker, packaging, outcome, msgpack, mdurl, lxml-html-clean, kiwisolver, joblib, importlib-resources, greenlet, fonttools, ffmpy, cycler, contourpy, aiofiles, trio, tiktoken, starlette, nltk, matplotlib, markdown-it-py, justext, ipython, deprecated, dateparser, courlan, azure-core, trio-websocket, seaborn, rich, llama-cloud, htmldate, gradio-client, fastapi, typer, trafilatura, selenium, msal, llama-index-legacy, llama-index-core, langchain-core, msal-extensions, llama-parse, llama-index-retrievers-bm25, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, langchain-text-splitters, langchain-community, gradio, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, langchain, azure-identity, llama-index-program-openai, llama-index-llms-azure-openai, llama-index-question-gen-openai, llama-index-multi-modal-llms-azure-openai, llama-index-embeddings-azure-openai, llama-index, lavague-core, lavague-gradio, lavague-drivers-selenium, lavague-contexts-openai, lavague\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.5.0\n",
      "    Uninstalling tenacity-8.5.0:\n",
      "      Successfully uninstalled tenacity-8.5.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.27.0\n",
      "    Uninstalling ipython-8.27.0:\n",
      "      Successfully uninstalled ipython-8.27.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.39\n",
      "    Uninstalling langchain-core-0.2.39:\n",
      "      Successfully uninstalled langchain-core-0.2.39\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.2.2\n",
      "    Uninstalling langchain-text-splitters-0.2.2:\n",
      "      Successfully uninstalled langchain-text-splitters-0.2.2\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.16\n",
      "    Uninstalling langchain-community-0.2.16:\n",
      "      Successfully uninstalled langchain-community-0.2.16\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.16\n",
      "    Uninstalling langchain-0.2.16:\n",
      "      Successfully uninstalled langchain-0.2.16\n",
      "Successfully installed PyJWT-2.9.0 aiofiles-23.2.1 azure-core-1.31.0 azure-identity-1.19.0 backcall-0.2.0 contourpy-1.3.0 courlan-1.3.1 cycler-0.12.1 dateparser-1.2.0 deprecated-1.2.14 dirtyjson-1.0.8 fastapi-0.115.3 ffmpy-0.4.0 fonttools-4.54.1 gradio-4.39.0 gradio-client-1.1.1 greenlet-3.1.1 htmldate-1.9.1 importlib-resources-6.4.5 ipython-7.34.0 joblib-1.4.2 justext-3.0.1 kiwisolver-1.4.7 langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.2 lavague-1.1.19 lavague-contexts-openai-0.2.4 lavague-core-0.2.35 lavague-drivers-selenium-0.2.15 lavague-gradio-0.2.8 llama-cloud-0.1.4 llama-index-0.10.56 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.56 llama-index-embeddings-azure-openai-0.1.11 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.2.7 llama-index-legacy-0.9.48.post3 llama-index-llms-azure-openai-0.1.10 llama-index-llms-openai-0.1.26 llama-index-multi-modal-llms-azure-openai-0.1.4 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.33 llama-index-readers-llama-parse-0.1.6 llama-index-retrievers-bm25-0.1.5 llama-parse-0.4.9 lxml-html-clean-0.1.1 markdown-it-py-3.0.0 matplotlib-3.9.2 mdurl-0.1.2 msal-1.31.0 msal-extensions-1.2.0 msgpack-1.1.0 nltk-3.9.1 outcome-1.3.0.post0 packaging-23.2 pickleshare-0.7.5 portalocker-2.10.1 pydub-0.25.1 pyparsing-3.2.0 pysocks-1.7.1 python-multipart-0.0.12 rank-bm25-0.2.2 rich-13.9.3 ruff-0.7.0 seaborn-0.13.2 selenium-4.25.0 semantic-version-2.10.0 shellingham-1.5.4 sortedcontainers-2.4.0 starlette-0.41.0 striprtf-0.0.26 tenacity-8.3.0 tiktoken-0.8.0 tld-0.13 tomlkit-0.12.0 trafilatura-1.12.2 trio-0.27.0 trio-websocket-0.11.1 typer-0.12.5 tzlocal-5.2 uvicorn-0.32.0 websockets-11.0.3 wrapt-1.16.0 wsproto-1.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lavague -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 19:43:35,733 - INFO - Screenshot folder cleared\n",
      "2024-10-22 19:43:43,765 - INFO - - The current screenshot shows the first page of a paper titled \"Attention Is All You Need\" on arXiv.\n",
      "- The objective is to provide the content of page 3 from this paper.\n",
      "- Since the paper is likely a PDF, the next step is to locate and click on the PDF link to access the full document.\n",
      "\n",
      "Next engine: Navigation Engine\n",
      "Instruction: Click on the 'PDF' link in the 'Download' section.\n",
      "2024-10-22 19:43:56,282 - INFO - Thoughts:\n",
      "- The current screenshot shows a PDF viewer displaying the paper \"Attention Is All You Need.\"\n",
      "- The objective is to provide the content of page 3 from this paper.\n",
      "- The PDF viewer is currently displaying the first page.\n",
      "- The next step is to navigate to page 3 to extract its content.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN to page 3.\n",
      "2024-10-22 19:44:00,065 - INFO - Sure, let's proceed with the next steps.\n",
      "\n",
      "Thoughts:\n",
      "- The current screenshot shows a PDF document open in a viewer.\n",
      "- The objective is to provide the content of page 3.\n",
      "- The document is currently on the first page, and the previous step was to scroll down to page 3.\n",
      "- The next step is to ensure we are on page 3 and extract its content.\n",
      "\n",
      "Next engine: Python Engine\n",
      "Instruction: Extract the text content from page 3 of the PDF document.\n",
      "2024-10-22 19:44:00,429 - ERROR - Error while running the agent: 1 validation error for Document\n",
      "text\n",
      "  none is not an allowed value (type=type_error.none.not_allowed)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Document\ntext\n  none is not an allowed value (type=type_error.none.not_allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m agent\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://arxiv.org/abs/1706.03762\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run agent with a specific objective\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGive me the content of page 3 from the paper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/gaia/lib/python3.10/site-packages/lavague/core/agents.py:521\u001b[0m, in \u001b[0;36mWebAgent.run\u001b[0;34m(self, objective, user_data, display, log_to_db, step_by_step)\u001b[0m\n\u001b[1;32m    519\u001b[0m     logging_print\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while running the agent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     origin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morigin \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlavague\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/venvs/gaia/lib/python3.10/site-packages/lavague/core/agents.py:506\u001b[0m, in \u001b[0;36mWebAgent.run\u001b[0;34m(self, objective, user_data, display, log_to_db, step_by_step)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps):\n\u001b[0;32m--> 506\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/gaia/lib/python3.10/site-packages/lavague/core/agents.py:470\u001b[0m, in \u001b[0;36mWebAgent.run_step\u001b[0;34m(self, objective)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mend_step()\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\n\u001b[0;32m--> 470\u001b[0m action_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_instruction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_engine_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m action_result\u001b[38;5;241m.\u001b[39mcode\n",
      "File \u001b[0;32m~/venvs/gaia/lib/python3.10/site-packages/lavague/core/action_engine.py:238\u001b[0m, in \u001b[0;36mActionEngine.dispatch_instruction\u001b[0;34m(self, next_engine_name, instruction)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03mDispatch the instruction to the appropriate ActionEngine\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    `Any`: The output of the code\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m next_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengines[next_engine_name]\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnext_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/gaia/lib/python3.10/site-packages/lavague/core/python_engine.py:191\u001b[0m, in \u001b[0;36mPythonEngine.execute_instruction\u001b[0;34m(self, instruction)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_screenshot()\n\u001b[1;32m    190\u001b[0m page_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_html(html)\n\u001b[0;32m--> 191\u001b[0m documents \u001b[38;5;241m=\u001b[39m [\u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    193\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(documents, embed_model\u001b[38;5;241m=\u001b[39membedding)\n\u001b[1;32m    194\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_query_engine(llm\u001b[38;5;241m=\u001b[39mllm)\n",
      "File \u001b[0;32m~/venvs/gaia/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Document\ntext\n  none is not an allowed value (type=type_error.none.not_allowed)"
     ]
    }
   ],
   "source": [
    "from lavague.drivers.selenium import SeleniumDriver\n",
    "from lavague.core import ActionEngine, WorldModel\n",
    "from lavague.core.agents import WebAgent\n",
    "\n",
    "# Set up our three key components: Driver, Action Engine, World Model\n",
    "driver = SeleniumDriver(headless=False)\n",
    "action_engine = ActionEngine(driver)\n",
    "world_model = WorldModel()\n",
    "\n",
    "# Create Web Agent\n",
    "agent = WebAgent(world_model, action_engine)\n",
    "\n",
    "# Set URL\n",
    "agent.get(\"https://arxiv.org/abs/1706.03762\")\n",
    "\n",
    "# Run agent with a specific objective\n",
    "agent.run(\"Give me the content of page 3 from the paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the output valid? True\n"
     ]
    }
   ],
   "source": [
    "example_blob = \"\"\"Thought: This is an example thought.\n",
    "Action:\n",
    "{\n",
    "    \"action\": \"some_action\",\n",
    "    \"action_input\": \"abc\"\n",
    "}\n",
    "<end_action>\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def validate_output(output, grammar):\n",
    "    return re.match(grammar, output) is not None\n",
    "\n",
    "\n",
    "from transformers.agents.agents import DEFAULT_JSON_GRAMMAR\n",
    "\n",
    "is_valid = validate_output(example_blob, DEFAULT_JSON_GRAMMAR)\n",
    "print(f\"Is the output valid? {is_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mHow much is 2+16?\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/aymeric/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mresult\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m2\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m+\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;139m16\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7manswer\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7mresult\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m18\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import ReactJsonAgent, HfEngine, Tool\n",
    "import os\n",
    "\n",
    "# to load SerpAPI key\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "\n",
    "llm_engine = HfEngine(model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\")\n",
    "\n",
    "agent = ReactJsonAgent(\n",
    "    llm_engine=llm_engine,\n",
    "    tools=[],\n",
    "    max_iterations=10,\n",
    ")\n",
    "agent.run(\"How much is 2+16?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/aymeric/.cache/huggingface/token\n",
      "Login successful\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mJust answer with \"2\"\n",
      "\u001b[0m\n",
      "\u001b[33;1mCalling tool: 'final_answer' with arguments: {'answer': '2'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from transformers import ReactJsonAgent, HfEngine, Tool, stream_to_gradio\n",
    "from gradio import (\n",
    "    Chatbot,\n",
    "    ChatMessage,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# to load SerpAPI key\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "\n",
    "llm_engine = HfEngine(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "\n",
    "agent = ReactJsonAgent(\n",
    "    llm_engine=llm_engine,\n",
    "    tools=[],\n",
    "    max_iterations=10,\n",
    ")\n",
    "\n",
    "function = \"\"\"import numpy as np\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\"\"\"\n",
    "\n",
    "task = \"\"\"Just answer with \"2\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def interact_with_agent(prompt):\n",
    "    full_prompt = task.replace(\"<<function>>\", prompt)\n",
    "    messages = []\n",
    "    messages.append(ChatMessage(role=\"user\", content=full_prompt))\n",
    "    yield messages\n",
    "    for msg in stream_to_gradio(agent, full_prompt):\n",
    "        messages.append(msg)\n",
    "        yield messages + [\n",
    "            ChatMessage(role=\"assistant\", content=\"⏳ Task not finished yet!\")\n",
    "        ]\n",
    "    yield messages\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=\"soft\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"### Python test generator\n",
    "Write your function in the left textbox, and the agent on the right will generate tests for you!\"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text_input = gr.Textbox(\n",
    "                lines=1, label=\"Your function to test\", value=function\n",
    "            )\n",
    "            submit = gr.Button(\"Generate tests!\")\n",
    "        with gr.Column():\n",
    "            chatbot = Chatbot(label=\"Agent\", type=\"messages\")\n",
    "\n",
    "    submit.click(interact_with_agent, [text_input], [chatbot])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (4.37.2)\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.38.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=5.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (5.3.0)\n",
      "Requirement already satisfied: fastapi in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.111.0)\n",
      "Requirement already satisfied: ffmpy in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.3.2)\n",
      "Collecting gradio-client==1.1.0 (from gradio)\n",
      "  Downloading gradio_client-1.1.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.23.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (3.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (3.10.3)\n",
      "Requirement already satisfied: packaging in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (10.3.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (2.7.1)\n",
      "Requirement already satisfied: pydub in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.4.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio) (0.30.1)\n",
      "Requirement already satisfied: fsspec in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio-client==1.1.0->gradio) (2024.3.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from gradio-client==1.1.0->gradio) (10.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio) (4.22.0)\n",
      "Requirement already satisfied: toolz in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: anyio in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
      "Requirement already satisfied: requests in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.52.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from fastapi->gradio) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from fastapi->gradio) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from fastapi->gradio) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from fastapi->gradio) (2.1.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/aymeric/venvs/disposable/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-4.38.1-py3-none-any.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.1.0-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gradio-client, gradio\n",
      "  Attempting uninstall: gradio-client\n",
      "    Found existing installation: gradio_client 1.0.2\n",
      "    Uninstalling gradio_client-1.0.2:\n",
      "      Successfully uninstalled gradio_client-1.0.2\n",
      "  Attempting uninstall: gradio\n",
      "    Found existing installation: gradio 4.37.2\n",
      "    Uninstalling gradio-4.37.2:\n",
      "      Successfully uninstalled gradio-4.37.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lavague-gradio 0.2.4 requires gradio==4.26.0, but you have gradio 4.38.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gradio-4.38.1 gradio-client-1.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def calculate_sum():\n",
      "    a = 2\n",
      "    b = 2\n",
      "    return a + b\n",
      "\n",
      "final_answer(calculate_sum())\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\"meta-llama/Meta-Llama-3.1-70B-Instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "Environment: ipython\n",
    "Tools: brave_search, final_answer\n",
    "\n",
    "Cutting Knowledge Date: 01 March 2023\n",
    "Today's Date: 13 July 2024\n",
    "\n",
    "You are a helpful Assistant.\n",
    "Please help me answer this question by laying out a code snippet.\n",
    "Call the tool final_answer with the answer, as in final_answer(your_answer), to give your answer\n",
    "\n",
    "Now let's start.\n",
    "\"\"\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "]\n",
    "\n",
    "output = client.chat_completion(messages=messages)\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mWhat is the 6th FIbonacci number?\u001b[0m\n",
      "\u001b[31;20mError in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: KdRTRtrMtd1FZWFjoL7Og)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1044, in step\n",
      "    llm_output = self.llm_engine(\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/llm_engine.py\", line 80, in __call__\n",
      "    response = self.client.chat_completion(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 834, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: KdRTRtrMtd1FZWFjoL7Og)\n",
      "\n",
      "Request failed during generation: Server error: '^'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 766, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1050, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: KdRTRtrMtd1FZWFjoL7Og)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\n",
      "\u001b[31;20mError in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: ZIvePBtJLza1ZIRCZkly4)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1044, in step\n",
      "    llm_output = self.llm_engine(\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/llm_engine.py\", line 80, in __call__\n",
      "    response = self.client.chat_completion(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 834, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: ZIvePBtJLza1ZIRCZkly4)\n",
      "\n",
      "Request failed during generation: Server error: '^'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 766, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1050, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: ZIvePBtJLza1ZIRCZkly4)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\n",
      "\u001b[31;20mError in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: 9SBKTowFfClt_3OJ3SUp5)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1044, in step\n",
      "    llm_output = self.llm_engine(\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/llm_engine.py\", line 80, in __call__\n",
      "    response = self.client.chat_completion(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 834, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: 9SBKTowFfClt_3OJ3SUp5)\n",
      "\n",
      "Request failed during generation: Server error: '^'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 766, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1050, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: 9SBKTowFfClt_3OJ3SUp5)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\n",
      "\u001b[31;20mError in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: vP3OHUhsgpJwSc0QzUIEe)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1044, in step\n",
      "    llm_output = self.llm_engine(\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/llm_engine.py\", line 80, in __call__\n",
      "    response = self.client.chat_completion(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 834, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: vP3OHUhsgpJwSc0QzUIEe)\n",
      "\n",
      "Request failed during generation: Server error: '^'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 766, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1050, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: vP3OHUhsgpJwSc0QzUIEe)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\n",
      "\u001b[31;20mError in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: zb6AviIXJBeot5cASX3tF)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1044, in step\n",
      "    llm_output = self.llm_engine(\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/llm_engine.py\", line 80, in __call__\n",
      "    response = self.client.chat_completion(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 834, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: zb6AviIXJBeot5cASX3tF)\n",
      "\n",
      "Request failed during generation: Server error: '^'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 766, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1050, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: zb6AviIXJBeot5cASX3tF)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\n",
      "\u001b[31;20mError in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: P3JW8A9qxIFJUglNGaYFx)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1044, in step\n",
      "    llm_output = self.llm_engine(\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/llm_engine.py\", line 80, in __call__\n",
      "    response = self.client.chat_completion(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 834, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/aymeric/venvs/disposable/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: P3JW8A9qxIFJUglNGaYFx)\n",
      "\n",
      "Request failed during generation: Server error: '^'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 766, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/aymeric/Documents/Code/original_transformers/transformers/src/transformers/agents/agents.py\", line 1050, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 424 Client Error: Failed Dependency for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: P3JW8A9qxIFJUglNGaYFx)\n",
      "\n",
      "Request failed during generation: Server error: '^'.\n",
      "\u001b[31;20mReached max iterations.\u001b[0m\n",
      "NoneType: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Error in generating final llm output: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions (Request ID: ZfElb-wYqbFwwm6k7BaYP).'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import (\n",
    "    load_tool,\n",
    "    ReactCodeAgent,\n",
    "    HfEngine,\n",
    ")\n",
    "\n",
    "# Import tool from Hub\n",
    "\n",
    "llm_engine = HfEngine(\"meta-llama/Meta-Llama-3.1-70B-Instruct\")\n",
    "\n",
    "# Initialize the agent with the image generation tool\n",
    "agent = ReactCodeAgent(tools=[], llm_engine=llm_engine)\n",
    "\n",
    "agent.run(\"What is the 6th FIbonacci number?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mMake me a picture of the Statue of Liberty.\u001b[0m\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mimage_generator\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mprompt\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mA high-resolution, photorealistic image of the Statue of Liberty at sunset with a bright blue sky and a slight mist in the background.\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1m>>> Final answer:\u001b[0m\n",
      "\u001b[32;20m/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmps6vlpwnv/a8350500-9033-4138-a09a-d7f49d65457b.png\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message ChatMessage(role='assistant', content='Thought: I will use the `image_generator` tool to create an image of the Statue of Liberty. I will provide a descriptive prompt to ensure a high-quality image.\\n\\n', metadata=Metadata(title=None))\n",
      "message ChatMessage(role='assistant', content='```py\\nimage = image_generator(prompt=\"A high-resolution, photorealistic image of the Statue of Liberty at sunset with a bright blue sky and a slight mist in the background.\")\\nfinal_answer(image)\\n```', metadata={'title': '🛠️ Used tool code interpreter'})\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import (\n",
    "    load_tool,\n",
    "    ReactCodeAgent,\n",
    "    HfEngine,\n",
    "    stream_to_gradio,\n",
    ")\n",
    "\n",
    "# Import tool from Hub\n",
    "image_generation_tool = load_tool(\"m-ric/text-to-image\")\n",
    "\n",
    "llm_engine = HfEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "\n",
    "# Initialize the agent with the image generation tool\n",
    "agent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)\n",
    "\n",
    "\n",
    "def interact_with_agent(task):\n",
    "    messages = []\n",
    "    messages.append(gr.ChatMessage(role=\"user\", content=task))\n",
    "    yield messages\n",
    "    for msg in stream_to_gradio(agent, task):\n",
    "        messages.append(msg)\n",
    "        yield messages + [\n",
    "            gr.ChatMessage(role=\"assistant\", content=\"⏳ Task not finished yet!\")\n",
    "        ]\n",
    "    yield messages\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    text_input = gr.Textbox(\n",
    "        lines=1,\n",
    "        label=\"Chat Message\",\n",
    "        value=\"Make me a picture of the Statue of Liberty.\",\n",
    "    )\n",
    "    submit = gr.Button(\"Run illustrator agent!\")\n",
    "    chatbot = gr.Chatbot(\n",
    "        label=\"Agent\",\n",
    "        type=\"messages\",\n",
    "        avatar_images=(\n",
    "            None,\n",
    "            \"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\",\n",
    "        ),\n",
    "    )\n",
    "    submit.click(interact_with_agent, [text_input], [chatbot])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
