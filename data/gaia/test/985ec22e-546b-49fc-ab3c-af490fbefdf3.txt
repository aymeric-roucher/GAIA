
Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Thermodynamics near absolute zero

Relation with Bose–Einstein condensate

Absolute temperature scales

Negative temperatures

History

        Limit to the "degree of cold"
        Charles's law
        Lord Kelvin's work
        The race to absolute zero
    Very low temperatures
    See also
    References
    Further reading
    External links

Absolute zero

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
This article is about the minimum temperature limit. For other uses, see Absolute Zero (disambiguation).
	
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Absolute zero" – news · newspapers · books · scholar · JSTOR (December 2022) (Learn how and when to remove this template message)
Zero kelvin (−273.15 °C) is defined as absolute zero.

Absolute zero is the lowest limit of the thermodynamic temperature scale; a state at which the enthalpy and entropy of a cooled ideal gas reach their minimum value, taken as zero kelvin. The fundamental particles of nature have minimum vibrational motion, retaining only quantum mechanical, zero-point energy-induced particle motion. The theoretical temperature is determined by extrapolating the ideal gas law; by international agreement, absolute zero is taken as −273.15 degrees on the Celsius scale (International System of Units),[1][2][3] which equals −459.67 degrees on the Fahrenheit scale (United States customary units or imperial units).[4] The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition.

It is commonly thought of as the lowest temperature possible, but it is not the lowest enthalpy state possible, because all real substances begin to depart from the ideal gas when cooled as they approach the change of state to liquid, and then to solid; and the sum of the enthalpy of vaporization (gas to liquid) and enthalpy of fusion (liquid to solid) exceeds the ideal gas's change in enthalpy to absolute zero. In the quantum-mechanical description, matter (solid) at absolute zero is in its ground state, the point of lowest internal energy.

The laws of thermodynamics indicate that absolute zero cannot be reached using only thermodynamic means, because the temperature of the substance being cooled approaches the temperature of the cooling agent asymptotically.[5] Even a system at absolute zero, if it could somehow be achieved, would still possess quantum mechanical zero-point energy, the energy of its ground state at absolute zero; the kinetic energy of the ground state cannot be removed.

Scientists and technologists routinely achieve temperatures close to absolute zero, where matter exhibits quantum effects such as Bose–Einstein condensate, superconductivity and superfluidity.
Thermodynamics near absolute zero

At temperatures near 0 K (−273.15 °C; −459.67 °F), nearly all molecular motion ceases and ΔS = 0 for any adiabatic process, where S is the entropy. In such a circumstance, pure substances can (ideally) form perfect crystals with no structural imperfections as T → 0. Max Planck's strong form of the third law of thermodynamics states the entropy of a perfect crystal vanishes at absolute zero. The original Nernst heat theorem makes the weaker and less controversial claim that the entropy change for any isothermal process approaches zero as T → 0:

    lim T → 0 Δ S = 0 \lim _{T\to 0}\Delta S=0

The implication is that the entropy of a perfect crystal approaches a constant value. An adiabat is a state with constant entropy, typically represented on a graph as a curve in a manner similar to isotherms and isobars.

    The Nernst postulate identifies the isotherm T = 0 as coincident with the adiabat S = 0, although other isotherms and adiabats are distinct. As no two adiabats intersect, no other adiabat can intersect the T = 0 isotherm. Consequently no adiabatic process initiated at nonzero temperature can lead to zero temperature. (≈ Callen, pp. 189–190)

A perfect crystal is one in which the internal lattice structure extends uninterrupted in all directions. The perfect order can be represented by translational symmetry along three (not usually orthogonal) axes. Every lattice element of the structure is in its proper place, whether it is a single atom or a molecular grouping. For substances that exist in two (or more) stable crystalline forms, such as diamond and graphite for carbon, there is a kind of chemical degeneracy. The question remains whether both can have zero entropy at T = 0 even though each is perfectly ordered.

Perfect crystals never occur in practice; imperfections, and even entire amorphous material inclusions, can and do get "frozen in" at low temperatures, so transitions to more stable states do not occur.

Using the Debye model, the specific heat and entropy of a pure crystal are proportional to T 3, while the enthalpy and chemical potential are proportional to T 4. (Guggenheim, p. 111) These quantities drop toward their T = 0 limiting values and approach with zero slopes. For the specific heats at least, the limiting value itself is definitely zero, as borne out by experiments to below 10 K. Even the less detailed Einstein model shows this curious drop in specific heats. In fact, all specific heats vanish at absolute zero, not just those of crystals. Likewise for the coefficient of thermal expansion. Maxwell's relations show that various other quantities also vanish. These phenomena were unanticipated.

Since the relation between changes in Gibbs free energy (G), the enthalpy (H) and the entropy is

    Δ G = Δ H − T Δ S \Delta G=\Delta H-T\Delta S\,

thus, as T decreases, ΔG and ΔH approach each other (so long as ΔS is bounded). Experimentally, it is found that all spontaneous processes (including chemical reactions) result in a decrease in G as they proceed toward equilibrium. If ΔS and/or T are small, the condition ΔG < 0 may imply that ΔH < 0, which would indicate an exothermic reaction. However, this is not required; endothermic reactions can proceed spontaneously if the TΔS term is large enough.

Moreover, the slopes of the derivatives of ΔG and ΔH converge and are equal to zero at T = 0. This ensures that ΔG and ΔH are nearly the same over a considerable range of temperatures and justifies the approximate empirical Principle of Thomsen and Berthelot, which states that the equilibrium state to which a system proceeds is the one that evolves the greatest amount of heat, i.e., an actual process is the most exothermic one. (Callen, pp. 186–187)

One model that estimates the properties of an electron gas at absolute zero in metals is the Fermi gas. The electrons, being fermions, must be in different quantum states, which leads the electrons to get very high typical velocities, even at absolute zero. The maximum energy that electrons can have at absolute zero is called the Fermi energy. The Fermi temperature is defined as this maximum energy divided by the Boltzmann constant, and is on the order of 80,000 K for typical electron densities found in metals. For temperatures significantly below the Fermi temperature, the electrons behave in almost the same way as at absolute zero. This explains the failure of the classical equipartition theorem for metals that eluded classical physicists in the late 19th century.
Relation with Bose–Einstein condensate
Main article: Bose–Einstein condensate
Velocity-distribution data of a gas of rubidium atoms at a temperature within a few billionths of a degree above absolute zero. Left: just before the appearance of a Bose–Einstein condensate. Center: just after the appearance of the condensate. Right: after further evaporation, leaving a sample of nearly pure condensate.

A Bose–Einstein condensate (BEC) is a state of matter of a dilute gas of weakly interacting bosons confined in an external potential and cooled to temperatures very near absolute zero. Under such conditions, a large fraction of the bosons occupy the lowest quantum state of the external potential, at which point quantum effects become apparent on a macroscopic scale.[6]

This state of matter was first predicted by Satyendra Nath Bose and Albert Einstein in 1924–25. Bose first sent a paper to Einstein on the quantum statistics of light quanta (now called photons). Einstein was impressed, translated the paper from English to German and submitted it for Bose to the Zeitschrift für Physik, which published it. Einstein then extended Bose's ideas to material particles (or matter) in two other papers.[7]

Seventy years later, in 1995, the first gaseous condensate was produced by Eric Cornell and Carl Wieman at the University of Colorado at Boulder NIST-JILA lab, using a gas of rubidium atoms cooled to 170 nanokelvin (nK)[8] (1.7×10−7 K).[9]

A record cold temperature of 450 ± 80 picokelvin (pK) (4.5×10−10 K) in a BEC of sodium atoms was achieved in 2003 by researchers at the Massachusetts Institute of Technology (MIT).[10] The associated black-body (peak emittance) wavelength of 6,400 kilometers is roughly the radius of Earth.
Absolute temperature scales

Absolute, or thermodynamic, temperature is conventionally measured in kelvin (Celsius-scaled increments) and in the Rankine scale (Fahrenheit-scaled increments) with increasing rarity. Absolute temperature measurement is uniquely determined by a multiplicative constant which specifies the size of the degree, so the ratios of two absolute temperatures, T2/T1, are the same in all scales. The most transparent definition of this standard comes from the Maxwell–Boltzmann distribution. It can also be found in Fermi–Dirac statistics (for particles of half-integer spin) and Bose–Einstein statistics (for particles of integer spin). All of these define the relative numbers of particles in a system as decreasing exponential functions of energy (at the particle level) over kT, with k representing the Boltzmann constant and T representing the temperature observed at the macroscopic level.[1]
Negative temperatures
Main article: Negative temperature

Temperatures that are expressed as negative numbers on the familiar Celsius or Fahrenheit scales are simply colder than the zero points of those scales. Certain systems can achieve truly negative temperatures; that is, their thermodynamic temperature (expressed in kelvins) can be of a negative quantity. A system with a truly negative temperature is not colder than absolute zero. Rather, a system with a negative temperature is hotter than any system with a positive temperature, in the sense that if a negative-temperature system and a positive-temperature system come in contact, heat flows from the negative to the positive-temperature system.[11]

Most familiar systems cannot achieve negative temperatures because adding energy always increases their entropy. However, some systems have a maximum amount of energy that they can hold, and as they approach that maximum energy their entropy actually begins to decrease. Because temperature is defined by the relationship between energy and entropy, such a system's temperature becomes negative, even though energy is being added.[11] As a result, the Boltzmann factor for states of systems at negative temperature increases rather than decreases with increasing state energy. Therefore, no complete system, i.e. including the electromagnetic modes, can have negative temperatures, since there is no highest energy state,[citation needed] so that the sum of the probabilities of the states would diverge for negative temperatures. However, for quasi-equilibrium systems (e.g. spins out of equilibrium with the electromagnetic field) this argument does not apply, and negative effective temperatures are attainable.

On 3 January 2013, physicists announced that for the first time they had created a quantum gas made up of potassium atoms with a negative temperature in motional degrees of freedom.[12]
History
Robert Boyle pioneered the idea of an absolute zero

One of the first to discuss the possibility of an absolute minimal temperature was Robert Boyle. His 1665 New Experiments and Observations touching Cold, articulated the dispute known as the primum frigidum.[13] The concept was well known among naturalists of the time. Some contended an absolute minimum temperature occurred within earth (as one of the four classical elements), others within water, others air, and some more recently within nitre. But all of them seemed to agree that, "There is some body or other that is of its own nature supremely cold and by participation of which all other bodies obtain that quality."[14]
Limit to the "degree of cold"

The question whether there is a limit to the degree of coldness possible, and, if so, where the zero must be placed, was first addressed by the French physicist Guillaume Amontons in 1702, in connection with his improvements in the air thermometer. His instrument indicated temperatures by the height at which a certain mass of air sustained a column of mercury—the volume, or "spring" of the air varying with temperature. Amontons therefore argued that the zero of his thermometer would be that temperature at which the spring of the air was reduced to nothing. He used a scale that marked the boiling point of water at +73 and the melting point of ice at +51+1⁄2, so that the zero was equivalent to about −240 on the Celsius scale.[15] Amontons held that the absolute zero cannot be reached, so never attempted to compute it explicitly.[16] The value of −240 °C, or "431 divisions [in Fahrenheit's thermometer] below the cold of freezing water"[17] was published by George Martine in 1740.

This close approximation to the modern value of −273.15 °C[1] for the zero of the air thermometer was further improved upon in 1779 by Johann Heinrich Lambert, who observed that −270 °C (−454.00 °F; 3.15 K) might be regarded as absolute cold.[18]

Values of this order for the absolute zero were not, however, universally accepted about this period. Pierre-Simon Laplace and Antoine Lavoisier, in their 1780 treatise on heat, arrived at values ranging from 1,500 to 3,000 below the freezing point of water, and thought that in any case it must be at least 600 below. John Dalton in his Chemical Philosophy gave ten calculations of this value, and finally adopted −3,000 °C as the natural zero of temperature.
Charles's law

From 1787 to 1802, it was determined by Jacques Charles (unpublished), John Dalton,[19] and Joseph Louis Gay-Lussac[20] that, at constant pressure, ideal gases expanded or contracted their volume linearly (Charles's law) by about 1/273 parts per degree Celsius of temperature's change up or down, between 0° and 100° C. This suggested that the volume of a gas cooled at about −273 °C would reach zero.
Lord Kelvin's work

After James Prescott Joule had determined the mechanical equivalent of heat, Lord Kelvin approached the question from an entirely different point of view, and in 1848 devised a scale of absolute temperature that was independent of the properties of any particular substance and was based on Carnot's theory of the Motive Power of Heat and data published by Henri Victor Regnault.[21] It followed from the principles on which this scale was constructed that its zero was placed at −273 °C, at almost precisely the same point as the zero of the air thermometer,[15] where the air volume would reach "nothing". This value was not immediately accepted; values ranging from −271.1 °C (−455.98 °F) to −274.5 °C (−462.10 °F), derived from laboratory measurements and observations of astronomical refraction, remained in use in the early 20th century.[22]
The race to absolute zero
See also: Timeline of low-temperature technology
Commemorative plaque in Leiden

With a better theoretical understanding of absolute zero, scientists were eager to reach this temperature in the lab.[23] By 1845, Michael Faraday had managed to liquefy most gases then known to exist, and reached a new record for lowest temperatures by reaching −130 °C (−202 °F; 143 K). Faraday believed that certain gases, such as oxygen, nitrogen, and hydrogen, were permanent gases and could not be liquefied.[24] Decades later, in 1873 Dutch theoretical scientist Johannes Diderik van der Waals demonstrated that these gases could be liquefied, but only under conditions of very high pressure and very low temperatures. In 1877, Louis Paul Cailletet in France and Raoul Pictet in Switzerland succeeded in producing the first droplets of liquid air −195 °C (−319.0 °F; 78.1 K). This was followed in 1883 by the production of liquid oxygen −218 °C (−360.4 °F; 55.1 K) by the Polish professors Zygmunt Wróblewski and Karol Olszewski.

Scottish chemist and physicist James Dewar and Dutch physicist Heike Kamerlingh Onnes took on the challenge to liquefy the remaining gases, hydrogen and helium. In 1898, after 20 years of effort, Dewar was the first to liquefy hydrogen, reaching a new low-temperature record of −252 °C (−421.6 °F; 21.1 K). However, Kamerlingh Onnes, his rival, was the first to liquefy helium, in 1908, using several precooling stages and the Hampson–Linde cycle. He lowered the temperature to the boiling point of helium −269 °C (−452.20 °F; 4.15 K). By reducing the pressure of the liquid helium, he achieved an even lower temperature, near 1.5 K. These were the coldest temperatures achieved on Earth at the time and his achievement earned him the Nobel Prize in 1913.[25] Kamerlingh Onnes would continue to study the properties of materials at temperatures near absolute zero, describing superconductivity and superfluids for the first time.
Very low temperatures
The rapid expansion of gases leaving the Boomerang Nebula, a bi-polar, filamentary, likely proto-planetary nebula in Centaurus, has a temperature of 1 K, the lowest observed outside of a laboratory.

The average temperature of the universe today is approximately 2.73 kelvins (−454.76 °F), or about −270.42 °C, based on measurements of cosmic microwave background radiation.[26][27] Standard models of the future expansion of the universe predict that the average temperature of the universe is decreasing over time.[28] This temperature is calculated as the mean density of energy in space; it should not be confused with the mean electron temperature (total energy divided by particle count) which has increased over time.[29]

Absolute zero cannot be achieved, although it is possible to reach temperatures close to it through the use of evaporative cooling, cryocoolers, dilution refrigerators,[30] and nuclear adiabatic demagnetization. The use of laser cooling has produced temperatures of less than a billionth of a kelvin.[31] At very low temperatures in the vicinity of absolute zero, matter exhibits many unusual properties, including superconductivity, superfluidity, and Bose–Einstein condensation. To study such phenomena, scientists have worked to obtain even lower temperatures.

    In November 2000, nuclear spin temperatures below 100 pK were reported for an experiment at the Helsinki University of Technology's Low Temperature Lab in Espoo, Finland. However, this was the temperature of one particular degree of freedom—a quantum property called nuclear spin—not the overall average thermodynamic temperature for all possible degrees in freedom.[32][33]
    In February 2003, the Boomerang Nebula was observed to have been releasing gases at a speed of 500,000 km/h (310,000 mph) for the last 1,500 years. This has cooled it down to approximately 1 K, as deduced by astronomical observation, which is the lowest natural temperature ever recorded.[34]
    In November 2003, 90377 Sedna was discovered and is one of the coldest known objects in the Solar System. With an average surface temperature of -400°F (-240°C),[35] due to its extremely far orbit of 903 astronomical units.
    In May 2005, the European Space Agency proposed research in space to achieve femtokelvin temperatures.[36]
    In May 2006, the Institute of Quantum Optics at the University of Hannover gave details of technologies and benefits of femtokelvin research in space.[37]
    In January 2013, physicist Ulrich Schneider of the University of Munich in Germany reported to have achieved temperatures formally below absolute zero ("negative temperature") in gases. The gas is artificially forced out of equilibrium into a high potential energy state, which is, however, cold. When it then emits radiation it approaches the equilibrium, and can continue emitting despite reaching formal absolute zero; thus, the temperature is formally negative.[38]
    In September 2014, scientists in the CUORE collaboration at the Laboratori Nazionali del Gran Sasso in Italy cooled a copper vessel with a volume of one cubic meter to 0.006 kelvins (−273.144 °C; −459.659 °F) for 15 days, setting a record for the lowest temperature in the known universe over such a large contiguous volume.[39]
    In June 2015, experimental physicists at MIT cooled molecules in a gas of sodium potassium to a temperature of 500 nanokelvin, and it is expected to exhibit an exotic state of matter by cooling these molecules somewhat further.[40]
    In 2017, Cold Atom Laboratory (CAL), an experimental instrument was developed for launch to the International Space Station (ISS) in 2018.[41] The instrument has created extremely cold conditions in the microgravity environment of the ISS leading to the formation of Bose–Einstein condensates. In this space-based laboratory, temperatures as low as 1 picokelvin (10−12 K) temperatures are projected to be achievable, and it could further the exploration of unknown quantum mechanical phenomena and test some of the most fundamental laws of physics.[42][43]
    The current world record for effective temperatures was set in 2021 at 38 picokelvin (pK), or 0.000000000038 of a kelvin, through matter-wave lensing of rubidium Bose–Einstein condensates.[44]

See also

    iconPhysics portal

    Kelvin (unit of temperature)
    Charles's law
    Heat
    International Temperature Scale of 1990
    Orders of magnitude (temperature)
    Thermodynamic temperature
    Triple point
    Ultracold atom
    Kinetic energy
    Entropy
    Planck temperature and Hagedorn temperature, hypothetical upper limits to the thermodynamic temperature scale

References

"Unit of thermodynamic temperature (kelvin)". SI Brochure, 8th edition. Bureau International des Poids et Mesures. 13 March 2010 [1967]. Section 2.1.1.5. Archived from the original on 7 October 2014. Retrieved 20 June 2017. Note: The triple point of water is 0.01 °C, not 0 °C; thus 0 K is −273.15 °C, not −273.16 °C.
Arora, C. P. (2001). Thermodynamics. Tata McGraw-Hill. Table 2.4 page 43. ISBN 978-0-07-462014-4.
"SI Brochure: The International System of Units (SI)". Bureau international des poids et mesures. Retrieved 8 February 2022.
Zielinski, Sarah (1 January 2008). "Absolute Zero". Smithsonian Institution. Archived from the original on 1 April 2013. Retrieved 26 January 2012.
Masanes, Lluís; Oppenheim, Jonathan (14 March 2017), "A general derivation and quantification of the third law of thermodynamics", Nature Communications, 8 (14538): 14538, arXiv:1412.3828, Bibcode:2017NatCo...814538M, doi:10.1038/ncomms14538, PMC 5355879, PMID 28290452
Donley, Elizabeth A.; Claussen, Neil R.; Cornish, Simon L.; Roberts, Jacob L.; Cornell, Eric A.; Wieman, Carl E. (2001). "Dynamics of collapsing and exploding Bose–Einstein condensates". Nature. 412 (6844): 295–299. arXiv:cond-mat/0105019. Bibcode:2001Natur.412..295D. doi:10.1038/35085500. PMID 11460153. S2CID 969048.
Clark, Ronald W. "Einstein: The Life and Times" (Avon Books, 1971) pp. 408–9 ISBN 0-380-01159-X
"New State of Matter Seen Near Absolute Zero". NIST. Archived from the original on 1 June 2010.
Levi, Barbara Goss (2001). "Cornell, Ketterle, and Wieman Share Nobel Prize for Bose–Einstein Condensates". Search & Discovery. Physics Today online. Archived from the original on 24 October 2007. Retrieved 26 January 2008.
Leanhardt, A. E.; Pasquini, TA; Saba, M; Schirotzek, A; Shin, Y; Kielpinski, D; Pritchard, DE; Ketterle, W (2003). "Cooling Bose–Einstein Condensates Below 500 Picokelvin" (PDF). Science. 301 (5639): 1513–1515. Bibcode:2003Sci...301.1513L. doi:10.1126/science.1088827. PMID 12970559. S2CID 30259606. Archived (PDF) from the original on 9 October 2022.
Chase, Scott. "Below Absolute Zero -What Does Negative Temperature Mean?". The Physics and Relativity FAQ. Archived from the original on 15 August 2011. Retrieved 2 July 2010.
Merali, Zeeya (2013). "Quantum gas goes below absolute zero". Nature. doi:10.1038/nature.2013.12146. S2CID 124101032.
Stanford, John Frederick (1892). The Stanford Dictionary of Anglicised Words and Phrases.
Boyle, Robert (1665). New Experiments and Observations touching Cold.
Chisholm, Hugh, ed. (1911). "Cold" . Encyclopædia Britannica (11th ed.). Cambridge University Press.
Talbot, G.R.; Pacey, A.C. (1972). "Antecedents of thermodynamics in the work of Guillaume Amontons". Centaurus. 16 (1): 20–40. Bibcode:1972Cent...16...20T. doi:10.1111/j.1600-0498.1972.tb00163.x.
Essays Medical and Philosophical, p. PA291, at Google Books
Lambert, Johann Heinrich (1779). Pyrometrie. Berlin. OCLC 165756016.
J. Dalton (1802), "Essay II. On the force of steam or vapour from water and various other liquids, both in vacuum and in air" and Essay IV. "On the expansion of elastic fluids by heat," Memoirs of the Literary and Philosophical Society of Manchester, vol. 8, pt. 2, pp. 550–74, 595–602.
Gay-Lussac, J. L. (1802), "Recherches sur la dilatation des gaz et des vapeurs", Annales de Chimie, XLIII: 137. English translation (extract).
Thomson, William (1848). "On an Absolute Thermometric Scale founded on Carnot's Theory of the Motive Power of Heat, and calculated from Regnault's observations". Proceedings of the Cambridge Philosophical Society. 1: 66–71.
Newcomb, Simon (1906), A Compendium of Spherical Astronomy, New York: The Macmillan Company, p. 175, OCLC 64423127
"ABSOLUTE ZERO – PBS NOVA DOCUMENTARY (full length)". YouTube. Archived from the original on 6 April 2017. Retrieved 23 November 2016.
Cryogenics. Scienceclarified.com. Retrieved on 22 July 2012.
"The Nobel Prize in Physics 1913: Heike Kamerlingh Onnes". Nobel Media AB. Retrieved 24 April 2012.
Kruszelnicki, Karl S. (25 September 2003). "Coldest Place in the Universe 1". Australian Broadcasting Corporation. Retrieved 24 September 2012.
"What's the temperature of space?". The Straight Dope. 3 August 2004. Retrieved 24 September 2012.
John, Anslyn J. (25 August 2021). "The building blocks of the universe". HTS Teologiese Studies/Theological Studies. 77 (3). doi:10.4102/hts.v77i3.6831. S2CID 238730757.
"History of temperature changes in the Universe revealed—First measurement using the Sunyaev-Zeldovich effect". Kavli Institute for the Physics and Mathematics of the Universe. 10 November 2020.
Zu, H.; Dai, W.; de Waele, A.T.A.M. (2022). "Development of Dilution refrigerators – A review". Cryogenics. 121. Bibcode:2022Cryo..121....1Z. doi:10.1016/j.cryogenics.2021.103390. ISSN 0011-2275. S2CID 244005391.
Catchpole, Heather (4 September 2008). "Cosmos Online – Verging on absolute zero". Archived from the original on 22 November 2008.
Knuuttila, Tauno (2000). Nuclear Magnetism and Superconductivity in Rhodium. Espoo, Finland: Helsinki University of Technology. ISBN 978-951-22-5208-4. Archived from the original on 28 April 2001. Retrieved 11 February 2008.
"Low Temperature World Record" (Press release). Low Temperature Laboratory, Teknillinen Korkeakoulu. 8 December 2000. Archived from the original on 18 February 2008. Retrieved 11 February 2008.
Sahai, Raghvendra; Nyman, Lars-Åke (1997). "The Boomerang Nebula: The Coldest Region of the Universe?". The Astrophysical Journal. 487 (2): L155–L159. Bibcode:1997ApJ...487L.155S. doi:10.1086/310897. hdl:2014/22450. S2CID 121465475.
"Mysterious Sedna | Science Mission Directorate". science.nasa.gov. Retrieved 25 November 2022.
"Scientific Perspectives for ESA's Future Programme in Life and Physical sciences in Space" (PDF). esf.org. Archived from the original (PDF) on 6 October 2014. Retrieved 28 March 2014.
"Atomic Quantum Sensors in Space" (PDF). University of California, Los Angeles. Archived (PDF) from the original on 9 October 2022.
"Atoms Reach Record Temperature, Colder than Absolute Zero". livescience.com. 3 January 2013.
"CUORE: The Coldest Heart in the Known Universe". INFN Press Release. Retrieved 21 October 2014.
"MIT team creates ultracold molecules". Massachusetts Institute of Technology, Massachusetts, Cambridge. Archived from the original on 18 August 2015. Retrieved 10 June 2015.
"Coolest science ever headed to the space station". Science | AAAS. 5 September 2017. Retrieved 24 September 2017.
"Cold Atom Laboratory Mission". Jet Propulsion Laboratory. NASA. 2017. Archived from the original on 29 March 2013. Retrieved 22 December 2016.
"Cold Atom Laboratory Creates Atomic Dance". NASA News. 26 September 2014. Retrieved 21 May 2015.

    Deppner, Christian; Herr, Waldemar; Cornelius, Merle; Stromberger, Peter; Sternke, Tammo; Grzeschik, Christoph; Grote, Alexander; Rudolph, Jan; Herrmann, Sven; Krutzik, Markus; Wenzlawski, André (30 August 2021). "Collective-Mode Enhanced Matter-Wave Optics". Physical Review Letters. 127 (10): 100401. Bibcode:2021PhRvL.127j0401D. doi:10.1103/PhysRevLett.127.100401. ISSN 0031-9007. PMID 34533345. S2CID 237396804.

Further reading

    Herbert B. Callen (1960). "Chapter 10". Thermodynamics. New York: John Wiley & Sons. ISBN 978-0-471-13035-2. OCLC 535083.
    Herbert B. Callen (1985). Thermodynamics and an Introduction to Thermostatistics (Second ed.). New York: John Wiley & Sons. ISBN 978-0-471-86256-7.
    E.A. Guggenheim (1967). Thermodynamics: An Advanced Treatment for Chemists and Physicists (Fifth ed.). Amsterdam: North Holland Publishing. ISBN 978-0-444-86951-7. OCLC 324553.
    George Stanley Rushbrooke (1949). Introduction to Statistical Mechanics. Oxford: Clarendon Press. OCLC 531928.
    BIPM Mise en pratique - Kelvin - Appendix 2 - SI Brochure

External links

    "Absolute zero": a two part NOVA episode originally aired January 2008
    "What is absolute zero?" Lansing State Journal

Portals:

    icon Physics
     Chemistry

Authority control: National Edit this at Wikidata	

    Germany

Categories:

    ColdCryogenicsTemperature

    This page was last edited on 19 July 2023, at 00:37 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
History

    Precursors
    Charles's law
    Lord Kelvin
    Triple point standard
    2019 redefinition

Practical uses

        Colour temperature
        Kelvin as a unit of noise temperature
    Derived units and SI multiples
    Orthography
    See also
    Notes
    References
    Bibliography
    External links

Kelvin

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
This article is about the unit of temperature. For other uses, see Kelvin (disambiguation).
kelvin
Thermometer with markings in degrees Celsius and in kelvins
General information
Unit system	SI
Unit of	temperature
Symbol	K
Named after	William Thomson, 1st Baron Kelvin
Conversions
x K in ...	... corresponds to ...
   Celsius	   (x − 273.15) °C
   Fahrenheit	   (1.8 x − 459.67) °F
   Rankine	   1.8 x °Ra

The kelvin, symbol K, is a unit of measurement for temperature.[1] The Kelvin scale is an absolute scale, which is defined such that 0 K is absolute zero and a change of thermodynamic temperature T by 1 kelvin corresponds to a change of thermal energy kT by 1.380649×10−23 J. The Boltzmann constant k = 1.380649×10−23 J⋅K−1 was exactly defined in the 2019 redefinition of the SI base units such that the triple point of water is 273.16±0.0001 K.[2] The kelvin is the base unit of temperature in the International System of Units (SI), used alongside its prefixed forms.[2][3][4] It is named after the Belfast-born and University of Glasgow-based engineer and physicist William Thomson, 1st Baron Kelvin (1824–1907).[5]

Historically, the Kelvin scale was developed from the Celsius scale, such that 273.15 K was 0 °C (the approximate melting point of ice) and a change of one kelvin was exactly equal to a change of one degree Celsius.[1][5] This relationship remains accurate, but the Celsius, Fahrenheit, and Rankine scales are now defined in terms of the Kelvin scale.[2][6][7] The kelvin is the primary unit of temperature for engineering and the physical sciences, while in most countries the Celsius scale remains the dominant scale outside of these fields.[5] In the United States, outside of the physical sciences, the Fahrenheit scale predominates, with the kelvin or Rankine scale employed for absolute temperature.[6]
History
See also: Thermodynamic temperature § History
Precursors
An ice water bath offered a practical calibration point for thermometers in a time before the physical nature of heat was well understood.

During the 18th century, multiple temperature scales were developed,[8] notably Fahrenheit and centigrade (later Celsius). These scales predated much of the modern science of thermodynamics, including atomic theory and the kinetic theory of gases which underpin the concept of absolute zero. Instead, they chose defining points within the range of human experience that could be reproduced easily and with reasonable accuracy, but lacked any deep significance in thermal physics. In the case of the Celsius scale (and the long since defunct Newton scale and Réaumur scale) the melting point of water served as such a starting point, with Celsius being defined, from the 1740s up until the 1940s, by calibrating a thermometer such that

    The freezing point of water is 0 degrees.
    The boiling point of water is 100 degrees.

This definition assumes pure water at a specific pressure chosen to approximate the natural air pressure at sea level. Thus an increment of 1 °C equals 1/100 of the temperature difference between the melting and boiling points. This temperature interval would go on to become the template for the kelvin.[citation needed]
Charles's law

From 1787 to 1802, it was determined by Jacques Charles (unpublished), John Dalton,[9][10] and Joseph Louis Gay-Lussac[11] that, at constant pressure, ideal gases expanded or contracted their volume linearly (Charles's law) by about 1/273 parts per degree Celsius of temperature's change up or down, between 0° and 100° C. This suggested that the volume of a gas cooled at about −273 °C would reach zero.
Lord Kelvin
Lord Kelvin, the namesake of the unit of measure.

In 1848, William Thomson, who was later ennobled as Lord Kelvin, published a paper On an Absolute Thermometric Scale.[12][13][14] Using the soon-to-be-defunct caloric theory, he proposed an "absolute" scale based on the following parameters:

    The melting point of water is 0 degrees.
    The boiling point of water is 100 degrees.

"The arbitrary points which coincide on the two scales are 0° and 100°"

    Any two heat engines whose heat source and heat sink are both separated by the same number of degrees will, per Carnot's theorem, be capable of producing the same amount of mechanical work per unit of "caloric" passing through.

"The characteristic property of the scale which I now propose is, that all degrees have the same value; that is, that a unit of heat descending from a body A at the temperature T° of this scale, to a body B at the temperature (T − 1)°, would give out the same mechanical effect, whatever be the number T. This may justly be termed an absolute scale, since its characteristic is quite independent of the physical properties of any specific substance."

As Carnot's theorem is understood in modern thermodynamics to simply describe the maximum efficiency with which thermal energy can be converted to mechanical energy and the predicted maximum efficiency is a function of the ratio between the absolute temperatures of the heat source and heat sink:

    Efficiency ≤ 1 − absolute temperate of heat sink/absolute temperature of heat source

It follows that increments of equal numbers of degrees on this scale must always represent equal proportional increases in absolute temperature. The numerical value of an absolute temperature, T, on the 1848 scale is related to the absolute temperature of the melting point of water, Tmpw, and the absolute temperature of the boiling point of water, Tbpw, by

    T (1848 scale) = 100 (ln T/Tmpw) / (ln Tbpw/Tmpw)

On this scale, an increase of 222 degrees always means an approximate doubling of absolute temperature regardless of the starting temperature.

In a footnote Thomson calculated that "infinite cold" (absolute zero, which would have a numerical value of negative infinity on this scale) was equivalent to −273 °C using the air thermometers of the time. This value of "−273" was the negative reciprocal of 0.00366—the accepted coefficient of thermal expansion of an ideal gas per degree Celsius relative to the ice point, giving a remarkable consistency to the currently accepted value.[citation needed]

Within a decade, Thomson had abandoned caloric theory and superseded the 1848 scale with a new one[13][15] based on the 2 features that would characterise all future versions of the Kelvin scale:

    Absolute zero is the null point.
    Increments have the same magnitude as they do in the Celsius scale.

In 1892, Thomson was awarded the noble title 1st Baron Kelvin of Largs, or more succinctly Lord Kelvin. This name was a reference to the River Kelvin which flows through the grounds of Glasgow University.

In the early decades of the 20th century, the Kelvin scale was often called the "absolute Celsius" scale, indicating Celsius degrees counted from absolute zero rather than the freezing point of water, and using the same symbol for regular Celsius degrees, °C.[16]
Triple point standard
A typical phase diagram. The solid green line applies to most substances; the dashed green line gives the anomalous behavior of water. The boiling line (solid blue) runs from the triple point to the critical point, beyond which further increases in temperature and pressure produce a supercritical fluid.

In 1873, William Thomson's older brother James coined the term triple point[17] to describe the combination of temperature and pressure at which the solid, liquid, and gas phases of a substance were capable of coexisting in thermodynamic equilibrium. While any two phases could coexist along a range of temperature-pressure combinations (e.g. the boiling point of water can be affected quite dramatically by raising or lowering the pressure), the triple point condition for a given substance can occur only at a single pressure and only at a single temperature. By the 1940s, the triple point of water had been experimentally measured to be about 0.6% of standard atmospheric pressure and very close to 0.01 °C per the historical definition of Celsius then in use.

In 1948, the Celsius scale was recalibrated by assigning the triple point temperature of water the value of 0.01 °C exactly[18] and allowing the melting point at standard atmospheric pressure to have an empirically determined value (and the actual melting point at ambient pressure to have a fluctuating value) close to 0 °C. This was justified on the grounds that the triple point was judged to give a more accurately reproducible reference temperature than the melting point.[19] The triple point could be measured with ±0.0001 °C accuracy, while the melting point just to ±0.001 °C.[18]

In 1954, with absolute zero having been experimentally determined to be about −273.15 °C per the definition of °C then in use, Resolution 3 of the 10th General Conference on Weights and Measures (CGPM) introduced a new internationally standardised Kelvin scale which defined the triple point as exactly 273.15 + 0.01 = 273.16 degrees Kelvin.[20][21]

In 1967/1968, Resolution 3 of the 13th CGPM renamed the unit increment of thermodynamic temperature "kelvin", symbol K, replacing "degree Kelvin", symbol °K.[22][23][24] The 13th CGPM also held in Resolution 4 that "The kelvin, unit of thermodynamic temperature, is equal to the fraction 1/273.16 of the thermodynamic temperature of the triple point of water."[4][25][26]

After the 1983 redefinition of the metre, this left the kelvin, the second, and the kilogram as the only SI units not defined with reference to any other unit.

In 2005, noting that the triple point could be influenced by the isotopic ratio of the hydrogen and oxygen making up a water sample and that this was "now one of the major sources of the observed variability between different realizations of the water triple point", the International Committee for Weights and Measures (CIPM), a committee of the CGPM, affirmed that for the purposes of delineating the temperature of the triple point of water, the definition of the kelvin would refer to water having the isotopic composition specified for Vienna Standard Mean Ocean Water.[4][27][28]
2019 redefinition
Main article: 2019 redefinition of the SI base units
The kelvin is now fixed in terms of the Boltzmann constant and the joule, itself defined by the caesium-133 hyperfine transition frequency and the Planck constant. Both k and kB are accepted shorthand for the Boltzmann constant.

In 2005, the CIPM began a programme to redefine the kelvin (along with the other SI units) using a more experimentally rigorous method. In particular, the committee proposed redefining the kelvin such that the Boltzmann constant takes the exact value 1.3806505×10−23 J/K.[29] The committee had hoped that the program would be completed in time for its adoption by the CGPM at its 2011 meeting, but at the 2011 meeting the decision was postponed to the 2014 meeting when it would be considered as part of a larger program.[30]

The redefinition was further postponed in 2014, pending more accurate measurements of the Boltzmann constant in terms of the current definition,[31] but was finally adopted at the 26th CGPM in late 2018, with a value of k = 1.380649×10−23 J⋅K−1.[32][29][1][2][4][33]

For scientific purposes, the main advantage is that this allows measurements at very low and very high temperatures to be made more accurately, as the techniques used depend on the Boltzmann constant. It also has the philosophical advantage of being independent of any particular substance. The unit J/K is equal to kg⋅m2⋅s−2⋅K−1, where the kilogram, metre and second are defined in terms of the Planck constant, the speed of light, and the duration of the caesium-133 ground-state hyperfine transition respectively.[2] Thus, this definition depends only on universal constants, and not on any physical artifacts as practiced previously. The challenge was to avoid degrading the accuracy of measurements close to the triple point. For practical purposes, the redefinition was unnoticed; water still freezes at 273.15 K (0 °C),[2][34] and the triple point of water continues to be a commonly used laboratory reference temperature.

The difference is that, before the redefinition, the triple point of water was exact and the Boltzmann constant had a measured value of 1.38064903(51)×10−23 J/K, with a relative standard uncertainty of 3.7×10−7.[35] Afterward, the Boltzmann constant is exact and the uncertainty is transferred to the triple point of water, which is now 273.1600(1) K.

The new definition officially came into force on 20 May 2019, the 144th anniversary of the Metre Convention.[33][1][2][4]
Practical uses
Colour temperature
See also: Stefan–Boltzmann constant

The kelvin is often used as a measure of the colour temperature of light sources. Colour temperature is based upon the principle that a black body radiator emits light with a frequency distribution characteristic of its temperature. Black bodies at temperatures below about 4000 K appear reddish, whereas those above about 7500 K appear bluish. Colour temperature is important in the fields of image projection and photography, where a colour temperature of approximately 5600 K is required to match "daylight" film emulsions. In astronomy, the stellar classification of stars and their place on the Hertzsprung–Russell diagram are based, in part, upon their surface temperature, known as effective temperature. The photosphere of the Sun, for instance, has an effective temperature of 5772 K [1][2][3][4] as adopted by IAU 2015 Resolution B3.

Digital cameras and photographic software often use colour temperature in K in edit and setup menus. The simple guide is that higher colour temperature produces an image with enhanced white and blue hues. The reduction in colour temperature produces an image more dominated by reddish, "warmer" colours.
Kelvin as a unit of noise temperature
Main article: Noise figure

For electronics, the kelvin is used as an indicator of how noisy a circuit is in relation to an ultimate noise floor, i.e. the noise temperature. The so-called Johnson–Nyquist noise of discrete resistors and capacitors is a type of thermal noise derived from the Boltzmann constant and can be used to determine the noise temperature of a circuit using the Friis formulas for noise.
Derived units and SI multiples
Main article: Orders of magnitude (temperature)

The only SI derived unit with a special name derived from the kelvin is the degree Celsius. Like other SI units, the kelvin can also be modified by adding a metric prefix that multiplies it by a power of 10:
SI multiples of kelvin (K) Submultiples 		Multiples
Value 	SI symbol 	Name 	Value 	SI symbol 	Name
10−1 K 	dK 	decikelvin 	101 K 	daK 	decakelvin
10−2 K 	cK 	centikelvin 	102 K 	hK 	hectokelvin
10−3 K 	mK 	millikelvin 	103 K 	kK 	kilokelvin
10−6 K 	µK 	microkelvin 	106 K 	MK 	megakelvin
10−9 K 	nK 	nanokelvin 	109 K 	GK 	gigakelvin
10−12 K 	pK 	picokelvin 	1012 K 	TK 	terakelvin
10−15 K 	fK 	femtokelvin 	1015 K 	PK 	petakelvin
10−18 K 	aK 	attokelvin 	1018 K 	EK 	exakelvin
10−21 K 	zK 	zeptokelvin 	1021 K 	ZK 	zettakelvin
10−24 K 	yK 	yoctokelvin 	1024 K 	YK 	yottakelvin
10−27 K 	rK 	rontokelvin 	1027 K 	RK 	ronnakelvin
10−30 K 	qK 	quectokelvin 	1030 K 	QK 	quettakelvin
Orthography

According to SI convention, the kelvin is never referred to nor written as a degree. The word "kelvin" is not capitalised when used as a unit. It may be pluralised as appropriate (for example, "it is 283 kelvins outside", in contrast with "it is 50 degrees Fahrenheit" or "10 degrees Celsius").[36][37][38][a] The unit symbol K is a capital letter.[22] It is common convention to capitalize Kelvin when referring to Lord Kelvin[5] or the Kelvin scale.[39]

The unit symbol K is encoded in Unicode at code point U+212A K KELVIN SIGN. However, this is a compatibility character provided for compatibility with legacy encodings. The Unicode standard recommends using U+004B K LATIN CAPITAL LETTER K instead; that is, a normal capital K. "Three letterlike symbols have been given canonical equivalence to regular letters: U+2126 Ω OHM SIGN, U+212A K KELVIN SIGN, and U+212B Å ANGSTROM SIGN. In all three instances, the regular letter should be used."[40]
See also

    iconEnergy portal

    Comparison of temperature scales
    International Temperature Scale of 1990
    Negative temperature

Notes

    SI Brochure 9 does not specify usage or include a current example.[2]

References

BIPM (2019-05-20). "Mise en pratique for the definition of the kelvin in the SI". BIPM.org. Retrieved 2022-02-18.
"SI Brochure: The International System of Units (SI) – 9th edition". BIPM. Retrieved 2022-02-21.
"SI base unit: kelvin (K)". bipm.org. BIPM. Retrieved 2022-03-05.
"A Turning Point for Humanity: Redefining the World's Measurement System". Nist. 2018-05-12. Retrieved 2022-02-21.
"Kelvin: Introduction". NIST. 2018-05-14. Retrieved 2022-09-02.
Benham, Elizabeth (2020-10-06). "Busting Myths about the Metric System". Nist. Taking Measure (official blog of the NIST). Retrieved 2022-02-21.
"Handbook 44 – 2022 – Appendix C – General Tables of Units of Measurement" (PDF). nist.gov. NIST. Retrieved 2022-02-21.
"Kelvin: History". Nist. 2018-05-14. Retrieved 2022-02-21.
Dalton, John (1801). "Essay II. On the force of steam or vapour from water and various other liquids, both in vacuum and in air". Memoirs of the Literary and Philosophical Society of Manchester. 5 part 2: 550–574.
Dalton, John (1801). "Essay IV. On the expansion of elastic fluids by heat". Memoirs of the Literary and Philosophical Society of Manchester. 5 part 2: 595–602.
Gay-Lussac, Joseph Louis (1802), "Recherches sur la dilatation des gaz et des vapeurs", Annales de Chimie, XLIII: 137. English translation (extract).
Thomson, William. "On an Absolute Thermometric Scale founded on Carnot's Theory of the Motive Power of Heat, and calculated from Regnault's Observations". zapatopi.net. Philosophical Magazine. Retrieved 2022-02-21.
Thomson, William. "On an Absolute Thermometric Scale founded on Carnot's Theory of the Motive Power of Heat, and calculated from Regnault's Observations (1881 reprint)" (PDF). Philosophical Magazine. Retrieved 2022-02-21.
Kelvin, William (October 1848). "On an Absolute Thermometric Scale". Philosophical Magazine. Archived from the original on 2008-02-01. Retrieved 2008-02-06.
Thomson, William. "On the Dynamical Theory of Heat, with numerical results deduced from Mr Joule's equivalent of a Thermal Unit, and M. Regnault's Observations on Steam (Excerpts)". Zapatopi.net. Transactions of the Royal Society of Edinburgh and Philosophical Magazine. Retrieved 2022-02-21.
For example, Encyclopaedia Britannica editions from the 1920s and 1950s, one example being the article "Planets".
Thomson, James (1873). "A quantitative investigation of certain relations between the gaseous, the liquid, and the solid states of water-substance". Proceedings of the Royal Society of London. 22: 28. Bibcode:1873RSPS...22...27T. ISSN 0370-1662. "and consequently that the three curves would meet or cross each other in one point, which I have called the triple point."
Swinton, F. L. (September 1967). "The triplet point of water". Journal of Chemical Education. 44 (9): 541. doi:10.1021/ed044p541. ISSN 0021-9584.
"Resolution 3 of the 9th CGPM (1948)". bipm.org. BIPM. Retrieved 2022-02-21.
"Resolution 3 of the 10th CGPM (1954)". bipm.org. BIPM. Retrieved 2022-02-21.
"Resolution 3: Definition of the thermodynamic temperature scale". Resolutions of the 10th CGPM. Bureau International des Poids et Mesures. 1954. Archived from the original on 2007-06-23. Retrieved 2008-02-06.
"Resolution 3 of the 13th CGPM (1967)". bipm.org. BIPM. Retrieved 2022-02-21.
"Resolution 3: SI unit of thermodynamic temperature (kelvin)". Resolutions of the 13th CGPM. Bureau International des Poids et Mesures. 1967. Archived from the original on 2007-04-21. Retrieved 2008-02-06.
Westphal, Wilhelm Heinrich (1952). "Nox, Dunkelleuchtdichte, Skot". Physikalisches Wörterbuch (in German) (1 ed.). Berlin / Göttingen / Heidelberg, Germany: Springer-Verlag OHG. pp. 125, 271, 389. doi:10.1007/978-3-662-12706-3. ISBN 978-3-662-12707-0. Retrieved 2023-03-16. pp. 271, 389: "Dunkelleuchtdichte. […] Unter Zugrundelegung dieser Empfindlichkeitskurve hat man 1940 in Deutschland die Dunkelleuchtdichte mit der Einheit Skot (sk) so festgesetzt, daß bei einem Licht der Farbtemperatur 2360 °K 1 sk = 10−3 asb gilt. 1948 ist von der Internationalen Beleuchtungskommission (IBK) die Bezugstemperatur auf 2046 °K, die Erstarrungstemperatur des Platins, festgesetzt worden. Die Bezeichnung Skot wurde von der IBK nicht übernommen, dafür soll "skotopisches Stilb" gesagt werden. Als höchstzulässiger Grenzwert für die Dunkelleuchtdichte ist in Deutschland 10 Skot festgesetzt worden, um eine Verwendung der Dunkelleuchtdichte im Gebiet des gemischten Zapfen- und Stäbchensehens zu vermeiden, da in diesem Bereich die photometrischen Maßgrößen wegen der allmählich gleitenden Augenempfindlichkeitskurve ihren Sinn verlieren. […] Skot, abgek[ürzt] sk, Einheit für die Dunkelleuchtdichte, welche für zahlenmäßige Angaben und zum Anschluß der Dunkelleuchtdichte an die normale Leuchtdichte 1940 von der Deutschen Lichttechnischen Gesellschaft [de] geschaffen wurde. Für diesen Anschluß wurde die Strahlung des schwarzen Körpers bei T = 2360 °K, d.h. eine Strahlung der Farbtemperatur T1 = 2360 °K vereinbart. Eine Lichtquelle strahlt mit der Dunkelleuchtdichte 1 sk, wenn sie photometrisch gleich einer Strahlung der Farbtemperatur T2 = 2360 °K und der Leuchtdichte von 10−3 asb (Apostilb) ist. Bei der Farbtemperatur T1 = 2360 °K gilt also die Relation: 1 sk = 10−3 asb = 10−7/π sb."
"Resolution 4 of the 13th CGPM (1967)". bipm.org. BIPM. Retrieved 2022-02-21.
"Resolution 4: Definition of the SI unit of thermodynamic temperature (kelvin)". Resolutions of the 13th CGPM. Bureau International des Poids et Mesures. 1967. Archived from the original on 2007-06-15. Retrieved 2008-02-06.
"Resolution 10 of the 23rd CGPM (2007)". bipm.org. BIPM. Retrieved 2022-02-21.
"Unit of thermodynamic temperature (kelvin)". SI Brochure, 8th edition. Bureau International des Poids et Mesures. 1967. pp. Section 2.1.1.5. Archived from the original on 2007-09-26. Retrieved 2008-02-06.
Ian Mills (2010-09-29). "Draft Chapter 2 for SI Brochure, following redefinitions of the base units" (PDF). CCU. Archived (PDF) from the original on 2011-01-10. Retrieved 2011-01-01.
"General Conference on Weights and Measures approves possible changes to the International System of Units, including redefinition of the kilogram" (PDF) (Press release). Sèvres, France: General Conference on Weights and Measures. 2011-10-23. Archived (PDF) from the original on 2012-02-09. Retrieved 2011-10-25.
Wood, B. (3–4 November 2014). "Report on the Meeting of the CODATA Task Group on Fundamental Constants" (PDF). BIPM. p. 7. Archived (PDF) from the original on 2015-10-13. "[BIPM director Martin] Milton responded to a question about what would happen if ... the CIPM or the CGPM voted not to move forward with the redefinition of the SI. He responded that he felt that by that time the decision to move forward should be seen as a foregone conclusion."
"2018 CODATA Value: Boltzmann constant". The NIST Reference on Constants, Units, and Uncertainty. NIST. 2019-05-20. Retrieved 2019-05-20.
"Resolution 1 of the 26th CGPM (2018)". bipm.org. BIPM. Retrieved 2022-02-21.
"Updating the definition of the kelvin" (PDF). International Bureau for Weights and Measures (BIPM). Archived (PDF) from the original on 2008-11-23. Retrieved 2010-02-23.
Newell, D B; Cabiati, F; Fischer, J; Fujii, K; Karshenboim, S G; Margolis, H S; de Mirandés, E; Mohr, P J; Nez, F; Pachucki, K; Quinn, T J; Taylor, B N; Wang, M; Wood, B M; Zhang, Z; et al. (Committee on Data for Science and Technology (CODATA) Task Group on Fundamental Constants) (2018-01-29). "The CODATA 2017 values of h, e, k, and NA for the revision of the SI". Metrologia. 55 (1): L13–L16. Bibcode:2018Metro..55L..13N. doi:10.1088/1681-7575/aa950a.
"Kelvin: Introduction". www.nist.gov. Retrieved 2023-08-21.
"Definition of KELVIN". www.merriam-webster.com. Retrieved 2023-08-21.
CERN English Language Style Guide (PDF). CERN. 2022. p. 64.
Brady, James E.; Senese, Fred (2008-01-28). Chemistry, Student Study Guide: The Study of Matter and Its Changes. John Wiley & Sons. p. 15. ISBN 978-0-470-18464-6.

    "22.2". The Unicode Standard, Version 8.0 (PDF). Mountain View, CA, USA: The Unicode Consortium. August 2015. ISBN 978-1-936213-10-8. Archived (PDF) from the original on 2016-12-06. Retrieved 2015-09-06.

Bibliography

    Bureau International des Poids et Mesures (2019). "The International System of Units (SI) Brochure" (PDF). 9th Edition. International Committee for Weights and Measures. Retrieved 2022-04-28.

External links
Look up kelvin in Wiktionary, the free dictionary.

    vte

Scales of temperature

    vte

SI units

    vte

CGS units
Categories:

    1848 introductionsScottish inventionsSI base unitsWilliam Thomson, 1st Baron KelvinScales of temperatureScales in meteorology

    This page was last edited on 26 August 2023, at 18:37 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Overview

Absolute zero of temperature

Boltzmann constant

Rankine scale

Modern redefinition of the kelvin

Relationship of temperature, motions, conduction, and thermal energy

        Nature of kinetic energy, translational motion, and temperature
            High speeds of translational motion
            Internal motions of molecules and internal energy
        Diffusion of thermal energy: entropy, phonons, and mobile conduction electrons
        Diffusion of thermal energy: black-body radiation
            Table of thermodynamic temperatures
        Heat of phase changes
        Internal energy
        Internal energy at absolute zero
    Practical applications for thermodynamic temperature
    Relationship to ideal gas law
    History
    See also
    Notes
    External links

Thermodynamic temperature

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
Thermodynamics
The classical Carnot heat engine
Branches
Laws
Systems
System properties
Material properties
Equations
Potentials

    HistoryCulture

Scientists
Other

    Category

    vte

Thermodynamic temperature is a quantity defined in thermodynamics as distinct from kinetic theory or statistical mechanics.

Historically, thermodynamic temperature was defined by Lord Kelvin in terms of a macroscopic relation between thermodynamic work and heat transfer as defined in thermodynamics, but the kelvin was redefined by international agreement in 2019 in terms of phenomena that are now understood as manifestations of the kinetic energy of free motion of microscopic particles such as atoms, molecules, and electrons. From the thermodynamic viewpoint, for historical reasons, because of how it is defined and measured, this microscopic kinetic definition is regarded as an "empirical" temperature. It was adopted because in practice it can generally be measured more precisely than can Kelvin's thermodynamic temperature.

A thermodynamic temperature reading of zero is of particular importance for the third law of thermodynamics. By convention, it is reported on the Kelvin scale of temperature in which the unit of measurement is the kelvin (unit symbol: K). For comparison, a temperature of 295 K is equal to 21.85 °C and 71.33 °F.
Overview

Thermodynamic temperature, as distinct from SI temperature, is defined in terms of a macroscopic Carnot cycle. Thermodynamic temperature is of importance in thermodynamics because it is defined in purely thermodynamic terms. SI temperature is conceptually far different from thermodynamic temperature. Thermodynamic temperature was rigorously defined historically long before there was a fair knowledge of microscopic particles such as atoms, molecules, and electrons.

The International System of Units (SI) specifies the international absolute scale for measuring temperature, and the unit of measure kelvin (unit symbol: K) for specific values along the scale. The kelvin is also used for denoting temperature intervals (a span or difference between two temperatures) as per the following example usage: "A 60/40 tin/lead solder is non-eutectic and is plastic through a range of 5 kelvins as it solidifies." A temperature interval of one degree Celsius is the same magnitude as one kelvin.

The magnitude of the kelvin was redefined in 2019 in relation to the physical property underlying thermodynamic temperature: the kinetic energy of atomic free particle motion. The redefinition fixed the Boltzmann constant at precisely 1.380649×10−23 joules per kelvin (J/K).[1]

The microscopic property that imbues material substances with a temperature can be readily understood by examining the ideal gas law, which relates, per the Boltzmann constant, how heat energy causes precisely defined changes in the pressure and temperature of certain gases. This is because monatomic gases like helium and argon behave kinetically like freely moving perfectly elastic and spherical billiard balls that move only in a specific subset of the possible motions that can occur in matter: that comprising the three translational degrees of freedom. The translational degrees of freedom are the familiar billiard ball-like movements along the X, Y, and Z axes of 3D space (see Fig. 1, below). This is why the noble gases all have the same specific heat capacity per atom and why that value is lowest of all the gases.

Molecules (two or more chemically bound atoms), however, have internal structure and therefore have additional internal degrees of freedom (see Fig. 3, below), which makes molecules absorb more heat energy for any given amount of temperature rise than do the monatomic gases. Heat energy is born in all available degrees of freedom; this is in accordance with the equipartition theorem, so all available internal degrees of freedom have the same temperature as their three external degrees of freedom. However, the property that gives all gases their pressure, which is the net force per unit area on a container arising from gas particles recoiling off it, is a function of the kinetic energy borne in the freely moving atoms' and molecules' three translational degrees of freedom.[2]

Fixing the Boltzmann constant at a specific value, along with other rule making, had the effect of precisely establishing the magnitude of the unit interval of SI temperature, the kelvin, in terms of the average kinetic behavior of the noble gases. Moreover, the starting point of the thermodynamic temperature scale, absolute zero, was reaffirmed as the point at which zero average kinetic energy remains in a sample; the only remaining particle motion being that comprising random vibrations due to zero-point energy.
Absolute zero of temperature
Main article: Absolute zero

Temperature scales are numerical. The numerical zero of a temperature scale is not bound to the absolute zero of temperature. Nevertheless, some temperature scales have their numerical zero coincident with the absolute zero of temperature. Examples are the International SI temperature scale, the Rankine temperature scale, and the thermodynamic temperature scale. Other temperature scales have their numerical zero far from the absolute zero of temperature. Examples are the Fahrenheit scale and the Celsius scale.

At the zero point of thermodynamic temperature, absolute zero, the particle constituents of matter have minimal motion and can become no colder.[3][4] Absolute zero, which is a temperature of zero kelvins (0 K), is precisely equal to −273.15 °C and −459.67 °F. Matter at absolute zero has no remaining transferable average kinetic energy and the only remaining particle motion is due to an ever-pervasive quantum mechanical phenomenon called ZPE (Zero-Point Energy).[5] Though the atoms in, for instance, a container of liquid helium that was precisely at absolute zero would still jostle slightly due to zero-point energy, a theoretically perfect heat engine with such helium as one of its working fluids could never transfer any net kinetic energy (heat energy) to the other working fluid and no thermodynamic work could occur.

Temperature is generally expressed in absolute terms when scientifically examining temperature's interrelationships with certain other physical properties of matter such as its volume or pressure (see Gay-Lussac's law), or the wavelength of its emitted black-body radiation. Absolute temperature is also useful when calculating chemical reaction rates (see Arrhenius equation). Furthermore, absolute temperature is typically used in cryogenics and related phenomena like superconductivity, as per the following example usage: "Conveniently, tantalum's transition temperature (Tc) of 4.4924 kelvin is slightly above the 4.2221 K boiling point of helium."
Boltzmann constant

The Boltzmann constant and its related formulas describe the realm of particle kinetics and velocity vectors whereas ZPE (zero-point energy) is an energy field that jostles particles in ways described by the mathematics of quantum mechanics. In atomic and molecular collisions in gases, ZPE introduces a degree of chaos, i.e., unpredictability, to rebound kinetics; it is as likely that there will be less ZPE-induced particle motion after a given collision as more. This random nature of ZPE is why it has no net effect upon either the pressure or volume of any bulk quantity (a statistically significant quantity of particles) of gases. However, in temperature T = 0 condensed matter; e.g., solids and liquids, ZPE causes inter-atomic jostling where atoms would otherwise be perfectly stationary. Inasmuch as the real-world effects that ZPE has on substances can vary as one alters a thermodynamic system (for example, due to ZPE, helium won't freeze unless under a pressure of at least 2.5 MPa (25 bar)), ZPE is very much a form of thermal energy and may properly be included when tallying a substance's internal energy.
Rankine scale

Though there have been many other temperature scales throughout history, there have been only two scales for measuring thermodynamic temperature where absolute zero is their null point (0): The Kelvin scale and the Rankine scale.

Throughout the scientific world where modern measurements are nearly always made using the International System of Units, thermodynamic temperature is measured using the Kelvin scale. The Rankine scale is part of English engineering units in the United States and finds use in certain engineering fields, particularly in legacy reference works. The Rankine scale uses the degree Rankine (symbol: °R) as its unit, which is the same magnitude as the degree Fahrenheit (symbol: °F).

A unit increment of one degree Rankine is precisely 1.8 times smaller in magnitude than one kelvin; thus, to convert a specific temperature on the Kelvin scale to the Rankine scale, K × 1.8 = °R, and to convert from a temperature on the Rankine scale to the Kelvin scale, °R / 1.8 = K. Consequently, absolute zero is "0" for both scales, but the melting point of water ice (0 °C and 273.15 K) is 491.67 °R.

To convert temperature intervals (a span or difference between two temperatures), one uses the same formulas from the preceding paragraph; for instance, a range of 5 kelvins is precisely equal to a range of 9 degrees Rankine.
Modern redefinition of the kelvin

For 65 years, between 1954 and the 2019 redefinition of the SI base units, a temperature interval of one kelvin was defined as 1/273.16 the difference between the triple point of water and absolute zero. The 1954 resolution by the International Bureau of Weights and Measures (known by the French-language acronym BIPM), plus later resolutions and publications, defined the triple point of water as precisely 273.16 K and acknowledged that it was "common practice" to accept that due to previous conventions (namely, that 0 °C had long been defined as the melting point of water and that the triple point of water had long been experimentally determined to be indistinguishably close to 0.01 °C), the difference between the Celsius scale and Kelvin scale is accepted as 273.15 kelvins; which is to say, 0 °C equals 273.15 kelvins.[6] The net effect of this as well as later resolutions was twofold: 1) they defined absolute zero as precisely 0 K, and 2) they defined that the triple point of special isotopically controlled water called Vienna Standard Mean Ocean Water was precisely 273.16 K and 0.01 °C. One effect of the aforementioned resolutions was that the melting point of water, while very close to 273.15 K and 0 °C, was not a defining value and was subject to refinement with more precise measurements.

The 1954 BIPM standard did a good job of establishing—within the uncertainties due to isotopic variations between water samples—temperatures around the freezing and triple points of water, but required that intermediate values between the triple point and absolute zero, as well as extrapolated values from room temperature and beyond, to be experimentally determined via apparatus and procedures in individual labs. This shortcoming was addressed by the International Temperature Scale of 1990, or ITS‑90, which defined 13 additional points, from 13.8033 K, to 1,357.77 K. While definitional, ITS‑90 had—and still has—some challenges, partly because eight of its extrapolated values depend upon the melting or freezing points of metal samples, which must remain exceedingly pure lest their melting or freezing points be affected—usually depressed.

The 2019 redefinition of the SI base units was primarily for the purpose of decoupling much of the SI system's definitional underpinnings from the kilogram, which was the last physical artifact defining an SI base unit (a platinum/iridium cylinder stored under three nested bell jars in a safe located in France) and which had highly questionable stability. The solution required that four physical constants, including the Boltzmann constant, be definitionally fixed.

Assigning the Boltzmann constant a precisely defined value had no practical effect on modern thermometry except for the most exquisitely precise measurements. Before the redefinition, the triple point of water was exactly 273.16 K and 0.01 °C and the Boltzmann constant was experimentally determined to be 1.38064903(51)×10−23 J/K, where the "(51)" denotes the uncertainty in the two least significant digits (the 03) and equals a relative standard uncertainty of 0.37 ppm.[7] Afterwards, by defining the Boltzmann constant as exactly 1.380649×10−23 J/K, the 0.37 ppm uncertainty was transferred to the triple point of water, which became an experimentally determined value of 273.1600±0.0001 K (0.0100±0.0001 °C). That the triple point of water ended up being exceedingly close to 273.16 K after the SI redefinition was no accident; the final value of the Boltzmann constant was determined, in part, through clever experiments with argon and helium that used the triple point of water for their key reference temperature.[8][9]

Notwithstanding the 2019 redefinition, water triple-point cells continue to serve in modern thermometry as exceedingly precise calibration references at 273.16 K and 0.01 °C. Moreover, the triple point of water remains one of the 14 calibration points comprising ITS‑90, which spans from the triple point of hydrogen (13.8033 K) to the freezing point of copper (1,357.77 K), which is a nearly hundredfold range of thermodynamic temperature.
Relationship of temperature, motions, conduction, and thermal energy
Figure 1 The translational motion of fundamental particles of nature such as atoms and molecules are directly related to temperature. Here, the size of helium atoms relative to their spacing is shown to scale under 1950 atmospheres of pressure. These room-temperature atoms have a certain average speed (slowed down here two trillion-fold). At any given instant however, a particular helium atom may be moving much faster than average while another may be nearly motionless. Five atoms are colored red to facilitate following their motions. This animation illustrates statistical mechanics, which is the science of how the group behavior of a large collection of microscopic objects emerges from the kinetic properties of each individual object.
Nature of kinetic energy, translational motion, and temperature

The thermodynamic temperature of any bulk quantity of a substance (a statistically significant quantity of particles) is directly proportional to the mean average kinetic energy of a specific kind of particle motion known as translational motion. These simple movements in the three X, Y, and Z–axis dimensions of space means the particles move in the three spatial degrees of freedom. This particular form of kinetic energy is sometimes referred to as kinetic temperature. Translational motion is but one form of heat energy and is what gives gases not only their temperature, but also their pressure and the vast majority of their volume. This relationship between the temperature, pressure, and volume of gases is established by the ideal gas law's formula pV = nRT and is embodied in the gas laws.

Though the kinetic energy borne exclusively in the three translational degrees of freedom comprise the thermodynamic temperature of a substance, molecules, as can be seen in Fig. 3, can have other degrees of freedom, all of which fall under three categories: bond length, bond angle, and rotational. All three additional categories are not necessarily available to all molecules, and even for molecules that can experience all three, some can be "frozen out" below a certain temperature. Nonetheless, all those degrees of freedom that are available to the molecules under a particular set of conditions contribute to the specific heat capacity of a substance; which is to say, they increase the amount of heat (kinetic energy) required to raise a given amount of the substance by one kelvin or one degree Celsius.

The relationship of kinetic energy, mass, and velocity is given by the formula Ek = 1/2mv2.[10] Accordingly, particles with one unit of mass moving at one unit of velocity have precisely the same kinetic energy, and precisely the same temperature, as those with four times the mass but half the velocity.

The extent to which the kinetic energy of translational motion in a statistically significant collection of atoms or molecules in a gas contributes to the pressure and volume of that gas is a proportional function of thermodynamic temperature as established by the Boltzmann constant (symbol: kB). The Boltzmann constant also relates the thermodynamic temperature of a gas to the mean kinetic energy of an individual particles' translational motion as follows:
E ~ = 3 2 k B T
{\displaystyle {\tilde {E}}={\frac {3}{2}}k_{\text{B}}T}
where:

    E ~ {\displaystyle {\tilde {E}}} is the mean kinetic energy for an individual particle
    kB = 1.380649×10−23 J/K
    T is the thermodynamic temperature of the bulk quantity of the substance

Figure 2 The translational motions of helium atoms occur across a range of speeds. Compare the shape of this curve to that of a Planck curve in Fig. 5 below.

While the Boltzmann constant is useful for finding the mean kinetic energy in a sample of particles, it is important to note that even when a substance is isolated and in thermodynamic equilibrium (all parts are at a uniform temperature and no heat is going into or out of it), the translational motions of individual atoms and molecules occurs across a wide range of speeds (see animation in Fig. 1 above). At any one instant, the proportion of particles moving at a given speed within this range is determined by probability as described by the Maxwell–Boltzmann distribution. The graph shown here in Fig. 2 shows the speed distribution of 5500 K helium atoms. They have a most probable speed of 4.780 km/s (0.2092 s/km). However, a certain proportion of atoms at any given instant are moving faster while others are moving relatively slowly; some are momentarily at a virtual standstill (off the x–axis to the right). This graph uses inverse speed for its x–axis so the shape of the curve can easily be compared to the curves in Fig. 5 below. In both graphs, zero on the x–axis represents infinite temperature. Additionally, the x and y–axis on both graphs are scaled proportionally.
High speeds of translational motion

Although very specialized laboratory equipment is required to directly detect translational motions, the resultant collisions by atoms or molecules with small particles suspended in a fluid produces Brownian motion that can be seen with an ordinary microscope. The translational motions of elementary particles are very fast[11] and temperatures close to absolute zero are required to directly observe them. For instance, when scientists at the NIST achieved a record-setting cold temperature of 700 nK (billionths of a kelvin) in 1994, they used optical lattice laser equipment to adiabatically cool cesium atoms. They then turned off the entrapment lasers and directly measured atom velocities of 7 mm per second to in order to calculate their temperature.[12] Formulas for calculating the velocity and speed of translational motion are given in the following footnote.[13]
Figure 2.5 This simulation illustrates an argon atom as it would appear through a 400-power optical microscope featuring a reticle graduated with 50-micron (0.05 mm) tick marks. This atom is moving with a velocity of 14.43 microns per second, which gives the atom a kinetic temperature of one-trillionth of a kelvin. The atom requires 13.9 seconds to travel 200 microns (0.2 mm). Though the atom is being invisibly jostled due to zero-point energy, its translational motion seen here comprises all its kinetic energy.

It is neither difficult to imagine atomic motions due to kinetic temperature, nor distinguish between such motions and those due to zero-point energy. Consider the following hypothetical thought experiment, as illustrated in Fig. 2.5 at left, with an atom that is exceedingly close to absolute zero. Imagine peering through a common optical microscope set to 400 power, which is about the maximum practical magnification for optical microscopes. Such microscopes generally provide fields of view a bit over 0.4 mm in diameter. At the center of the field of view is a single levitated argon atom (argon comprises about 0.93% of air) that is illuminated and glowing against a dark backdrop. If this argon atom was at a beyond-record-setting one-trillionth of a kelvin above absolute zero,[14] and was moving perpendicular to the field of view towards the right, it would require 13.9 seconds to move from the center of the image to the 200-micron tick mark; this travel distance is about the same as the width of the period at the end of this sentence on modern computer monitors. As the argon atom slowly moved, the positional jitter due to zero-point energy would be much less than the 200-nanometer (0.0002 mm) resolution of an optical microscope. Importantly, the atom's translational velocity of 14.43 microns per second constitutes all its retained kinetic energy due to not being precisely at absolute zero. Were the atom precisely at absolute zero, imperceptible jostling due to zero-point energy would cause it to very slightly wander, but the atom would perpetually be located, on average, at the same spot within the field of view. This is analogous to a boat that has had its motor turned off and is now bobbing slightly in relatively calm and windless ocean waters; even though the boat randomly drifts to and fro, it stays in the same spot in the long term and makes no headway through the water. Accordingly, an atom that was precisely at absolute zero would not be "motionless", and yet, a statistically significant collection of such atoms would have zero net kinetic energy available to transfer to any other collection of atoms. This is because regardless of the kinetic temperature of the second collection of atoms, they too experience the effects of zero-point energy. Such are the consequences of statistical mechanics and the nature of thermodynamics.
Internal motions of molecules and internal energy
Figure 3 Molecules have internal structures because they are composed of atoms that have different ways of moving within molecules. Being able to store kinetic energy in these internal degrees of freedom contributes to a substance's specific heat capacity, or internal energy, allowing it to contain more internal energy at the same temperature.

As mentioned above, there are other ways molecules can jiggle besides the three translational degrees of freedom that imbue substances with their kinetic temperature. As can be seen in the animation at right, molecules are complex objects; they are a population of atoms and thermal agitation can strain their internal chemical bonds in three different ways: via rotation, bond length, and bond angle movements; these are all types of internal degrees of freedom. This makes molecules distinct from monatomic substances (consisting of individual atoms) like the noble gases helium and argon, which have only the three translational degrees of freedom (the X, Y, and Z axis). Kinetic energy is stored in molecules' internal degrees of freedom, which gives them an internal temperature. Even though these motions are called "internal", the external portions of molecules still move—rather like the jiggling of a stationary water balloon. This permits the two-way exchange of kinetic energy between internal motions and translational motions with each molecular collision. Accordingly, as internal energy is removed from molecules, both their kinetic temperature (the kinetic energy of translational motion) and their internal temperature simultaneously diminish in equal proportions. This phenomenon is described by the equipartition theorem, which states that for any bulk quantity of a substance in equilibrium, the kinetic energy of particle motion is evenly distributed among all the active degrees of freedom available to the particles. Since the internal temperature of molecules are usually equal to their kinetic temperature, the distinction is usually of interest only in the detailed study of non-local thermodynamic equilibrium (LTE) phenomena such as combustion, the sublimation of solids, and the diffusion of hot gases in a partial vacuum.

The kinetic energy stored internally in molecules causes substances to contain more heat energy at any given temperature and to absorb additional internal energy for a given temperature increase. This is because any kinetic energy that is, at a given instant, bound in internal motions, is not contributing to the molecules' translational motions at that same instant .[15] This extra kinetic energy simply increases the amount of internal energy that substance absorbs for a given temperature rise. This property is known as a substance's specific heat capacity.

Different molecules absorb different amounts of internal energy for each incremental increase in temperature; that is, they have different specific heat capacities. High specific heat capacity arises, in part, because certain substances' molecules possess more internal degrees of freedom than others do. For instance, room-temperature nitrogen, which is a diatomic molecule, has five active degrees of freedom: the three comprising translational motion plus two rotational degrees of freedom internally. Not surprisingly, in accordance with the equipartition theorem, nitrogen has five-thirds the specific heat capacity per mole (a specific number of molecules) as do the monatomic gases.[16] Another example is gasoline (see table showing its specific heat capacity). Gasoline can absorb a large amount of heat energy per mole with only a modest temperature change because each molecule comprises an average of 21 atoms and therefore has many internal degrees of freedom. Even larger, more complex molecules can have dozens of internal degrees of freedom.
Diffusion of thermal energy: entropy, phonons, and mobile conduction electrons
Figure 4 The temperature-induced translational motion of particles in solids takes the form of phonons. Shown here are phonons with identical amplitudes but with wavelengths ranging from 2 to 12 average inter-molecule separations (a).

Heat conduction is the diffusion of thermal energy from hot parts of a system to cold parts. A system can be either a single bulk entity or a plurality of discrete bulk entities. The term bulk in this context means a statistically significant quantity of particles (which can be a microscopic amount). Whenever thermal energy diffuses within an isolated system, temperature differences within the system decrease (and entropy increases).

One particular heat conduction mechanism occurs when translational motion, the particle motion underlying temperature, transfers momentum from particle to particle in collisions. In gases, these translational motions are of the nature shown above in Fig. 1. As can be seen in that animation, not only does momentum (heat) diffuse throughout the volume of the gas through serial collisions, but entire molecules or atoms can move forward into new territory, bringing their kinetic energy with them. Consequently, temperature differences equalize throughout gases very quickly—especially for light atoms or molecules; convection speeds this process even more.[17]

Translational motion in solids, however, takes the form of phonons (see Fig. 4 at right). Phonons are constrained, quantized wave packets that travel at the speed of sound of a given substance. The manner in which phonons interact within a solid determines a variety of its properties, including its thermal conductivity. In electrically insulating solids, phonon-based heat conduction is usually inefficient[18] and such solids are considered thermal insulators (such as glass, plastic, rubber, ceramic, and rock). This is because in solids, atoms and molecules are locked into place relative to their neighbors and are not free to roam.

Metals however, are not restricted to only phonon-based heat conduction. Thermal energy conducts through metals extraordinarily quickly because instead of direct molecule-to-molecule collisions, the vast majority of thermal energy is mediated via very light, mobile conduction electrons. This is why there is a near-perfect correlation between metals' thermal conductivity and their electrical conductivity.[19] Conduction electrons imbue metals with their extraordinary conductivity because they are delocalized (i.e., not tied to a specific atom) and behave rather like a sort of quantum gas due to the effects of zero-point energy (for more on ZPE, see Note 1 below). Furthermore, electrons are relatively light with a rest mass only 1⁄1836 that of a proton. This is about the same ratio as a .22 Short bullet (29 grains or 1.88 g) compared to the rifle that shoots it. As Isaac Newton wrote with his third law of motion,

    Law #3: All forces occur in pairs, and these two forces are equal in magnitude and opposite in direction.

However, a bullet accelerates faster than a rifle given an equal force. Since kinetic energy increases as the square of velocity, nearly all the kinetic energy goes into the bullet, not the rifle, even though both experience the same force from the expanding propellant gases. In the same manner, because they are much less massive, thermal energy is readily borne by mobile conduction electrons. Additionally, because they're delocalized and very fast, kinetic thermal energy conducts extremely quickly through metals with abundant conduction electrons.
Diffusion of thermal energy: black-body radiation
Figure 5 The spectrum of black-body radiation has the form of a Planck curve. A 5500 K black-body has a peak emittance wavelength of 527 nm. Compare the shape of this curve to that of a Maxwell distribution in Fig. 2 above.

Thermal radiation is a byproduct of the collisions arising from various vibrational motions of atoms. These collisions cause the electrons of the atoms to emit thermal photons (known as black-body radiation). Photons are emitted anytime an electric charge is accelerated (as happens when electron clouds of two atoms collide). Even individual molecules with internal temperatures greater than absolute zero also emit black-body radiation from their atoms. In any bulk quantity of a substance at equilibrium, black-body photons are emitted across a range of wavelengths in a spectrum that has a bell curve-like shape called a Planck curve (see graph in Fig. 5 at right). The top of a Planck curve (the peak emittance wavelength) is located in a particular part of the electromagnetic spectrum depending on the temperature of the black-body. Substances at extreme cryogenic temperatures emit at long radio wavelengths whereas extremely hot temperatures produce short gamma rays (see Table of common temperatures).

Black-body radiation diffuses thermal energy throughout a substance as the photons are absorbed by neighboring atoms, transferring momentum in the process. Black-body photons also easily escape from a substance and can be absorbed by the ambient environment; kinetic energy is lost in the process.

As established by the Stefan–Boltzmann law, the intensity of black-body radiation increases as the fourth power of absolute temperature. Thus, a black-body at 824 K (just short of glowing dull red) emits 60 times the radiant power as it does at 296 K (room temperature). This is why one can so easily feel the radiant heat from hot objects at a distance. At higher temperatures, such as those found in an incandescent lamp, black-body radiation can be the principal mechanism by which thermal energy escapes a system.
Table of thermodynamic temperatures

The table below shows various points on the thermodynamic scale, in order of increasing temperature.
	kelvin 	Peak emittance
wavelength[20] of
black-body photons
Absolute zero
(precisely by definition) 	0 K 	  ∞ [5]
Coldest measured
temperature [21] 	450 pK 	6,400 km
One millikelvin
(precisely by definition) 	0.001 K 	2.897 77 m
(radio, FM band)[22]
cosmic microwave
background radiation 	2.725 K 	1.063 mm (peak wavelength)
Water's triple point 	273.16 K 	10.6083 μm
(long wavelength I.R.)
ISO 1 standard temperature
for precision metrology
(precisely 20 °C by definition) 	293.15 K 	9.88495 μm
(long wavelength I.R.)
Incandescent lamp[A] 	2500 K[B] 	1.16 μm
(near infrared)[C]
Sun's visible surface[23][24][25][26] 	5772 K 	502 nm
(green light)
Lightning bolt's
channel 	28,000 K 	100 nm
(far ultraviolet light)
Sun's core 	16 MK 	0.18 nm (X-rays)
Thermonuclear explosion
(peak temperature)[27] 	350 MK 	8.3 × 10−3 nm
(gamma rays)
Sandia National Labs'
Z machine[D][28] 	2 GK 	1.4 × 10−3 nm
(gamma rays)
Core of a high-mass
star on its last day[29] 	3 GK 	1 × 10−3 nm
(gamma rays)
Merging binary neutron
star system[30] 	350 GK 	8 × 10−6 nm
(gamma rays)
Gamma-ray burst progenitors[31] 	1 TK 	3 × 10−6 nm
(gamma rays)
CERN's proton vs.
nucleus collisions[32] 	10 TK 	3 × 10−7 nm
(gamma rays)

For a true blackbody (which tungsten filaments are not). Tungsten filaments' emissivity is greater at shorter wavelengths, which makes them appear whiter.
The 2500 K value is approximate.
Effective photosphere temperature.

    For a true blackbody (which the plasma was not). The Z machine's dominant emission originated from 40 MK electrons (soft x–ray emissions) within the plasma.

Heat of phase changes
Figure 6 Ice and water: two phases of the same substance

The kinetic energy of particle motion is just one contributor to the total thermal energy in a substance; another is phase transitions, which are the potential energy of molecular bonds that can form in a substance as it cools (such as during condensing and freezing). The thermal energy required for a phase transition is called latent heat. This phenomenon may more easily be grasped by considering it in the reverse direction: latent heat is the energy required to break chemical bonds (such as during evaporation and melting). Almost everyone is familiar with the effects of phase transitions; for instance, steam at 100 °C can cause severe burns much faster than the 100 °C air from a hair dryer. This occurs because a large amount of latent heat is liberated as steam condenses into liquid water on the skin.

Even though thermal energy is liberated or absorbed during phase transitions, pure chemical elements, compounds, and eutectic alloys exhibit no temperature change whatsoever while they undergo them (see Fig. 7, below right). Consider one particular type of phase transition: melting. When a solid is melting, crystal lattice chemical bonds are being broken apart; the substance is transitioning from what is known as a more ordered state to a less ordered state. In Fig. 7, the melting of ice is shown within the lower left box heading from blue to green.
Figure 7 Water's temperature does not change during phase transitions as heat flows into or out of it. The total heat capacity of a mole of water in its liquid phase (the green line) is 7.5507 kJ.

At one specific thermodynamic point, the melting point (which is 0 °C across a wide pressure range in the case of water), all the atoms or molecules are, on average, at the maximum energy threshold their chemical bonds can withstand without breaking away from the lattice. Chemical bonds are all-or-nothing forces: they either hold fast, or break; there is no in-between state. Consequently, when a substance is at its melting point, every joule of added thermal energy only breaks the bonds of a specific quantity of its atoms or molecules,[33] converting them into a liquid of precisely the same temperature; no kinetic energy is added to translational motion (which is what gives substances their temperature). The effect is rather like popcorn: at a certain temperature, additional thermal energy can't make the kernels any hotter until the transition (popping) is complete. If the process is reversed (as in the freezing of a liquid), thermal energy must be removed from a substance.

As stated above, the thermal energy required for a phase transition is called latent heat. In the specific cases of melting and freezing, it's called enthalpy of fusion or heat of fusion. If the molecular bonds in a crystal lattice are strong, the heat of fusion can be relatively great, typically in the range of 6 to 30 kJ per mole for water and most of the metallic elements.[34] If the substance is one of the monatomic gases, (which have little tendency to form molecular bonds) the heat of fusion is more modest, ranging from 0.021 to 2.3 kJ per mole.[35] Relatively speaking, phase transitions can be truly energetic events. To completely melt ice at 0 °C into water at 0 °C, one must add roughly 80 times the thermal energy as is required to increase the temperature of the same mass of liquid water by one degree Celsius. The metals' ratios are even greater, typically in the range of 400 to 1200 times.[36] And the phase transition of boiling is much more energetic than freezing. For instance, the energy required to completely boil or vaporize water (what is known as enthalpy of vaporization) is roughly 540 times that required for a one-degree increase.[37]

Water's sizable enthalpy of vaporization is why one's skin can be burned so quickly as steam condenses on it (heading from red to green in Fig. 7 above); water vapors (gas phase) are liquefied on the skin with releasing a large amount of energy (enthalpy) to the environment including the skin, resulting in skin damage. In the opposite direction, this is why one's skin feels cool as liquid water on it evaporates (a process that occurs at a sub-ambient wet-bulb temperature that is dependent on relative humidity); the water evaporation on the skin takes a large amount of energy from the environment including the skin, reducing the skin temperature. Water's highly energetic enthalpy of vaporization is also an important factor underlying why solar pool covers (floating, insulated blankets that cover swimming pools when the pools are not in use) are so effective at reducing heating costs: they prevent evaporation. (In other words, taking energy from water when it is evaporated is limited.) For instance, the evaporation of just 20 mm of water from a 1.29-meter-deep pool chills its water 8.4 degrees Celsius (15.1 °F).
Internal energy

The total energy of all translational and internal particle motions, including that of conduction electrons, plus the potential energy of phase changes, plus zero-point energy[5] of a substance comprise the internal energy of it.
Figure 8 When many of the chemical elements, such as the noble gases and platinum-group metals, freeze to a solid — the most ordered state of matter — their crystal structures have a close-packed arrangement. This yields the greatest possible packing density and the lowest energy state.
Internal energy at absolute zero

As a substance cools, different forms of internal energy and their related effects simultaneously decrease in magnitude: the latent heat of available phase transitions is liberated as a substance changes from a less ordered state to a more ordered state; the translational motions of atoms and molecules diminish (their kinetic energy or temperature decreases); the internal motions of molecules diminish (their internal energy or temperature decreases); conduction electrons (if the substance is an electrical conductor) travel somewhat slower;[38] and black-body radiation's peak emittance wavelength increases (the photons' energy decreases). When particles of a substance are as close as possible to complete rest and retain only ZPE (Zero Point Energy)-induced quantum mechanical motion, the substance is at the temperature of absolute zero (T = 0).
Figure 9 Due to the effects of zero-point energy, helium at ambient pressure remains a superfluid even when exceedingly close to absolute zero; it won't freeze unless under 25 bar of pressure (~25 atmospheres).

Note that whereas absolute zero is the point of zero thermodynamic temperature and is also the point at which the particle constituents of matter have minimal motion, absolute zero is not necessarily the point at which a substance contains zero internal energy; one must be very precise with what one means by internal energy. Often, all the phase changes that can occur in a substance, will have occurred by the time it reaches absolute zero. However, this is not always the case. Notably, T = 0 helium remains liquid at room pressure (Fig. 9 at right) and must be under a pressure of at least 25 bar (2.5 MPa) to crystallize. This is because helium's heat of fusion (the energy required to melt helium ice) is so low (only 21 joules per mole) that the motion-inducing effect of zero-point energy is sufficient to prevent it from freezing at lower pressures.

A further complication is that many solids change their crystal structure to more compact arrangements at extremely high pressures (up to millions of bars, or hundreds of gigapascals). These are known as solid–solid phase transitions wherein latent heat is liberated as a crystal lattice changes to a more thermodynamically favorable, compact one.

The above complexities make for rather cumbersome blanket statements regarding the internal energy in T = 0 substances. Regardless of pressure though, what can be said is that at absolute zero, all solids with a lowest-energy crystal lattice such those with a closest-packed arrangement (see Fig. 8, above left) contain minimal internal energy, retaining only that due to the ever-present background of zero-point energy.[5] [39] One can also say that for a given substance at constant pressure, absolute zero is the point of lowest enthalpy (a measure of work potential that takes internal energy, pressure, and volume into consideration).[40] Lastly, it is always true to say that all T = 0 substances contain zero kinetic thermal energy.[5] [13]
Practical applications for thermodynamic temperature

Thermodynamic temperature is useful not only for scientists, it can also be useful for lay-people in many disciplines involving gases. By expressing variables in absolute terms and applying Gay-Lussac's law of temperature/pressure proportionality, solutions to everyday problems are straightforward; for instance, calculating how a temperature change affects the pressure inside an automobile tire. If the tire has a cold gage[41] pressure of 200 kPa, then its absolute pressure is 300 kPa.[42][43] Room temperature ("cold" in tire terms) is 296 K. If the tire temperature is 20 °C hotter (20 kelvins), the solution is calculated as 316 K/296 K = 6.8% greater thermodynamic temperature and absolute pressure; that is, an absolute pressure of 320 kPa, which is a gage pressure of 220 kPa.
Relationship to ideal gas law

The thermodynamic temperature is closely linked to the ideal gas law and its consequences. It can be linked also to the second law of thermodynamics. The thermodynamic temperature can be shown to have special properties, and in particular can be seen to be uniquely defined (up to some constant multiplicative factor) by considering the efficiency of idealized heat engines. Thus the ratio T2/T1 of two temperatures T1 and T2 is the same in all absolute scales.

Strictly speaking, the temperature of a system is well-defined only if it is at thermal equilibrium. From a microscopic viewpoint, a material is at thermal equilibrium if the quantity of heat between its individual particles cancel out. There are many possible scales of temperature, derived from a variety of observations of physical phenomena.

Loosely stated, temperature differences dictate the direction of heat between two systems such that their combined energy is maximally distributed among their lowest possible states. We call this distribution "entropy". To better understand the relationship between temperature and entropy, consider the relationship between heat, work and temperature illustrated in the Carnot heat engine. The engine converts heat into work by directing a temperature gradient between a higher temperature heat source, TH, and a lower temperature heat sink, TC, through a gas filled piston. The work done per cycle is equal in magnitude to net heat taken up, which is sum of the heat qH taken up by the engine from the high-temperature source, plus the waste heat given off by the engine, qC < 0.[44] The efficiency of the engine is the work divided by the heat put into the system or
Efficiency = | w cy | q H = q H + q C q H = 1 + q C q H = 1 − | q C | | q H |                                         ( 1 )
{\displaystyle {\textrm {Efficiency}}={\frac {|w_{\text{cy}}|}{q_{\text{H}}}}={\frac {q_{\text{H}}+q_{\text{C}}}{q_{\text{H}}}}=1+{\frac {q_{\text{C}}}{q_{\text{H}}}}=1-{\frac {|q_{\text{C}}|}{|q_{\text{H}}|}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1)}
where w cy {\displaystyle w_{\text{cy}}} is the work done per cycle. Thus the efficiency depends only on |qC| / |qH|.

Carnot's theorem states that all reversible engines operating between the same heat reservoirs are equally efficient. Thus, any reversible heat engine operating between temperatures T1 and T2 must have the same efficiency, that is to say, the efficiency is the function of only temperatures

| q C | | q H | = f ( T H , T C ) .                 ( 2 )
{\displaystyle {\frac {|q_{\text{C}}|}{|q_{\text{H}}|}}=f(T_{\text{H}},T_{\text{C}}).\ \ \ \ \ \ \ \ (2)}

In addition, a reversible heat engine operating between a pair of thermal reservoirs at temperatures T1 and T3 must have the same efficiency as one consisting of two cycles, one between T1 and another (intermediate) temperature T2, and the second between T2 andT3. If this were not the case, then energy (in the form of q) will be wasted or gained, resulting in different overall efficiencies every time a cycle is split into component cycles; clearly a cycle can be composed of any number of smaller cycles as an engine design choice, and any reversible engine between the same reservoir at T1 and T3 must be equally efficient regardless of the engine design.

If we choose engines such that work done by the one cycle engine and the two cycle engine are same, then the efficiency of each heat engine is written as the below.

    η 1 = 1 − | q 3 | | q 1 | = 1 − f ( T 1 , T 3 ) {\displaystyle \eta _{1}=1-{\frac {|q_{3}|}{|q_{1}|}}=1-f(T_{1},T_{3})},
    η 2 = 1 − | q 2 | | q 1 | = 1 − f ( T 1 , T 2 ) {\displaystyle \eta _{2}=1-{\frac {|q_{2}|}{|q_{1}|}}=1-f(T_{1},T_{2})},
    η 3 = 1 − | q 3 | | q 2 | = 1 − f ( T 2 , T 3 ) {\displaystyle \eta _{3}=1-{\frac {|q_{3}|}{|q_{2}|}}=1-f(T_{2},T_{3})}.


Here, the engine 1 is the one cycle engine, and the engines 2 and 3 make the two cycle engine where there is the intermediate reservoir at T2. We also have used the fact that the heat q 2 q_{2} passes through the intermediate thermal reservoir at T 2 T_{2} without losing its energy. (I.e., q 2 q_{2} is not lost during its passage through the reservoir at T 2 T_{2}.) This fact can be proved by the following.

    η 2 = 1 − | q 2 | | q 1 | → | w 2 | = | q 1 | − | q 2 | , η 3 = 1 − | q 3 | | q 2 ∗ | → | w 3 | = | q 2 ∗ | − | q 3 | , | w 2 | + | w 3 | = ( | q 1 | − | q 2 | ) + ( | q 2 ∗ | − | q 3 | ) , η 1 = 1 − | q 3 | | q 1 | = ( | w 2 | + | w 3 | ) | q 1 | = ( | q 1 | − | q 2 | ) + ( | q 2 ∗ | − | q 3 | ) | q 1 | . {\displaystyle {\begin{aligned}&{{\eta }_{2}}=1-{\frac {|{{q}_{2}}|}{|{{q}_{1}}|}}\to |{{w}_{2}}|=|{{q}_{1}}|-|{{q}_{2}}|,\\&{{\eta }_{3}}=1-{\frac {|{{q}_{3}}|}{|{{q}_{2}}^{*}|}}\to |{{w}_{3}}|=|{{q}_{2}}^{*}|-|{{q}_{3}}|,\\&|{{w}_{2}}|+|{{w}_{3}}|=(|{{q}_{1}}|-|{{q}_{2}}|)+(|{{q}_{2}}^{*}|-|{{q}_{3}}|),\\&{{\eta }_{1}}=1-{\frac {|{{q}_{3}}|}{|{{q}_{1}}|}}={\frac {(|{{w}_{2}}|+|{{w}_{3}}|)}{|{{q}_{1}}|}}={\frac {(|{{q}_{1}}|-|{{q}_{2}}|)+(|{{q}_{2}}^{*}|-|{{q}_{3}}|)}{|{{q}_{1}}|}}.\\\end{aligned}}}

In order to have the consistency in the last equation, the heat q 2 q_{2} flown from the engine 2 to the intermediate reservoir must be equal to the heat q 2 ∗ {\displaystyle q_{2}^{*}} flown out from the reservoir to the engine 3.

With this understanding of q1, q2 and q3, mathematically,
f ( T 1 , T 3 ) = | q 3 | | q 1 | = | q 2 | | q 3 | | q 1 | | q 2 | = f ( T 1 , T 2 ) f ( T 2 , T 3 ) .
{\displaystyle f(T_{1},T_{3})={\frac {|q_{3}|}{|q_{1}|}}={\frac {|q_{2}||q_{3}|}{|q_{1}||q_{2}|}}=f(T_{1},T_{2})f(T_{2},T_{3}).}

But since the first function is NOT a function of T2, the product of the final two functions MUST result in the removal of T2 as a variable. The only way is therefore to define the function f as follows:
f ( T 1 , T 2 ) = g ( T 2 ) g ( T 1 )
{\displaystyle f(T_{1},T_{2})={\frac {g(T_{2})}{g(T_{1})}}}
and
f ( T 2 , T 3 ) = g ( T 3 ) g ( T 2 )
{\displaystyle f(T_{2},T_{3})={\frac {g(T_{3})}{g(T_{2})}}}
so that
f ( T 1 , T 3 ) = g ( T 3 ) g ( T 1 ) = | q 3 | | q 1 | .
{\displaystyle f(T_{1},T_{3})={\frac {g(T_{3})}{g(T_{1})}}={\frac {|q_{3}|}{|q_{1}|}}.}

I.e. the ratio of heat exchanged is a function of the respective temperatures at which they occur. We can choose any monotonic function for our g ( T ) g(T);[45] it is a matter of convenience and convention that we choose g ( T ) = T g(T)=T. Choosing then one fixed reference temperature (i.e. triple point of water), we establish the thermodynamic temperature scale.

Such a definition coincides with that of the ideal gas derivation; also it is this definition of the thermodynamic temperature that enables us to represent the Carnot efficiency in terms of TH and TC, and hence derive that the (complete) Carnot cycle is isentropic:

| q C | | q H | = f ( T H , T C ) = T C T H .                       ( 3 )
{\displaystyle {\frac {|q_{\text{C}}|}{|q_{\text{H}}|}}=f(T_{\text{H}},T_{\text{C}})={\frac {T_{\text{C}}}{T_{\text{H}}}}.\ \ \ \ \ \ \ \ \ \ \ (3)}

Substituting this back into our first formula for efficiency yields a relationship in terms of temperature:

Efficiency = 1 + q C q H = 1 − | q C | | q H | = 1 − T C T H .                                     ( 4 )
{\displaystyle {\textrm {Efficiency}}=1+{\frac {q_{\text{C}}}{q_{\text{H}}}}=1-{\frac {|q_{\text{C}}|}{|q_{\text{H}}|}}=1-{\frac {T_{\text{C}}}{T_{\text{H}}}}.\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4)}

Note that for TC = 0 the efficiency is 100% and that efficiency becomes greater than 100% for TC < 0, which is unrealistic. Subtracting 1 from the right hand side of the Equation (4) and the middle portion gives q C q H = − T C T H {\displaystyle {\frac {q_{\text{C}}}{q_{\text{H}}}}=-{\frac {T_{\text{C}}}{T_{\text{H}}}}} and thus [46][44]
q H T H + q C T C = 0.
{\displaystyle {\frac {q_{\text{H}}}{T_{\text{H}}}}+{\frac {q_{\text{C}}}{T_{\text{C}}}}=0.}

The generalization of this equation is the Clausius theorem, which proposes the existence of a state function S S (i.e., a function which depends only on the state of the system, not on how it reached that state) defined (up to an additive constant) by
d S = d q r e v T ,
{\displaystyle dS={\frac {dq_{\mathrm {rev} }}{T}},}
	

 
	

 
	

 

 
	

(5)

where the subscript rev indicates heat transfer in a reversible process. The function S S is the entropy of the system, mentioned previously, and the change of S S around any cycle is zero (as is necessary for any state function). The Equation 5 can be rearranged to get an alternative definition for temperature in terms of entropy and heat (to avoid a logic loop, we should first define entropy through statistical mechanics):
T = d q r e v d S .
{\displaystyle T={\frac {dq_{\mathrm {rev} }}{dS}}.}

For a constant-volume system (so no mechanical work W W) in which the entropy S S is a function S ( E ) S(E) of its internal energy E E, d E = d q r e v {\displaystyle dE=dq_{rev}} and the thermodynamic temperature T T is therefore given by
1 T = d S d E ,
{\displaystyle {\frac {1}{T}}={\frac {dS}{dE}},}
so that the reciprocal of the thermodynamic temperature is the rate of change of entropy with respect to the internal energy at the constant volume.

History
Guillaume Amontons

1702–1703: Guillaume Amontons (1663–1705) published two papers that may be used to credit him as being the first researcher to deduce the existence of a fundamental (thermodynamic) temperature scale featuring an absolute zero. He made the discovery while endeavoring to improve upon the air thermometers in use at the time. His J-tube thermometers comprised a mercury column that was supported by a fixed mass of air entrapped within the sensing portion of the thermometer. In thermodynamic terms, his thermometers relied upon the volume / temperature relationship of gas under constant pressure. His measurements of the boiling point of water and the melting point of ice showed that regardless of the mass of air trapped inside his thermometers or the weight of mercury the air was supporting, the reduction in air volume at the ice point was always the same ratio. This observation led him to posit that a sufficient reduction in temperature would reduce the air volume to zero. In fact, his calculations projected that absolute zero was equivalent to −240 °C—only 33.15 degrees short of the true value of −273.15 °C. Amonton's discovery of a one-to-one relationship between absolute temperature and absolute pressure was rediscovered a century later and popularized within the scientific community by Joseph Louis Gay-Lussac. Today, this principle of thermodynamics is commonly known as Gay-Lussac's law but is also known as Amonton's law.
Anders Celsius

1742: Anders Celsius (1701–1744) created a "backwards" version of the modern Celsius temperature scale. In Celsius's original scale, zero represented the boiling point of water and 100 represented the melting point of ice. In his paper Observations of two persistent degrees on a thermometer, he recounted his experiments showing that ice's melting point was effectively unaffected by pressure. He also determined with remarkable precision how water's boiling point varied as a function of atmospheric pressure. He proposed that zero on his temperature scale (water's boiling point) would be calibrated at the mean barometric pressure at mean sea level.
Carl Linnaeus

1744: Coincident with the death of Anders Celsius, the famous botanist Carl Linnaeus (1707–1778) effectively reversed[47] Celsius's scale upon receipt of his first thermometer featuring a scale where zero represented the melting point of ice and 100 represented water's boiling point. The custom-made linnaeus-thermometer, for use in his greenhouses, was made by Daniel Ekström, Sweden's leading maker of scientific instruments at the time. For the next 204 years, the scientific and thermometry communities worldwide referred to this scale as the centigrade scale. Temperatures on the centigrade scale were often reported simply as degrees or, when greater specificity was desired, degrees centigrade. The symbol for temperature values on this scale was °C (in several formats over the years). Because the term centigrade was also the French-language name for a unit of angular measurement (one-hundredth of a right angle) and had a similar connotation in other languages, the term "centesimal degree" was used when very precise, unambiguous language was required by international standards bodies such as the International Bureau of Weights and Measures (Bureau international des poids et mesures) (BIPM). The 9th CGPM (General Conference on Weights and Measures (Conférence générale des poids et mesures) and the CIPM (International Committee for Weights and Measures (Comité international des poids et mesures) formally adopted[48] degree Celsius (symbol: °C) in 1948.
Johann Heinrich Lambert

1777: In his book Pyrometrie (Berlin: Haude & Spener, 1779) completed four months before his death, Johann Heinrich Lambert (1728–1777), sometimes incorrectly referred to as Joseph Lambert, proposed an absolute temperature scale based on the pressure/temperature relationship of a fixed volume of gas. This is distinct from the volume/temperature relationship of gas under constant pressure that Guillaume Amontons discovered 75 years earlier. Lambert stated that absolute zero was the point where a simple straight-line extrapolation reached zero gas pressure and was equal to −270 °C.
Jacques Alexandre César Charles

Circa 1787: Notwithstanding the work of Guillaume Amontons 85 years earlier, Jacques Alexandre César Charles (1746–1823) is often credited with discovering, but not publishing, that the volume of a gas under constant pressure is proportional to its absolute temperature. The formula he created was V1/T1 = V2/T2.
Joseph Louis Gay-Lussac

1802: Joseph Louis Gay-Lussac (1778–1850) published work (acknowledging the unpublished lab notes of Jacques Charles fifteen years earlier) describing how the volume of gas under constant pressure changes linearly with its absolute (thermodynamic) temperature. This behavior is called Charles's Law and is one of the gas laws. His are the first known formulas to use the number 273 for the expansion coefficient of gas relative to the melting point of ice (indicating that absolute zero was equivalent to −273 °C).
Lord Kelvin

1848: William Thomson, (1824–1907) also known as Lord Kelvin, wrote in his paper, On an Absolute Thermometric Scale, of the need for a scale whereby infinite cold (absolute zero) was the scale's zero point, and which used the degree Celsius for its unit increment.

Like Gay-Lussac, Thomson calculated that absolute zero was equivalent to −273 °C on the air thermometers of the time. This absolute scale is known today as the kelvin thermodynamic temperature scale. It's noteworthy that Thomson's value of −273 was actually derived from 0.00366, which was the accepted expansion coefficient of gas per degree Celsius relative to the ice point. The inverse of −0.00366 expressed to five significant digits is −273.22 °C which is remarkably close to the true value of −273.15 °C.

In the paper he proposed to define temperature using idealized heat engines. In detail, he proposed that, given three heat reservoirs at temperatures T A , T B , T C {\displaystyle T_{A},T_{B},T_{C}}, if two reversible heat engines (Carnot engine), one working between T A , T B {\displaystyle T_{A},T_{B}} and another between T B , T C {\displaystyle T_{B},T_{C}}, can produce the same amount of mechanical work W W by letting the same amount of heat Q Q pass through, then define T A − T B = T B − T C {\displaystyle T_{A}-T_{B}=T_{B}-T_{C}}.

Note that like Carnot, Kelvin worked under the assumption that heat is conserved ("the conversion of heat (or caloric) into mechanical effect is probably impossible"), and if heat Q Q goes into the heat engine, then heat Q Q must come out.[49]

1851: William Thomson, realizing after Joule's experiments that heat is not a conserved quantity, but is convertible with mechanical work, modified his scale in An Account of Carnot's Theory of the Motive Power of Heat. In this work, he defined as follows:[50]

    Given two heat reservoirs T A , T B {\displaystyle T_{A},T_{B}}, and a reversible heat engine working between them, such that if during an engine cycle, heat Q A Q_{A} moves into the engine, and heat Q B Q_{B} comes out of the engine, then T A T B = Q A Q B {\displaystyle {\frac {T_{A}}{T_{B}}}={\frac {Q_{A}}{Q_{B}}}}.

The above definition fixes the ratios between absolute temperatures, but it does not fix a scale for absolute temperature. For the scale, Thomson proposed to use the Celsius degree, that is, 1 100 {\frac {1}{100}} the interval between the freezing and the boiling point of water.
Macquorn Rankine

1859: Macquorn Rankine (1820–1872) proposed a thermodynamic temperature scale similar to William Thomson's but which used the degree Fahrenheit for its unit increment, that is, 1 180 {\displaystyle {\frac {1}{180}}} the interval between the freezing and the boiling point of water. This absolute scale is known today as the Rankine thermodynamic temperature scale.
Ludwig Boltzmann

1877–1884: Ludwig Boltzmann (1844–1906) made major contributions to thermodynamics through an understanding of the role that particle kinetics and black body radiation played. His name is now attached to several of the formulas used today in thermodynamics.

Circa 1930s: Gas thermometry experiments carefully calibrated to the melting point of ice and boiling point of water showed that absolute zero was equivalent to −273.15 °C.

1948: Resolution 3 of the 9th CGPM (Conférence Générale des Poids et Mesures, also known as the General Conference on Weights and Measures) fixed the triple point of water at precisely 0.01 °C. At this time, the triple point still had no formal definition for its equivalent kelvin value, which the resolution declared "will be fixed at a later date". The implication is that if the value of absolute zero measured in the 1930s was truly −273.15 °C, then the triple point of water (0.01 °C) was equivalent to 273.16 K. Additionally, both the CIPM (Comité international des poids et mesures, also known as the International Committee for Weights and Measures) and the CGPM formally adopted the name Celsius for the degree Celsius and the Celsius temperature scale. [51]

1954: Resolution 3 of the 10th CGPM gave the kelvin scale its modern definition by choosing the triple point of water as its upper defining point (with no change to absolute zero being the null point) and assigning it a temperature of precisely 273.16 kelvins (what was actually written 273.16 degrees Kelvin at the time). This, in combination with Resolution 3 of the 9th CGPM, had the effect of defining absolute zero as being precisely zero kelvins and −273.15 °C.

1967/1968: Resolution 3 of the 13th CGPM renamed the unit increment of thermodynamic temperature kelvin, symbol K, replacing degree absolute, symbol °K. Further, feeling it useful to more explicitly define the magnitude of the unit increment, the 13th CGPM also decided in Resolution 4 that "The kelvin, unit of thermodynamic temperature, is the fraction 1/273.16 of the thermodynamic temperature of the triple point of water".

2005: The CIPM (Comité International des Poids et Mesures, also known as the International Committee for Weights and Measures) affirmed that for the purposes of delineating the temperature of the triple point of water, the definition of the kelvin thermodynamic temperature scale would refer to water having an isotopic composition defined as being precisely equal to the nominal specification of Vienna Standard Mean Ocean Water.

2019: In November 2018, the 26th General Conference on Weights and Measures (CGPM) changed the definition of the Kelvin by fixing the Boltzmann constant to 1.380649×10−23 when expressed in the unit J/K. This change (and other changes in the definition of SI units) was made effective on the 144th anniversary of the Metre Convention, 20 May 2019.
See also

    Absolute zero
    Hagedorn temperature
    Adiabatic process
    Boltzmann constant
    Carnot heat engine
    Energy conversion efficiency
    Enthalpy
    Entropy
    Equipartition theorem
    Fahrenheit
    First law of thermodynamics
    Freezing
    Gas laws
    International System of Quantities
    ITS-90
    Ideal gas law
    Kelvin
    Laws of thermodynamics
    Maxwell–Boltzmann distribution
    Orders of magnitude (temperature)
    Phase transition
    Planck's law of black-body radiation
    Rankine scale
    Specific heat capacity
    Standard enthalpy change of fusion
    Standard enthalpy change of vaporization
    Temperature
    Temperature conversion formulas
    Thermal radiation
    Thermodynamic beta
    Thermodynamic equations
    Thermodynamic equilibrium
    Thermodynamics
    Thermodynamics Category (list of articles)
    Timeline of heat engine technology
    Timeline of temperature and pressure measurement technology
    Triple point

Notes

    In the following notes, wherever numeric equalities are shown in concise form, such as 1.85487(14)×1043, the two digits between the parentheses denotes the uncertainty at 1-σ (1 standard deviation, 68% confidence level) in the two least significant digits of the significand.

CODATA Value: Boltzmann constant. The NIST Reference on Constants, Units, and Uncertainty. National Institute of Standards and Technology.
Georgia State University, HyperPhysics Project, "Equipartition of Energy"
Rankine, W. J. M., "A manual of the steam engine and other prime movers", Richard Griffin and Co., London (1859), p. 306–307.
William Thomson, 1st Baron Kelvin, "Heat", Adam and Charles Black, Edinburgh (1880), p. 39.
Absolute zero's relationship to zero-point energy
While scientists are achieving temperatures ever closer to absolute zero, they can not fully achieve a state of zero temperature. However, even if scientists could remove all kinetic thermal energy from matter, quantum mechanical zero-point energy (ZPE) causes particle motion that can never be eliminated. Encyclopædia Britannica Online defines zero-point energy as the "vibrational energy that molecules retain even at the absolute zero of temperature". ZPE is the result of all-pervasive energy fields in the vacuum between the fundamental particles of nature; it is responsible for the Casimir effect and other phenomena. See Zero Point Energy and Zero Point Field. See also Solid Helium Archived 2008-02-12 at the Wayback Machine by the University of Alberta's Department of Physics to learn more about ZPE's effect on Bose–Einstein condensates of helium.

Although absolute zero (T = 0) is not a state of zero molecular motion, it is the point of zero temperature and, in accordance with the Boltzmann constant, is also the point of zero particle kinetic energy and zero kinetic velocity. To understand how atoms can have zero kinetic velocity and simultaneously be vibrating due to ZPE, consider the following thought experiment: two T = 0 helium atoms in zero gravity are carefully positioned and observed to have an average separation of 620 pm between them (a gap of ten atomic diameters). It is an "average" separation because ZPE causes them to jostle about their fixed positions. Then one atom is given a kinetic kick of precisely 83 yoctokelvins (1 yK = 1×10−24 K). This is done in a way that directs this atom's velocity vector at the other atom. With 83 yK of kinetic energy between them, the 620 pm gap through their common barycenter would close at a rate of 719 pm/s and they would collide after 0.862 second. This is the same speed as shown in the Fig. 1 animation above. Before being given the kinetic kick, both T = 0 atoms had zero kinetic energy and zero kinetic velocity because they could persist indefinitely in that state and relative orientation even though both were being jostled by ZPE. At T = 0, no kinetic energy is available for transfer to other systems.

Note too that absolute zero serves as the baseline atop which thermodynamics and its equations are founded because they deal with the exchange of thermal energy between "systems" (a plurality of particles and fields modeled as an average). Accordingly, one may examine ZPE-induced particle motion within a system that is at absolute zero but there can never be a net outflow of thermal energy from such a system. Also, the peak emittance wavelength of black-body radiation shifts to infinity at absolute zero; indeed, a peak no longer exists and black-body photons can no longer escape. Because of ZPE, however, virtual photons are still emitted at T = 0. Such photons are called "virtual" because they can't be intercepted and observed. Furthermore, this zero-point radiation has a unique zero-point spectrum. However, even though a T = 0 system emits zero-point radiation, no net heat flow Q out of such a system can occur because if the surrounding environment is at a temperature greater than T = 0, heat will flow inward, and if the surrounding environment is at 'T = 0, there will be an equal flux of ZP radiation both inward and outward. A similar Q equilibrium exists at T = 0 with the ZPE-induced spontaneous emission of photons (which is more properly called a stimulated emission in this context). The graph at upper right illustrates the relationship of absolute zero to zero-point energy. The graph also helps in the understanding of how zero-point energy got its name: it is the vibrational energy matter retains at the zero-kelvin point. Derivation of the classical electromagnetic zero-point radiation spectrum via a classical thermodynamic operation involving van der Waals forces, Daniel C. Cole, Physical Review A, 42 (1990) 1847.
"SI brochure, section 2.1.1.5". International Bureau of Weights and Measures. Archived from the original on 26 September 2007. Retrieved 9 May 2008.
Newell, D B; Cabiati, F; Fischer, J; Fujii, K; Karshenboim, S G; Margolis, H S; de Mirandés, E; Mohr, P J; Nez, F; Pachucki, K; Quinn, T J; Taylor, B N; Wang, M; Wood, B M; Zhang, Z; et al. (Committee on Data for Science and Technology (CODATA) Task Group on Fundamental Constants) (29 January 2018). "The CODATA 2017 values of h, e, k, and NA for the revision of the SI". Metrologia. 55 (1): L13–L16. Bibcode:2018Metro..55L..13N. doi:10.1088/1681-7575/aa950a.
"SI Redefinition – Kelvin: Boltzmann Constant". National Institute of Standards and Technology. Archived from the original on 1 July 2020. Retrieved 13 Dec 2020.
"Acoustic Thermometry". National Institute of Standards and Technology. Archived from the original on 23 September 2020. Retrieved 13 Dec 2020.
At non-relativistic temperatures of less than about 30 GK, classical mechanics are sufficient to calculate the velocity of particles. At 30 GK, individual neutrons (the constituent of neutron stars and one of the few materials in the universe with temperatures in this range) have a 1.0042 γ (gamma or Lorentz factor). Thus, the classic Newtonian formula for kinetic energy is in error less than half a percent for temperatures less than 30 GK.
Even room–temperature air has an average molecular translational speed (not vector-isolated velocity) of 1822 km/hour. This is relatively fast for something the size of a molecule considering there are roughly 2.42×1016 of them crowded into a single cubic millimeter. Assumptions: Average molecular weight of wet air = 28.838 g/mol and T = 296.15 K. Assumption's primary variables: An altitude of 194 meters above mean sea level (the world–wide median altitude of human habitation), an indoor temperature of 23 °C, a dewpoint of 9 °C (40.85% relative humidity), and 760 mmHg (101.325 kPa) sea level–corrected barometric pressure.
Citation: Adiabatic Cooling of Cesium to 700 nK in an Optical Lattice, A. Kastberg et al., Physical Review Letters 74, No. 9, 27 Feb. 1995, Pg. 1542. It's noteworthy that a record cold temperature of 450 pK in a Bose–Einstein condensate of sodium atoms (achieved by A. E. Leanhardt et al.. of MIT) equates to an average vector-isolated atom velocity of 0.4 mm/s and an average atom speed of 0.7 mm/s.
The rate of translational motion of atoms and molecules is calculated based on thermodynamic temperature as follows:
v ~ = k B 2 ⋅ T m 2
{\displaystyle {\tilde {v}}={\sqrt {\frac {{\frac {k_{\text{B}}}{2}}\cdot T}{\frac {m}{2}}}}}
where

    v ~ {\displaystyle {\tilde {v}}} is the vector-isolated mean velocity of translational particle motion in m/s
    kB (Boltzmann constant) = 1.380649×10−23 J/K
    T is the thermodynamic temperature in kelvins
    m is the molecular mass of substance in kg/particle

In the above formula, molecular mass, m, in kg/particle is the quotient of a substance's molar mass (also known as atomic weight, atomic mass, relative atomic mass, and unified atomic mass units) in g/mol or daltons divided by 6.02214076×1026 (which is the Avogadro constant times one thousand). For diatomic molecules such as H2, N2, and O2, multiply atomic weight by two before plugging it into the above formula. The mean speed (not vector-isolated velocity) of an atom or molecule along any arbitrary path is calculated as follows:
s ~ = v ~ ⋅ 3
{\displaystyle {\tilde {s}}={\tilde {v}}\cdot {\sqrt {3}}}
where s ~ {\tilde {s}} is the mean speed of translational particle motion in m/s. Note that the mean energy of the translational motions of a substance's constituent particles correlates to their mean speed, not velocity. Thus, substituting s ~ {\tilde {s}} for v in the classic formula for kinetic energy, Ek = 1⁄2m ⋅ v2 produces precisely the same value as does Emean = 3/2kBT (as shown in the section titled The nature of kinetic energy, translational motion, and temperature).  Note too that the Boltzmann constant and its related formulas establish that absolute zero is the point of both zero kinetic energy of particle motion and zero kinetic velocity (see also Note 1 above).
One-trillionth of a kelvin is to one kelvin as two sheets of kitchen aluminum foil (0.04 mm) are to the distance around Earth at the equator.
The internal degrees of freedom of molecules cause their external surfaces to vibrate and can also produce overall spinning motions (what can be likened to the jiggling and spinning of an otherwise stationary water balloon). If one examines a single molecule as it impacts a containers' wall, some of the kinetic energy borne in the molecule's internal degrees of freedom can constructively add to its translational motion during the instant of the collision and extra kinetic energy will be transferred into the container's wall. This would induce an extra, localized, impulse-like contribution to the average pressure on the container. However, since the internal motions of molecules are random, they have an equal probability of destructively interfering with translational motion during a collision with a container's walls or another molecule. Averaged across any bulk quantity of a gas, the internal thermal motions of molecules have zero net effect upon the temperature, pressure, or volume of a gas. Molecules' internal degrees of freedom simply provide additional locations where kinetic energy is stored. This is precisely why molecular-based gases have greater specific internal capacity than monatomic gases (where additional internal energy must be added to achieve a given temperature rise).
When measured at constant-volume since different amounts of work must be performed if measured at constant-pressure. Nitrogen's CvH (100 kPa, 20 °C) equals 20.8 J⋅mol–1⋅K–1 vs. the monatomic gases, which equal 12.4717 J mol–1 K–1. Citations: W.H. Freeman's Physical Chemistry, Part 3: Change (422 kB PDF, here), Exercise 21.20b, Pg. 787. Also Georgia State University's Molar Specific Heats of Gases.
The speed at which thermal energy equalizes throughout the volume of a gas is very rapid. However, since gases have extremely low density relative to solids, the heat flux (the thermal power passing per area) through gases is comparatively low. This is why the dead-air spaces in multi-pane windows have insulating qualities.
Diamond is a notable exception. Highly quantized modes of phonon vibration occur in its rigid crystal lattice. Therefore, not only does diamond have exceptionally poor specific heat capacity, it also has exceptionally high thermal conductivity.
Correlation is 752 (W⋅m−1⋅K−1)/(MS⋅cm), σ = 81, through a 7:1 range in conductivity. Value and standard deviation based on data for Ag, Cu, Au, Al, Ca, Be, Mg, Rh, Ir, Zn, Co, Ni, Os, Fe, Pa, Pt, and Sn. Citation: Data from CRC Handbook of Chemistry and Physics, 1st Student Edition and this link to Web Elements' home page.
The cited emission wavelengths are for true black bodies in equilibrium. In this table, only the sun so qualifies. CODATA recommended value of 2.897771955...×10−3 m⋅K used for Wien displacement law constant b.
A record cold temperature of 450 ±80 pK in a Bose–Einstein condensate (BEC) of sodium (23Na) atoms was achieved in 2003 by researchers at MIT. Citation: Cooling Bose–Einstein Condensates Below 500 Picokelvin, A. E. Leanhardt et al., Science 301, 12 Sept. 2003, Pg. 1515. The thermal velocity of the atoms averaged about 0.4 mm/s. It is noteworthy that this record's peak emittance black-body wavelength of 6,400 kilometers is roughly the radius of Earth.
The peak emittance wavelength of 2.897 77 m is a frequency of 103.456 MHz
"Resolution B3 on recommended nominal conversion constants for selected solar and planetary properties" (PDF). 2015.
Hertel, Ingolf V.; Schulz, Claus-Peter (2014-10-24). Atoms, Molecules and Optical Physics 1: Atoms and Spectroscopy. Springer. p. 35. ISBN 978-3-642-54322-7.
Vignola, Frank; Michalsky, Joseph; Stoffel, Thomas (2019-07-30). Solar and Infrared Radiation Measurements, Second Edition. CRC Press. pp. chapter 2.1 2.2. ISBN 978-1-351-60020-0.
"Sun Fact Sheet". nssdc.gsfc.nasa.gov. Retrieved 2023-08-27.
The 350 MK value is the maximum peak fusion fuel temperature in a thermonuclear weapon of the Teller–Ulam configuration (commonly known as a "hydrogen bomb"). Peak temperatures in Gadget-style fission bomb cores (commonly known as an "atomic bomb") are in the range of 50 to 100 MK. Citation: Nuclear Weapons Frequently Asked Questions, 3.2.5 Matter At High Temperatures. Link to relevant Web page. All referenced data was compiled from publicly available sources.
Peak temperature for a bulk quantity of matter was achieved by a pulsed-power machine used in fusion physics experiments. The term "bulk quantity" draws a distinction from collisions in particle accelerators wherein high "temperature" applies only to the debris from two subatomic particles or nuclei at any given instant. The >2 GK temperature was achieved over a period of about ten nanoseconds during "shot Z1137". In fact, the iron and manganese ions in the plasma averaged 3.58 ±0.41 GK (309 ±35 keV) for 3 ns (ns 112 through 115). Citation: Ion Viscous Heating in a Magnetohydrodynamically Unstable Z Pinch at Over 2 × 109 Kelvin, M. G. Haines et al., Physical Review Letters 96, Issue 7, id. 075003. Link to Sandia's news release. Archived 2006-07-02 at the Wayback Machine
Core temperature of a high–mass (>8–11 solar masses) star after it leaves the main sequence on the Hertzsprung–Russell diagram and begins the alpha process (which lasts one day) of fusing silicon–28 into heavier elements in the following steps: sulfur–32 → argon–36 → calcium–40 → titanium–44 → chromium–48 → iron–52 → nickel–56. Within minutes of finishing the sequence, the star explodes as a Type II supernova. Citation: Stellar Evolution: The Life and Death of Our Luminous Neighbors (by Arthur Holland and Mark Williams of the University of Michigan). Link to Web site. More informative links can be found here, and here Archived 2011-08-14 at the Wayback Machine, and a concise treatise on stars by NASA is here. Archived July 20, 2015, at the Wayback Machine
Based on a computer model that predicted a peak internal temperature of 30 MeV (350 GK) during the merger of a binary neutron star system (which produces a gamma–ray burst). The neutron stars in the model were 1.2 and 1.6 solar masses respectively, were roughly 20 km in diameter, and were orbiting around their barycenter (common center of mass) at about 390 Hz during the last several milliseconds before they completely merged. The 350 GK portion was a small volume located at the pair's developing common core and varied from roughly 1 to 7 km across over a time span of around 5 ms. Imagine two city-sized objects of unimaginable density orbiting each other at the same frequency as the G4 musical note (the 28th white key on a piano). It is also noteworthy that at 350 GK, the average neutron has a vibrational speed of 30% the speed of light and a relativistic mass (m) 5% greater than its rest mass (m0). Citation: Oechslin, R.; Janka, H.- T. (2006). "Torus formation in neutron star mergers and well-localized short gamma-ray bursts". Monthly Notices of the Royal Astronomical Society. 368 (4): 1489–1499. arXiv:astro-ph/0507099v2. Bibcode:2006MNRAS.368.1489O. doi:10.1111/j.1365-2966.2006.10238.x. S2CID 15036056. To view a browser-based summary of the research, click here.
NewScientist: Eight extremes: The hottest thing in the universe, 07 March 2011, which stated "While the details of this process are currently unknown, it must involve a fireball of relativistic particles heated to something in the region of a trillion kelvin."
Citation: How do physicists study particles? Archived 2007-10-11 at the Wayback Machine by CERN.
Water's enthalpy of fusion (0 °C, 101.325 kPa) equates to 0.062284 eV per molecule so adding one joule of thermal energy to 0 °C water ice causes 1.0021×1020 water molecules to break away from the crystal lattice and become liquid.
Water's enthalpy of fusion is 6.0095 kJ⋅mol−1 K−1 (0 °C, 101.325 kPa). Citation: Water Structure and Science, Water Properties, Enthalpy of fusion, (0 °C, 101.325 kPa) (by London South Bank University). Link to Web site. The only metals with enthalpies of fusion not in the range of 6–30 J mol−1 K−1 are (on the high side): Ta, W, and Re; and (on the low side) most of the group 1 (alkaline) metals plus Ga, In, Hg, Tl, Pb, and Np. Citation: This link to Web Elements' home page.
Xenon value citation: This link to WebElements' xenon data (available values range from 2.3 to 3.1 kJ/mol). It is also noteworthy that helium's heat of fusion of only 0.021 kJ/mol is so weak of a bonding force that zero-point energy prevents helium from freezing unless it is under a pressure of at least 25 atmospheres.
CRC Handbook of Chemistry and Physics, 1st Student Edition and Web Elements.
H2Ospecific heat capacity, Cp = 0.075327 kJ⋅mol−1⋅K−1 (25 °C); Enthalpy of fusion = 6.0095 kJ/mol (0 °C, 101.325 kPa); Enthalpy of vaporization (liquid) = 40.657 kJ/mol (100 °C). Citation: Water Structure and Science, Water Properties (by London South Bank University). Link to Web site.
Mobile conduction electrons are delocalized, i.e. not tied to a specific atom, and behave rather like a sort of quantum gas due to the effects of zero-point energy. Consequently, even at absolute zero, conduction electrons still move between atoms at the Fermi velocity of about 1.6×106 m/s. Kinetic thermal energy adds to this speed and also causes delocalized electrons to travel farther away from the nuclei.
No other crystal structure can exceed the 74.048% packing density of a closest-packed arrangement. The two regular crystal lattices found in nature that have this density are hexagonal close packed (HCP) and face-centered cubic (FCC). These regular lattices are at the lowest possible energy state. Diamond is a closest-packed structure with an FCC crystal lattice. Note too that suitable crystalline chemical compounds, although usually composed of atoms of different sizes, can be considered as closest-packed structures when considered at the molecular level. One such compound is the common mineral known as magnesium aluminum spinel (MgAl2O4). It has a face-centered cubic crystal lattice and no change in pressure can produce a lattice with a lower energy state.
Nearly half of the 92 naturally occurring chemical elements that can freeze under a vacuum also have a closest-packed crystal lattice. This set includes beryllium, osmium, neon, and iridium (but excludes helium), and therefore have zero latent heat of phase transitions to contribute to internal energy (symbol: U). In the calculation of enthalpy (formula: H = U + pV), internal energy may exclude different sources of thermal energy (particularly ZPE) depending on the nature of the analysis. Accordingly, all T = 0 closest-packed matter under a perfect vacuum has either minimal or zero enthalpy, depending on the nature of the analysis. Use Of Legendre Transforms In Chemical Thermodynamics, Robert A. Alberty, Pure Appl.Chem., 73 (2001) 1349.
Regarding the spelling "gage" vs. "gauge" in the context of pressures measured relative to atmospheric pressure, the preferred spelling varies by country and even by industry. Further, both spellings are often used within a particular industry or country. Industries in British English-speaking countries typically use the spelling "gauge pressure" to distinguish it from the pressure-measuring instrument, which in the U.K., is spelled pressure gage. For the same reason, many of the largest American manufacturers of pressure transducers and instrumentation use the spelling gage pressure (the convention used here) in their formal documentation to distinguish it from the instrument, which is spelled pressure gauge. (see Honeywell-Sensotec's FAQ page and Fluke Corporation's product search page).
Pressure also must be in absolute terms. The air still in a tire at a gage pressure of 0 kPa expands too as it gets hotter. It's not uncommon for engineers to overlook that one must work in terms of absolute pressure when compensating for temperature. For instance, a dominant manufacturer of aircraft tires published a document on temperature-compensating tire pressure, which used gage pressure in the formula. However, the high gage pressures involved (180 psi; 12.4 bar; 1.24 MPa) means the error would be quite small. With low-pressure automobile tires, where gage pressures are typically around 2 bar (200 kPa), failing to adjust to absolute pressure results in a significant error. Referenced document: Aircraft Tire Ratings (155 kB PDF, here).
A difference of 100 kPa is used here instead of the 101.325 kPa value of one standard atmosphere. In 1982, the International Union of Pure and Applied Chemistry (IUPAC) recommended that for the purposes of specifying the physical properties of substances, the standard pressure (atmospheric pressure) should be defined as precisely 100 kPa (≈ 750.062 Torr). Besides being a round number, this had a very practical effect: relatively few people live and work at precisely sea level; 100 kPa equates to the mean pressure at an altitude of about 112 meters, which is closer to the 194–meter, worldwide median altitude of human habitation. For especially low-pressure or high-accuracy work, true atmospheric pressure must be measured. Citation: IUPAC.org, Gold Book, Standard Pressure
Planck, M. (1945). Treatise on Thermodynamics. Dover Publications. p. §90 & §137. "eqs.(39), (40), & (65)".
Here, need to add a reason of requiring the function g(T) to be a monotonic function. The Carnot efficiency (efficiency of all reversible engines) may be a reason.
Fermi, E. (1956). Thermodynamics. Dover Publications (still in print). p. 48. "eq.(64)".
A Brief History of Temperature Measurement and; Uppsala University (Sweden), Linnaeus' thermometer
bipm.org
Lemons, Don S. (2020). "Chapter 4: Absolute Temperature". Thermodynamic weirdness : from Fahrenheit to Clausius (First MIT Press Paperback ed.). Cambridge, Massachusetts. ISBN 978-0-262-53894-7. OCLC 1143850952.
Lemons, Don S. (2020). "Chapter 8: Absolute Temperature—Again". Thermodynamic weirdness : from Fahrenheit to Clausius (First MIT Press Paperback ed.). Cambridge, Massachusetts. ISBN 978-0-262-53894-7. OCLC 1143850952.

    According to The Oxford English Dictionary (OED), the term "Celsius's thermometer" had been used at least as early as 1797. Further, the term "The Celsius or Centigrade thermometer" was again used in reference to a particular type of thermometer at least as early as 1850. The OED also cites this 1928 reporting of a temperature: "My altitude was about 5,800 metres, the temperature was 28° Celsius". However, dictionaries seek to find the earliest use of a word or term and are not a useful resource as regards the terminology used throughout the history of science. According to several writings of Dr. Terry Quinn CBE FRS, Director of the BIPM (1988–2004), including Temperature Scales from the early days of thermometry to the 21st century (150 kB PDF, here) as well as Temperature (2nd Edition / 1990 / Academic Press / 0125696817), the term Celsius in connection with the centigrade scale was not used whatsoever by the scientific or thermometry communities until after the CIPM and CGPM adopted the term in 1948. The BIPM wasn't even aware that degree Celsius was in sporadic, non-scientific use before that time. It's also noteworthy that the twelve-volume, 1933 edition of OED did not even have a listing for the word Celsius (but did have listings for both centigrade and centesimal in the context of temperature measurement). The 1948 adoption of Celsius accomplished three objectives:
        All common temperature scales would have their units named after someone closely associated with them; namely, Kelvin, Celsius, Fahrenheit, Réaumur and Rankine.
        Notwithstanding the important contribution of Linnaeus who gave the Celsius scale its modern form, Celsius's name was the obvious choice because it began with the letter C. Thus, the symbol °C that for centuries had been used in association with the name centigrade could continue to be used and would simultaneously inherit an intuitive association with the new name.
        The new name eliminated the ambiguity of the term centigrade, freeing it to refer exclusively to the French-language name for the unit of angular measurement.

External links

    Zero Point Energy and Zero Point Field. A Web site with in-depth explanations of a variety of quantum effects. By Bernard Haisch, of Calphysics Institute.

    vte

SI base quantities
Authority control: National Edit this at Wikidata	

    Germany

Categories:

    TemperatureSI base quantitiesState functions

    This page was last edited on 27 August 2023, at 12:03 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Notation and units

Classical thermodynamics

    Heat and entropy
    Heat and enthalpy

History

    Classical thermodynamics
        Clausius (1850)
        Bryan (1907)
        Carathéodory (1909)
        Planck (1926)

Heat transfer

    Heat transfer between two bodies
    Heat engine
    Heat pump or refrigerator
    Macroscopic view
    Microscopic view
    Calorimetry
    Engineering

Latent and sensible heat

Heat capacity

"Hotness"

See also

References

        Quotations
        Bibliography of cited references
        Further bibliography
    External links

Heat

    Article
    Talk

    Read
    View source
    View history

Tools

From Wikipedia, the free encyclopedia
This article is about a mode of transfer of energy. For the system in a home, see Central heating. For other uses, see Heat (disambiguation).
see caption
A glowing-hot metal bar showing incandescence, the emission of light due to its temperature, is often recognized as a source of heat.
Thermodynamics
The classical Carnot heat engine
Branches
Laws
Systems
System properties
Material properties
Equations
Potentials

    HistoryCulture

Scientists
Other

    Category

    vte

In thermodynamics, heat is the thermal energy transferred between systems due to a temperature difference.[1] In colloquial use, heat sometimes refers to thermal energy itself.

An example of formal vs. informal usage may be obtained from the right-hand photo, in which the metal bar is "conducting heat" from its hot end to its cold end, but if the metal bar is considered a thermodynamic system, then the energy flowing within the metal bar is called internal energy, not heat. The hot metal bar is also transferring heat to its surroundings, a correct statement for both the strict and loose meanings of heat. Another example of informal usage is the term heat content, used despite the fact that physics defines heat as energy transfer. More accurately, it is thermal energy that is contained in the system or body, as it is stored in the microscopic degrees of freedom of the modes of vibration.[2]

Heat is energy in transfer to or from a thermodynamic system, by a mechanism that involves the microscopic atomic modes of motion or the corresponding macroscopic properties.[3] This descriptive characterization excludes the transfers of energy by thermodynamic work or mass transfer. Defined quantitatively, the heat involved in a process is the difference in internal energy between the final and initial states of a system, and subtracting the work done in the process.[4] This is the formulation of the first law of thermodynamics.

The measurement of energy transferred as heat is called calorimetry, performed by measuring its effect on the states of interacting bodies. For example, heat can be measured by the amount of ice melted, or by change in temperature of a body in the surroundings of the system.[5]

In the International System of Units (SI) the unit of measurement for heat, as a form of energy, is the joule (J).
Notation and units

As a form of energy, heat has the unit joule (J) in the International System of Units (SI). In addition, many applied branches of engineering use other, traditional units, such as the British thermal unit (BTU) and the calorie. The standard unit for the rate of heating is the watt (W), defined as one joule per second.

The symbol Q for heat was introduced by Rudolf Clausius and Macquorn Rankine in c. 1859.[6]

Heat released by a system into its surroundings is by convention a negative quantity (Q < 0); when a system absorbs heat from its surroundings, it is positive (Q > 0). Heat transfer rate, or heat flow per unit time, is denoted by Q ˙ {\dot {Q}}, but it is not a time derivative of a function of state (which can also be written with the dot notation) since heat is not a function of state.[7] Heat flux is defined as rate of heat transfer per unit cross-sectional area (watts per square metre).
Classical thermodynamics
Heat and entropy
Main article: Entropy
Rudolf Clausius

In 1856, Rudolf Clausius, referring to closed systems, in which transfers of matter do not occur, defined the second fundamental theorem (the second law of thermodynamics) in the mechanical theory of heat (thermodynamics): "if two transformations which, without necessitating any other permanent change, can mutually replace one another, be called equivalent, then the generations of the quantity of heat Q from work at the temperature T, has the equivalence-value:"[8][9]

    Q T . {\displaystyle {\frac {Q}{T}}.}

In 1865, he came to define the entropy symbolized by S, such that, due to the supply of the amount of heat Q at temperature T the entropy of the system is increased by

    Δ S = Q T {\displaystyle \Delta S={\frac {Q}{T}}}
    	

     
    	

     
    	

     

     
    	

    (1)

In a transfer of energy as heat without work being done, there are changes of entropy in both the surroundings which lose heat and the system which gains it. The increase, ΔS, of entropy in the system may be considered to consist of two parts, an increment, ΔS′ that matches, or 'compensates', the change, −ΔS′, of entropy in the surroundings, and a further increment, ΔS′′ that may be considered to be 'generated' or 'produced' in the system, and is said therefore to be 'uncompensated'. Thus

    Δ S = Δ S ′ + Δ S ″ . {\displaystyle \Delta S=\Delta S'+\Delta S''.}

This may also be written

    Δ S s y s t e m = Δ S c o m p e n s a t e d + Δ S u n c o m p e n s a t e d with Δ S c o m p e n s a t e d = − Δ S s u r r o u n d i n g s . \Delta S_{\mathrm {system} }=\Delta S_{\mathrm {compensated} }+\Delta S_{\mathrm {uncompensated} }\,\,\,\,{\text{with}}\,\,\,\,\Delta S_{\mathrm {compensated} }=-\Delta S_{\mathrm {surroundings} }.

The total change of entropy in the system and surroundings is thus

    Δ S o v e r a l l = Δ S ′ + Δ S ′ ′ − Δ S ′ = Δ S ′ ′ . \Delta S_{\mathrm {overall} }=\Delta S^{\prime }+\Delta S^{\prime \prime }-\Delta S^{\prime }=\Delta S^{\prime \prime }.

This may also be written

    Δ S o v e r a l l = Δ S c o m p e n s a t e d + Δ S u n c o m p e n s a t e d + Δ S s u r r o u n d i n g s = Δ S u n c o m p e n s a t e d . \Delta S_{\mathrm {overall} }=\Delta S_{\mathrm {compensated} }+\Delta S_{\mathrm {uncompensated} }+\Delta S_{\mathrm {surroundings} }=\Delta S_{\mathrm {uncompensated} }.

It is then said that an amount of entropy ΔS′ has been transferred from the surroundings to the system. Because entropy is not a conserved quantity, this is an exception to the general way of speaking, in which an amount transferred is of a conserved quantity.

From the second law of thermodynamics it follows that in a spontaneous transfer of heat, in which the temperature of the system is different from that of the surroundings:

    Δ S o v e r a l l > 0. {\displaystyle \Delta S_{\mathrm {overall} }>0.}

For purposes of mathematical analysis of transfers, one thinks of fictive processes that are called reversible, with the temperature T of the system being hardly less than that of the surroundings, and the transfer taking place at an imperceptibly slow rate.

Following the definition above in formula (1), for such a fictive reversible process, a quantity of transferred heat δQ (an inexact differential) is analyzed as a quantity T dS, with dS (an exact differential):

    T d S = δ Q . T\,\mathrm {d} S=\delta Q.

This equality is only valid for a fictive transfer in which there is no production of entropy, that is to say, in which there is no uncompensated entropy.

If, in contrast, the process is natural, and can really occur, with irreversibility, then there is entropy production, with dSuncompensated > 0. The quantity T dSuncompensated was termed by Clausius the "uncompensated heat", though that does not accord with present-day terminology. Then one has

    T s u r r d S = δ Q + T d S u n c o m p e n s a t e d > δ Q . {\displaystyle T_{surr}\,\mathrm {d} S=\delta Q+T\,\mathrm {d} S_{\mathrm {uncompensated} }>\delta Q.}

This leads to the statement

    T s u r r d S ≥ δ Q (second law) . {\displaystyle T_{surr}\,\mathrm {d} S\geq \delta Q\quad {\text{(second law)}}\,.}

which is the second law of thermodynamics for closed systems.

In non-equilibrium thermodynamics that makes the approximation of assuming the hypothesis of local thermodynamic equilibrium, there is a special notation for this. The transfer of energy as heat is assumed to take place across an infinitesimal temperature difference, so that the system element and its surroundings have near enough the same temperature T. Then one writes

    d S = d S e + d S i , {\displaystyle \mathrm {d} S=\mathrm {d} S_{\mathrm {e} }+\mathrm {d} S_{\mathrm {i} }\,,}

where by definition

    δ Q = T d S e and d S i ≡ d S u n c o m p e n s a t e d . \delta Q=T\,\mathrm {d} S_{\mathrm {e} }\,\,\,\,\,{\text{and}}\,\,\,\,\,\mathrm {d} S_{\mathrm {i} }\equiv \mathrm {d} S_{\mathrm {uncompensated} }.

The second law for a natural process asserts that[10][11][12][13]

    d S i > 0. {\displaystyle \mathrm {d} S_{\mathrm {i} }>0.}

Heat and enthalpy
Further information: Internal energy and Enthalpy

For a closed system (a system from which no matter can enter or exit), one version of the first law of thermodynamics states that the change in internal energy ΔU of the system is equal to the amount of heat Q supplied to the system minus the amount of thermodynamic work W done by system on its surroundings. The foregoing sign convention for work is used in the present article, but an alternate sign convention, followed by IUPAC, for work, is to consider the work performed on the system by its surroundings as positive. This is the convention adopted by many modern textbooks of physical chemistry, such as those by Peter Atkins and Ira Levine, but many textbooks on physics define work as work done by the system.

    Δ U = Q − W . \Delta U=Q-W\,.

This formula can be re-written so as to express a definition of quantity of energy transferred as heat, based purely on the concept of adiabatic work, if it is supposed that ΔU is defined and measured solely by processes of adiabatic work:

    Q = Δ U + W . Q=\Delta U+W.

The thermodynamic work done by the system is through mechanisms defined by its thermodynamic state variables, for example, its volume V, not through variables that necessarily involve mechanisms in the surroundings. The latter are such as shaft work, and include isochoric work.

The internal energy, U, is a state function. In cyclical processes, such as the operation of a heat engine, state functions of the working substance return to their initial values upon completion of a cycle.

The differential, or infinitesimal increment, for the internal energy in an infinitesimal process is an exact differential dU. The symbol for exact differentials is the lowercase letter d.

In contrast, neither of the infinitesimal increments δQ nor δW in an infinitesimal process represents the change in a state function of the system. Thus, infinitesimal increments of heat and work are inexact differentials. The lowercase Greek letter delta, δ, is the symbol for inexact differentials. The integral of any inexact differential in a process where the system leaves and then returns to the same thermodynamic state does not necessarily equal zero.

As recounted above, in the section headed heat and entropy, the second law of thermodynamics observes that if heat is supplied to a system in a reversible process, the increment of heat δQ and the temperature T form the exact differential

    d S = δ Q T , \mathrm {d} S={\frac {\delta Q}{T}},

and that S, the entropy of the working body, is a state function. Likewise, with a well-defined pressure, P, behind a slowly moving (quasistatic) boundary, the work differential, δW, and the pressure, P, combine to form the exact differential

    d V = δ W P , \mathrm {d} V={\frac {\delta W}{P}},

with V the volume of the system, which is a state variable. In general, for systems of uniform pressure and temperature without composition change,

    d U = T d S − P d V . \mathrm {d} U=T\mathrm {d} S-P\mathrm {d} V.

Associated with this differential equation is the concept that the internal energy may be considered to be a function U (S,V) of its natural variables S and V. The internal energy representation of the fundamental thermodynamic relation is written as[14][15]

    U = U ( S , V ) . U=U(S,V).

If V is constant

    T d S = d U ( V constant) T\mathrm {d} S=\mathrm {d} U\,\,\,\,\,\,\,\,\,\,\,\,(V\,\,{\text{constant)}}

and if P is constant

    T d S = d H ( P constant) T\mathrm {d} S=\mathrm {d} H\,\,\,\,\,\,\,\,\,\,\,\,(P\,\,{\text{constant)}}

with the enthalpy H defined by

    H = U + P V . H=U+PV.

The enthalpy may be considered to be a function H(S, P) of its natural variables S and P. The enthalpy representation of the fundamental thermodynamic relation is written[15][16]

    H = H ( S , P ) . H=H(S,P).

The internal energy representation and the enthalpy representation are partial Legendre transforms of one another. They contain the same physical information, written in different ways. Like the internal energy, the enthalpy stated as a function of its natural variables is a thermodynamic potential and contains all thermodynamic information about a body.[16][17]

If a quantity Q of heat is added to a body while it does only expansion work W on its surroundings, one has

    Δ H = Δ U + Δ ( P V ) . \Delta H=\Delta U+\Delta (PV)\,.

If this is constrained to happen at constant pressure, i.e. with ΔP = 0, the expansion work W done by the body is given by W = P ΔV; recalling the first law of thermodynamics, one has

    Δ U = Q − W = Q − P Δ V  and  Δ ( P V ) = P Δ V . \Delta U=Q-W=Q-P\,\Delta V{\text{ and }}\Delta (PV)=P\,\Delta V\,.

Consequently, by substitution one has

    Δ H = Q − P Δ V + P Δ V = Q at constant pressure without electrical work. {\displaystyle {\begin{aligned}\Delta H&=Q-P\,\Delta V+P\,\Delta V\\&=Q\qquad \qquad \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,{\text{at constant pressure without electrical work.}}\end{aligned}}}

In this scenario, the increase in enthalpy is equal to the quantity of heat added to the system. This is the basis of the determination of enthalpy changes in chemical reactions by calorimetry. Since many processes do take place at constant atmospheric pressure, the enthalpy is sometimes given the misleading name of 'heat content'[18] or heat function,[19] while it actually depends strongly on the energies of covalent bonds and intermolecular forces.

In terms of the natural variables S and P of the state function H, this process of change of state from state 1 to state 2 can be expressed as

    Δ H = ∫ S 1 S 2 ( ∂ H ∂ S ) P d S + ∫ P 1 P 2 ( ∂ H ∂ P ) S d P = ∫ S 1 S 2 ( ∂ H ∂ S ) P d S at constant pressure without electrical work. {\displaystyle {\begin{aligned}\Delta H&=\int _{S_{1}}^{S_{2}}\left({\frac {\partial H}{\partial S}}\right)_{P}\mathrm {d} S+\int _{P_{1}}^{P_{2}}\left({\frac {\partial H}{\partial P}}\right)_{S}\mathrm {d} P\\&=\int _{S_{1}}^{S_{2}}\left({\frac {\partial H}{\partial S}}\right)_{P}\mathrm {d} S\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,{\text{at constant pressure without electrical work.}}\end{aligned}}}

It is known that the temperature T(S, P) is identically stated by

    ( ∂ H ∂ S ) P ≡ T ( S , P ) . \left({\frac {\partial H}{\partial S}}\right)_{P}\equiv T(S,P)\,.

Consequently,

    Δ H = ∫ S 1 S 2 T ( S , P ) d S at constant pressure without electrical work. {\displaystyle \Delta H=\int _{S_{1}}^{S_{2}}T(S,P)\mathrm {d} S\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,{\text{at constant pressure without electrical work.}}}

In this case, the integral specifies a quantity of heat transferred at constant pressure.
History
Main article: History of thermodynamics

As a common noun, English heat or warmth (just as French chaleur, German Wärme, Latin calor, Greek θάλπος, etc.) refers to (the human perception of) either thermal energy or temperature. Speculation on thermal energy or "heat" as a separate form of matter has a long history, identified as caloric theory, phlogiston theory, and fire.

Heat has been discussed in ordinary language by philosophers. An example is this 1720 quote from John Locke:

                    Heat, is a very brisk agitation of the insensible parts of the object, which produces in us that sensation from whence we denominate the object hot; so what in our sensation is heat, in the object is nothing but motion. This appears by the way, whereby heat is produc’d: for we see that the rubbing of a brass nail upon a board, will make it very hot; and the axle-trees of carts and coaches are often hot, and sometimes to a degree, that it sets them on fire, by the rubbing of the nave of the wheel upon it.[20]

This source was repeatedly quoted by Joule. John Tyndall's Heat Considered as Mode of Motion (1863) was instrumental in popularizing the idea of heat as motion to the English-speaking public. The theory was developed in academic publications in French, English and German. From an early time, the French technical term chaleur used by Carnot was taken as equivalent to the English heat and German Wärme (lit. "warmth", while the equivalent of heat would be German Hitze).
Classical thermodynamics

The modern understanding of heat is often partly attributed to Thompson's 1798 mechanical theory of heat (An Experimental Enquiry Concerning the Source of the Heat which is Excited by Friction), postulating a mechanical equivalent of heat. A collaboration between Nicolas Clément and Sadi Carnot (Reflections on the Motive Power of Fire) in the 1820s had some related thinking along similar lines.[21] In 1842, Julius Robert Mayer frictionally generated heat in paper pulp and measured the temperature rise.[22] In 1845, Joule published a paper entitled The Mechanical Equivalent of Heat, in which he specified a numerical value for the amount of mechanical work required to "produce a unit of heat", based on heat production by friction in the passage of electricity through a resistor and in the rotation of a paddle in a vat of water.[23] The theory of classical thermodynamics matured in the 1850s to 1860s.
Clausius (1850)

In 1850, Clausius, responding to Joule's experimental demonstrations of heat production by friction, rejected the caloric doctrine of conservation of heat, writing:

                    If we assume that heat, like matter, cannot be lessened in quantity, we must also assume that it cannot be increased; but it is almost impossible to explain the ascension of temperature brought about by friction otherwise than by assuming an actual increase of heat. The careful experiments of Joule, who developed heat in various ways by the application of mechanical force, establish almost to a certainty, not only the possibility of increasing the quantity of heat, but also the fact that the newly-produced heat is proportional to the work expended in its production. It may be remarked further, that many facts have lately transpired which tend to overthrow the hypothesis that heat is itself a body, and to prove that it consists in a motion of the ultimate particles of bodies.[24]

The process function Q was introduced by Rudolf Clausius in 1850. Clausius described it with the German compound Wärmemenge, translated as "amount of heat".[24]

James Clerk Maxwell in his 1871 Theory of Heat outlines four stipulations for the definition of heat:

    It is something which may be transferred from one body to another, according to the second law of thermodynamics.
    It is a measurable quantity, and so can be treated mathematically.
    It cannot be treated as a material substance, because it may be transformed into something that is not a material substance, e.g., mechanical work.
    Heat is one of the forms of energy.[25]

The process function Q is referred to as Wärmemenge by Clausius, or as "amount of heat" in translation. Use of "heat" as an abbreviated form of the specific concept of "quantity of energy transferred as heat" led to some terminological confusion by the early 20th century. The generic meaning of "heat", even in classical thermodynamics, is just "thermal energy".[26] Since the 1920s, it has been recommended practice to use enthalpy to refer to the "heat content at constant volume", and to thermal energy when "heat" in the general sense is intended, while "heat" is reserved for the very specific context of the transfer of thermal energy between two systems. Leonard Benedict Loeb in his Kinetic Theory of Gases (1927) makes a point of using "quantity of heat" or "heat–quantity" when referring to Q:[27]

                    After the perfection of thermometry [...] the next great advance made in the field of heat was the definition of a term which is called the quantity of heat. [... after the abandonment of caloric theory,] It still remains to interpret this very definite concept, the quantity of heat, in terms of a theory ascribing all heat to the kinetics of gas molecules.[28]

Richard Feynman introduced heat with a physical depiction, as associated with the jiggling motion of atoms and molecules, with faster motion corresponding to increased temperature.[29] To explain physics further, he used the term "heat energy,"[30] along with "heat".[31]
Bryan (1907)

In 1907, G.H. Bryan published an investigation of the foundations of thermodynamics, Thermodynamics: an Introductory Treatise dealing mainly with First Principles and their Direct Applications, B.G. Teubner, Leipzig.

Bryan was writing when thermodynamics had been established empirically, but people were still interested to specify its logical structure. The 1909 work of Carathéodory also belongs to this historical era. Bryan was a physicist while Carathéodory was a mathematician.

Bryan started his treatise with an introductory chapter on the notions of heat and of temperature. He gives an example of where the notion of heating as raising a body's temperature contradicts the notion of heating as imparting a quantity of heat to that body.

He defined an adiabatic transformation as one in which the body neither gains nor loses heat. This is not quite the same as defining an adiabatic transformation as one that occurs to a body enclosed by walls impermeable to radiation and conduction.

He recognized calorimetry as a way of measuring quantity of heat. He recognized water as having a temperature of maximum density. This makes water unsuitable as a thermometric substance around that temperature. He intended to remind readers of why thermodynamicists preferred an absolute scale of temperature, independent of the properties of a particular thermometric substance.

His second chapter started with the recognition of friction as a source of heat, by Benjamin Thompson, by Humphry Davy, by Robert Mayer, and by James Prescott Joule.

He stated the First Law of Thermodynamics, or Mayer–Joule Principle as follows:

                    When heat is transformed into work or conversely work is transformed into heat, the quantity of heat gained or lost is proportional to the quantity of work lost or gained.[32]

He wrote:

                    If heat be measured in dynamical units the mechanical equivalent becomes equal to unity, and the equations of thermodynamics assume a simpler and more symmetrical form.[32]

He explained how the caloric theory of Lavoisier and Laplace made sense in terms of pure calorimetry, though it failed to account for conversion of work into heat by such mechanisms as friction and conduction of electricity.

Having rationally defined quantity of heat, he went on to consider the second law, including the Kelvin definition of absolute thermodynamic temperature.

In section 41, he wrote:

                             §41. Physical unreality of reversible processes. In Nature all phenomena are irreversible in a greater or less degree. The motions of celestial bodies afford the closest approximations to reversible motions, but motions which occur on this earth are largely retarded by friction, viscosity, electric and other resistances, and if the relative velocities of moving bodies were reversed, these resistances would still retard the relative motions and would not accelerate them as they should if the motions were perfectly reversible.[32]

He then stated the principle of conservation of energy.

He then wrote:

                    In connection with irreversible phenomena the following axioms have to be assumed.
                             (1) If a system can undergo an irreversible change it will do so.
                             (2) A perfectly reversible change cannot take place of itself; such a change can only be regarded as the limiting form of an irreversible change.[32]

On page 46, thinking of closed systems in thermal connection, he wrote:

                    We are thus led to postulate a system in which energy can pass from one element to another otherwise than by the performance of mechanical work.[32]

On page 47, still thinking of closed systems in thermal connection, he wrote:

                             §58. Quantity of Heat. Definition. When energy flows from one system or part of a system to another otherwise than by the performance of work, the energy so transferred i[s] called heat.[32]

On page 48, he wrote:

                             § 59. When two bodies act thermically on one another the quantities of heat gained by one and lost by the other are not necessarily equal.
                             In the case of bodies at a distance, heat may be taken from or given to the intervening medium.
                             The quantity of heat received by any portion of the ether may be defined in the same way as that received by a material body. [He was thinking of thermal radiation.]
                             Another important exception occurs when sliding takes place between two rough bodies in contact. The algebraic sum of the works done is different from zero, because, although the action and reaction are equal and opposite the velocities of the parts of the bodies in contact are different. Moreover, the work lost in the process does not increase the mutual potential energy of the system and there is no intervening medium between the bodies. Unless the lost energy can be accounted for in other ways, (as when friction produces electrification), it follows from the Principle of Conservation of Energy that the algebraic sum of the quantities of heat gained by the two systems is equal to the quantity of work lost by friction. [This thought was echoed by Bridgman, as above.][32]

Carathéodory (1909)

A celebrated and frequent definition of heat in thermodynamics is based on the work of Carathéodory (1909), referring to processes in a closed system.[33][34][35][36][37][38] Carathéodory was responding to a suggestion by Max Born that he examine the logical structure of thermodynamics.

The internal energy UX of a body in an arbitrary state X can be determined by amounts of work adiabatically performed by the body on its surroundings when it starts from a reference state O. Such work is assessed through quantities defined in the surroundings of the body. It is supposed that such work can be assessed accurately, without error due to friction in the surroundings; friction in the body is not excluded by this definition. The adiabatic performance of work is defined in terms of adiabatic walls, which allow transfer of energy as work, but no other transfer, of energy or matter. In particular they do not allow the passage of energy as heat. According to this definition, work performed adiabatically is in general accompanied by friction within the thermodynamic system or body. On the other hand, according to Carathéodory (1909), there also exist non-adiabatic, diathermal walls, which are postulated to be permeable only to heat.

For the definition of quantity of energy transferred as heat, it is customarily envisaged that an arbitrary state of interest Y is reached from state O by a process with two components, one adiabatic and the other not adiabatic. For convenience one may say that the adiabatic component was the sum of work done by the body through volume change through movement of the walls while the non-adiabatic wall was temporarily rendered adiabatic, and of isochoric adiabatic work. Then the non-adiabatic component is a process of energy transfer through the wall that passes only heat, newly made accessible for the purpose of this transfer, from the surroundings to the body. The change in internal energy to reach the state Y from the state O is the difference of the two amounts of energy transferred.

Although Carathéodory himself did not state such a definition, following his work it is customary in theoretical studies to define heat, Q, to the body from its surroundings, in the combined process of change to state Y from the state O, as the change in internal energy, ΔUY, minus the amount of work, W, done by the body on its surrounds by the adiabatic process, so that Q = ΔUY − W.

In this definition, for the sake of conceptual rigour, the quantity of energy transferred as heat is not specified directly in terms of the non-adiabatic process. It is defined through knowledge of precisely two variables, the change of internal energy and the amount of adiabatic work done, for the combined process of change from the reference state O to the arbitrary state Y. It is important that this does not explicitly involve the amount of energy transferred in the non-adiabatic component of the combined process. It is assumed here that the amount of energy required to pass from state O to state Y, the change of internal energy, is known, independently of the combined process, by a determination through a purely adiabatic process, like that for the determination of the internal energy of state X above. The rigour that is prized in this definition is that there is one and only one kind of energy transfer admitted as fundamental: energy transferred as work. Energy transfer as heat is considered as a derived quantity. The uniqueness of work in this scheme is considered to guarantee rigor and purity of conception. The conceptual purity of this definition, based on the concept of energy transferred as work as an ideal notion, relies on the idea that some frictionless and otherwise non-dissipative processes of energy transfer can be realized in physical actuality. The second law of thermodynamics, on the other hand, assures us that such processes are not found in nature.

Before the rigorous mathematical definition of heat based on Carathéodory's 1909 paper, historically, heat, temperature, and thermal equilibrium were presented in thermodynamics textbooks as jointly primitive notions.[39] Carathéodory introduced his 1909 paper thus: "The proposition that the discipline of thermodynamics can be justified without recourse to any hypothesis that cannot be verified experimentally must be regarded as one of the most noteworthy results of the research in thermodynamics that was accomplished during the last century." Referring to the "point of view adopted by most authors who were active in the last fifty years", Carathéodory wrote: "There exists a physical quantity called heat that is not identical with the mechanical quantities (mass, force, pressure, etc.) and whose variations can be determined by calorimetric measurements." James Serrin introduces an account of the theory of thermodynamics thus: "In the following section, we shall use the classical notions of heat, work, and hotness as primitive elements, ... That heat is an appropriate and natural primitive for thermodynamics was already accepted by Carnot. Its continued validity as a primitive element of thermodynamical structure is due to the fact that it synthesizes an essential physical concept, as well as to its successful use in recent work to unify different constitutive theories."[40][41] This traditional kind of presentation of the basis of thermodynamics includes ideas that may be summarized by the statement that heat transfer is purely due to spatial non-uniformity of temperature, and is by conduction and radiation, from hotter to colder bodies. It is sometimes proposed that this traditional kind of presentation necessarily rests on "circular reasoning".

This alternative approach to the definition of quantity of energy transferred as heat differs in logical structure from that of Carathéodory, recounted just above.

This alternative approach admits calorimetry as a primary or direct way to measure quantity of energy transferred as heat. It relies on temperature as one of its primitive concepts, and used in calorimetry.[42] It is presupposed that enough processes exist physically to allow measurement of differences in internal energies. Such processes are not restricted to adiabatic transfers of energy as work. They include calorimetry, which is the commonest practical way of finding internal energy differences.[43] The needed temperature can be either empirical or absolute thermodynamic.

In contrast, the Carathéodory way recounted just above does not use calorimetry or temperature in its primary definition of quantity of energy transferred as heat. The Carathéodory way regards calorimetry only as a secondary or indirect way of measuring quantity of energy transferred as heat. As recounted in more detail just above, the Carathéodory way regards quantity of energy transferred as heat in a process as primarily or directly defined as a residual quantity. It is calculated from the difference of the internal energies of the initial and final states of the system, and from the actual work done by the system during the process. That internal energy difference is supposed to have been measured in advance through processes of purely adiabatic transfer of energy as work, processes that take the system between the initial and final states. By the Carathéodory way it is presupposed as known from experiment that there actually physically exist enough such adiabatic processes, so that there need be no recourse to calorimetry for measurement of quantity of energy transferred as heat. This presupposition is essential but is explicitly labeled neither as a law of thermodynamics nor as an axiom of the Carathéodory way. In fact, the actual physical existence of such adiabatic processes is indeed mostly supposition, and those supposed processes have in most cases not been actually verified empirically to exist.[44]
Planck (1926)

Over the years, for example in his 1879 thesis, but particularly in 1926, Planck advocated regarding the generation of heat by rubbing as the most specific way to define heat.[45] Planck criticised Carathéodory for not attending to this.[46] Carathéodory was a mathematician who liked to think in terms of adiabatic processes, and perhaps found friction to tricky to think about, while Planck was a physicist.
Heat transfer
Main article: Heat transfer
Heat transfer between two bodies

Referring to conduction, Partington writes: "If a hot body is brought in conducting contact with a cold body, the temperature of the hot body falls and that of the cold body rises, and it is said that a quantity of heat has passed from the hot body to the cold body."[47]

Referring to radiation, Maxwell writes: "In Radiation, the hotter body loses heat, and the colder body receives heat by means of a process occurring in some intervening medium which does not itself thereby become hot."[48]

Maxwell writes that convection as such "is not a purely thermal phenomenon".[49] In thermodynamics, convection in general is regarded as transport of internal energy. If, however, the convection is enclosed and circulatory, then it may be regarded as an intermediary that transfers energy as heat between source and destination bodies, because it transfers only energy and not matter from the source to the destination body.[50]

In accordance with the first law for closed systems, energy transferred solely as heat leaves one body and enters another, changing the internal energies of each. Transfer, between bodies, of energy as work is a complementary way of changing internal energies. Though it is not logically rigorous from the viewpoint of strict physical concepts, a common form of words that expresses this is to say that heat and work are interconvertible.

Cyclically operating engines that use only heat and work transfers have two thermal reservoirs, a hot and a cold one. They may be classified by the range of operating temperatures of the working body, relative to those reservoirs. In a heat engine, the working body is at all times colder than the hot reservoir and hotter than the cold reservoir. In a sense, it uses heat transfer to produce work. In a heat pump, the working body, at stages of the cycle, goes both hotter than the hot reservoir, and colder than the cold reservoir. In a sense, it uses work to produce heat transfer.
Heat engine

In classical thermodynamics, a commonly considered model is the heat engine. It consists of four bodies: the working body, the hot reservoir, the cold reservoir, and the work reservoir. A cyclic process leaves the working body in an unchanged state, and is envisaged as being repeated indefinitely often. Work transfers between the working body and the work reservoir are envisaged as reversible, and thus only one work reservoir is needed. But two thermal reservoirs are needed, because transfer of energy as heat is irreversible. A single cycle sees energy taken by the working body from the hot reservoir and sent to the two other reservoirs, the work reservoir and the cold reservoir. The hot reservoir always and only supplies energy and the cold reservoir always and only receives energy. The second law of thermodynamics requires that no cycle can occur in which no energy is received by the cold reservoir. Heat engines achieve higher efficiency when the ratio of the initial and final temperature is greater.
Heat pump or refrigerator

Another commonly considered model is the heat pump or refrigerator. Again there are four bodies: the working body, the hot reservoir, the cold reservoir, and the work reservoir. A single cycle starts with the working body colder than the cold reservoir, and then energy is taken in as heat by the working body from the cold reservoir. Then the work reservoir does work on the working body, adding more to its internal energy, making it hotter than the hot reservoir. The hot working body passes heat to the hot reservoir, but still remains hotter than the cold reservoir. Then, by allowing it to expand without passing heat to another body, the working body is made colder than the cold reservoir. It can now accept heat transfer from the cold reservoir to start another cycle.

The device has transported energy from a colder to a hotter reservoir, but this is not regarded as by an inanimate agency; rather, it is regarded as by the harnessing of work . This is because work is supplied from the work reservoir, not just by a simple thermodynamic process, but by a cycle of thermodynamic operations and processes, which may be regarded as directed by an animate or harnessing agency. Accordingly, the cycle is still in accord with the second law of thermodynamics. The 'efficiency' of a heat pump (which exceeds unity) is best when the temperature difference between the hot and cold reservoirs is least.

Functionally, such engines are used in two ways, distinguishing a target reservoir and a resource or surrounding reservoir. A heat pump transfers heat to the hot reservoir as the target from the resource or surrounding reservoir. A refrigerator transfers heat, from the cold reservoir as the target, to the resource or surrounding reservoir. The target reservoir may be regarded as leaking: when the target leaks heat to the surroundings, heat pumping is used; when the target leaks coldness to the surroundings, refrigeration is used. The engines harness work to overcome the leaks.
Macroscopic view
	
This section may need to be rewritten to comply with Wikipedia's quality standards. You can help. The talk page may contain suggestions. (May 2016)

According to Planck, there are three main conceptual approaches to heat.[51] One is the microscopic or kinetic theory approach. The other two are macroscopic approaches. One of the macroscopic approaches is through the law of conservation of energy taken as prior to thermodynamics, with a mechanical analysis of processes, for example in the work of Helmholtz. This mechanical view is taken in this article as currently customary for thermodynamic theory. The other macroscopic approach is the thermodynamic one, which admits heat as a primitive concept, which contributes, by scientific induction[52] to knowledge of the law of conservation of energy. This view is widely taken as the practical one, quantity of heat being measured by calorimetry.

Bailyn also distinguishes the two macroscopic approaches as the mechanical and the thermodynamic.[53] The thermodynamic view was taken by the founders of thermodynamics in the nineteenth century. It regards quantity of energy transferred as heat as a primitive concept coherent with a primitive concept of temperature, measured primarily by calorimetry. A calorimeter is a body in the surroundings of the system, with its own temperature and internal energy; when it is connected to the system by a path for heat transfer, changes in it measure heat transfer. The mechanical view was pioneered by Helmholtz and developed and used in the twentieth century, largely through the influence of Max Born.[54] It regards quantity of heat transferred as heat as a derived concept, defined for closed systems as quantity of heat transferred by mechanisms other than work transfer, the latter being regarded as primitive for thermodynamics, defined by macroscopic mechanics. According to Born, the transfer of internal energy between open systems that accompanies transfer of matter "cannot be reduced to mechanics".[55] It follows that there is no well-founded definition of quantities of energy transferred as heat or as work associated with transfer of matter.

Nevertheless, for the thermodynamical description of non-equilibrium processes, it is desired to consider the effect of a temperature gradient established by the surroundings across the system of interest when there is no physical barrier or wall between system and surroundings, that is to say, when they are open with respect to one another. The impossibility of a mechanical definition in terms of work for this circumstance does not alter the physical fact that a temperature gradient causes a diffusive flux of internal energy, a process that, in the thermodynamic view, might be proposed as a candidate concept for transfer of energy as heat.

In this circumstance, it may be expected that there may also be active other drivers of diffusive flux of internal energy, such as gradient of chemical potential which drives transfer of matter, and gradient of electric potential which drives electric current and iontophoresis; such effects usually interact with diffusive flux of internal energy driven by temperature gradient, and such interactions are known as cross-effects.[56]

If cross-effects that result in diffusive transfer of internal energy were also labeled as heat transfers, they would sometimes violate the rule that pure heat transfer occurs only down a temperature gradient, never up one. They would also contradict the principle that all heat transfer is of one and the same kind, a principle founded on the idea of heat conduction between closed systems. One might to try to think narrowly of heat flux driven purely by temperature gradient as a conceptual component of diffusive internal energy flux, in the thermodynamic view, the concept resting specifically on careful calculations based on detailed knowledge of the processes and being indirectly assessed. In these circumstances, if perchance it happens that no transfer of matter is actualized, and there are no cross-effects, then the thermodynamic concept and the mechanical concept coincide, as if one were dealing with closed systems. But when there is transfer of matter, the exact laws by which temperature gradient drives diffusive flux of internal energy, rather than being exactly knowable, mostly need to be assumed, and in many cases are practically unverifiable. Consequently, when there is transfer of matter, the calculation of the pure 'heat flux' component of the diffusive flux of internal energy rests on practically unverifiable assumptions.[57][quotations 1][58] This is a reason to think of heat as a specialized concept that relates primarily and precisely to closed systems, and applicable only in a very restricted way to open systems.

In many writings in this context, the term "heat flux" is used when what is meant is therefore more accurately called diffusive flux of internal energy; such usage of the term "heat flux" is a residue of older and now obsolete language usage that allowed that a body may have a "heat content".[59]
Microscopic view

In the kinetic theory, heat is explained in terms of the microscopic motions and interactions of constituent particles, such as electrons, atoms, and molecules.[60] The immediate meaning of the kinetic energy of the constituent particles is not as heat. It is as a component of internal energy. In microscopic terms, heat is a transfer quantity, and is described by a transport theory, not as steadily localized kinetic energy of particles. Heat transfer arises from temperature gradients or differences, through the diffuse exchange of microscopic kinetic and potential particle energy, by particle collisions and other interactions. An early and vague expression of this was made by Francis Bacon.[61][62] Precise and detailed versions of it were developed in the nineteenth century.[63]

In statistical mechanics, for a closed system (no transfer of matter), heat is the energy transfer associated with a disordered, microscopic action on the system, associated with jumps in occupation numbers of the energy levels of the system, without change in the values of the energy levels themselves.[64] It is possible for macroscopic thermodynamic work to alter the occupation numbers without change in the values of the system energy levels themselves, but what distinguishes transfer as heat is that the transfer is entirely due to disordered, microscopic action, including radiative transfer. A mathematical definition can be formulated for small increments of quasi-static adiabatic work in terms of the statistical distribution of an ensemble of microstates.
Calorimetry
Main article: Calorimetry

Quantity of heat transferred can be measured by calorimetry, or determined through calculations based on other quantities.

Calorimetry is the empirical basis of the idea of quantity of heat transferred in a process. The transferred heat is measured by changes in a body of known properties, for example, temperature rise, change in volume or length, or phase change, such as melting of ice.[65][66]

A calculation of quantity of heat transferred can rely on a hypothetical quantity of energy transferred as adiabatic work and on the first law of thermodynamics. Such calculation is the primary approach of many theoretical studies of quantity of heat transferred.[33][67][68]
Engineering
	
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2016) (Learn how and when to remove this template message)
A red-hot iron rod from which heat transfer to the surrounding environment will be primarily through radiation.

The discipline of heat transfer, typically considered an aspect of mechanical engineering and chemical engineering, deals with specific applied methods by which thermal energy in a system is generated, or converted, or transferred to another system. Although the definition of heat implicitly means the transfer of energy, the term heat transfer encompasses this traditional usage in many engineering disciplines and laymen language.

Heat transfer is generally described as including the mechanisms of heat conduction, heat convection, thermal radiation, but may include mass transfer and heat in processes of phase changes.

Convection may be described as the combined effects of conduction and fluid flow. From the thermodynamic point of view, heat flows into a fluid by diffusion to increase its energy, the fluid then transfers (advects) this increased internal energy (not heat) from one location to another, and this is then followed by a second thermal interaction which transfers heat to a second body or system, again by diffusion. This entire process is often regarded as an additional mechanism of heat transfer, although technically, "heat transfer" and thus heating and cooling occurs only on either end of such a conductive flow, but not as a result of flow. Thus, conduction can be said to "transfer" heat only as a net result of the process, but may not do so at every time within the complicated convective process.
Latent and sensible heat
Joseph Black

In an 1847 lecture entitled On Matter, Living Force, and Heat, James Prescott Joule characterized the terms latent heat and sensible heat as components of heat each affecting distinct physical phenomena, namely the potential and kinetic energy of particles, respectively.[69][quotations 2] He described latent energy as the energy possessed via a distancing of particles where attraction was over a greater distance, i.e. a form of potential energy, and the sensible heat as an energy involving the motion of particles, i.e. kinetic energy.

Latent heat is the heat released or absorbed by a chemical substance or a thermodynamic system during a change of state that occurs without a change in temperature. Such a process may be a phase transition, such as the melting of ice or the boiling of water.[70][71]
Heat capacity

Heat capacity is a measurable physical quantity equal to the ratio of the heat added to an object to the resulting temperature change.[72] The molar heat capacity is the heat capacity per unit amount (SI unit: mole) of a pure substance, and the specific heat capacity, often called simply specific heat, is the heat capacity per unit mass of a material. Heat capacity is a physical property of a substance, which means that it depends on the state and properties of the substance under consideration.

The specific heats of monatomic gases, such as helium, are nearly constant with temperature. Diatomic gases such as hydrogen display some temperature dependence, and triatomic gases (e.g., carbon dioxide) still more.

Before the development of the laws of thermodynamics, heat was measured by changes in the states of the participating bodies.

Some general rules, with important exceptions, can be stated as follows.

In general, most bodies expand on heating. In this circumstance, heating a body at a constant volume increases the pressure it exerts on its constraining walls, while heating at a constant pressure increases its volume.

Beyond this, most substances have three ordinarily recognized states of matter, solid, liquid, and gas. Some can also exist in a plasma. Many have further, more finely differentiated, states of matter, such as glass and liquid crystal. In many cases, at fixed temperature and pressure, a substance can exist in several distinct states of matter in what might be viewed as the same 'body'. For example, ice may float in a glass of water. Then the ice and the water are said to constitute two phases within the 'body'. Definite rules are known, telling how distinct phases may coexist in a 'body'. Mostly, at a fixed pressure, there is a definite temperature at which heating causes a solid to melt or evaporate, and a definite temperature at which heating causes a liquid to evaporate. In such cases, cooling has the reverse effects.

All of these, the commonest cases, fit with a rule that heating can be measured by changes of state of a body. Such cases supply what are called thermometric bodies, that allow the definition of empirical temperatures. Before 1848, all temperatures were defined in this way. There was thus a tight link, apparently logically determined, between heat and temperature, though they were recognized as conceptually thoroughly distinct, especially by Joseph Black in the later eighteenth century.

There are important exceptions. They break the obviously apparent link between heat and temperature. They make it clear that empirical definitions of temperature are contingent on the peculiar properties of particular thermometric substances, and are thus precluded from the title 'absolute'. For example, water contracts on being heated near 277 K. It cannot be used as a thermometric substance near that temperature. Also, over a certain temperature range, ice contracts on heating. Moreover, many substances can exist in metastable states, such as with negative pressure, that survive only transiently and in very special conditions. Such facts, sometimes called 'anomalous', are some of the reasons for the thermodynamic definition of absolute temperature.

In the early days of measurement of high temperatures, another factor was important, and used by Josiah Wedgwood in his pyrometer. The temperature reached in a process was estimated by the shrinkage of a sample of clay. The higher the temperature, the more the shrinkage. This was the only available more or less reliable method of measurement of temperatures above 1000 °C (1,832 °F). But such shrinkage is irreversible. The clay does not expand again on cooling. That is why it could be used for the measurement. But only once. It is not a thermometric material in the usual sense of the word.

Nevertheless, the thermodynamic definition of absolute temperature does make essential use of the concept of heat, with proper circumspection.
"Hotness"

The property of hotness is a concern of thermodynamics that should be defined without reference to the concept of heat. Consideration of hotness leads to the concept of empirical temperature.[73][74] All physical systems are capable of heating or cooling others.[75] With reference to hotness, the comparative terms hotter and colder are defined by the rule that heat flows from the hotter body to the colder.[76][77][78]

If a physical system is inhomogeneous or very rapidly or irregularly changing, for example by turbulence, it may be impossible to characterize it by a temperature, but still there can be transfer of energy as heat between it and another system. If a system has a physical state that is regular enough, and persists long enough to allow it to reach thermal equilibrium with a specified thermometer, then it has a temperature according to that thermometer. An empirical thermometer registers degree of hotness for such a system. Such a temperature is called empirical.[79][80][81] For example, Truesdell writes about classical thermodynamics: "At each time, the body is assigned a real number called the temperature. This number is a measure of how hot the body is."[82]

Physical systems that are too turbulent to have temperatures may still differ in hotness. A physical system that passes heat to another physical system is said to be the hotter of the two. More is required for the system to have a thermodynamic temperature. Its behavior must be so regular that its empirical temperature is the same for all suitably calibrated and scaled thermometers, and then its hotness is said to lie on the one-dimensional hotness manifold. This is part of the reason why heat is defined following Carathéodory and Born, solely as occurring other than by work or transfer of matter; temperature is advisedly and deliberately not mentioned in this now widely accepted definition.

This is also the reason that the zeroth law of thermodynamics is stated explicitly. If three physical systems, A, B, and C are each not in their own states of internal thermodynamic equilibrium, it is possible that, with suitable physical connections being made between them, A can heat B and B can heat C and C can heat A. In non-equilibrium situations, cycles of flow are possible. It is the special and uniquely distinguishing characteristic of internal thermodynamic equilibrium that this possibility is not open to thermodynamic systems (as distinguished amongst physical systems) which are in their own states of internal thermodynamic equilibrium; this is the reason why the zeroth law of thermodynamics needs explicit statement. That is to say, the relation 'is not colder than' between general non-equilibrium physical systems is not transitive, whereas, in contrast, the relation 'has no lower a temperature than' between thermodynamic systems in their own states of internal thermodynamic equilibrium is transitive. It follows from this that the relation 'is in thermal equilibrium with' is transitive, which is one way of stating the zeroth law.

Just as temperature may be undefined for a sufficiently inhomogeneous system, so also may entropy be undefined for a system not in its own state of internal thermodynamic equilibrium. For example, 'the temperature of the solar system' is not a defined quantity. Likewise, 'the entropy of the solar system' is not defined in classical thermodynamics. It has not been possible to define non-equilibrium entropy, as a simple number for a whole system, in a clearly satisfactory way.[83]
See also

    iconEnergy portal

    Effect of sun angle on climate
    Heat death of the Universe
    Heat diffusion
    Heat equation
    Heat exchanger
    Heat wave
    Heat flux sensor
    Heat transfer coefficient
    History of heat
    Orders of magnitude (temperature)
    Sigma heat
    Shock heating
    Thermal battery
    Thermal management of electronic devices and systems
    Thermometer
    Relativistic heat conduction
    Uniform Mechanical Code
    Uniform Solar Energy and Hydronics Code
    Waste heat

References

Van Wylen, Gordon; Sonntag, Richard (1978). Fundamentals of Classical Thermodynamics (Second edition, SI Version, Revised Printing ed.). Chapter 4.7, Definition of Heat: John Wiley & Sons. p. 76. ISBN 0-471-04188-2.
D.V. Schroeder (1999). An Introduction to Thermal Physics. Addison-Wesley. p. 15. ISBN 0-201-38027-7.
Herbert B. Callen (1985). Thermodynamics and an Introduction to Thermostatics (2 ed.). John Wiley & Sons. http://cvika.grimoar.cz/callen/ Archived 17 October 2018 at the Wayback Machine or http://keszei.chem.elte.hu/1alapFizkem/H.B.Callen-Thermodynamics.pdf Archived 30 December 2016 at the Wayback Machine, p. 8: Energy may be transferred via ... work. "But it is equally possible to transfer energy via the hidden atomic modes of motion as well as via those that happen to be macroscopically observable. An energy transfer via the hidden atomic modes is called heat."
Callen, p.19
Maxwell, J.C. (1871), Chapter III.
Macquorn Rankine in the same year used the same symbol. The two physicists were in correspondence at the time, so that it is difficult to say which of the two first introduced the symbol. (Kenneth L. Caneva, Helmholtz and the Conservation of Energy: Contexts of Creation and Reception (2021), p. 562.
Baierlein, R. (1999), p. 21.
Clausius, R. (1854).
Clausius, R. (1865), pp. 125–126.
De Groot, S.R., Mazur, P. (1962), p. 20.
Kondepudi, D, Prigogine, I. (1998), p. 82.
Kondepudi, D. (2008), p. 114.
Lebon, g., Jou, D., Casas-Vásquez, J. (2008), p. 41.
Callen, H.B., (1985), Section 2-3, pp. 40–42.
Adkins, C.J. (1983), p. 101.
Callen, H.B. (1985), p. 147.
Adkins, C.J. (1983), pp. 100–104.
Adkins, C.J. (1968/1983), p. 46.
Bailyn, M. (1994), p. 208.
Locke, J. (1720). From page 224 of A Collection of several Pieces of Mr. John Locke, Never before printed, or not extant in his Works, edited by an author not named in the copy that I found, (https://play.google.com/books/reader?id=QqxsP-VKrpkC&pg=GBS.PA215&hl=en_GB), published in London, printed by J. Bettenham for R. Francklin, in 1720, price 5 shillings, in the section 'Elements of Natural Philosophy', Chapter XI, ‘Of the five senses’. Sourced from a reference in the Wikipedia article on Benjamin Thompson’s https://en.wikipedia.org/wiki/An_Experimental_Enquiry_Concerning_the_Source_of_the_Heat_which_is_Excited_by_Friction.
Lervig, P. Sadi Carnot and the steam engine:Nicolas Clément's lectures on industrial chemistry, 1823–28. Br. J Hist. Sci. 18:147, 1985.
Blundell, S.J., Blundell, K.M. (2006). Concepts in Thermal Physics, Oxford University Press, Oxford UK, ISBN 9780198567691, p. 106.
Joule, J.P. (1845)."On the Mechanical Equivalent of Heat". Philosophical Transactions of the Royal Society of London. 140: 61–82. 1850. doi:10.1098/rstl.1850.0004.
Die Wärmemenge, welche dem Gase mitgetheilt werden muss, während es aus irgend einem früheren Zustande auf einem bestimmten Wege in den Zustand übergeführt wird, in welchem sein Volumen = v und seine Temperatur = t ist, möge Q heissen (R. Clausius, Ueber die bewegende Kraft der Wärme und die Gesetze, welche sich daraus für die Wärmelehre selbst ableiten lassen Archived 17 April 2019 at the Wayback Machine, communication to the Academy of Berlin, February 1850, published in Pogendorff's Annalen vol. 79, March/April 1850, first translated in Philosophical Magazine vol. 2, July 1851, as "First Memoir" in: The Mechanical Theory of Heat, with its Applications to the Steam-Engine and to the Physical Properties of Bodies, trans. John Tyndall, London, 1867, p. 25).
Maxwell, J.C. (1871), p. 7.
"in a gas, heat is nothing else than the kinetic or mechanical energy of motion of the gas molecules". B.L. Loeb, The Kinetic Theory of Gases (1927), p. 14.
From this terminological choice may derive a tradition to the effect that the letter Q represents "quantity", but there is no indication that Clausius had this in mind when he selected the letter in what seemed to be an ad hoc calculation in 1850.
B.L. Loeb, The Kinetic Theory of Gases (1927), p. 426 Archived 24 June 2018 at the Wayback Machine.
Feynman, Richard; Leighton, Robert; Sands, Matthew (1963). The Feynman Lectures on Physics, Volume 1 (Library of Congress number 63-20717, fourth printing, 1966 ed.). Chapter 1-2, Matter is made of atoms: Addison-Wesley Publishing Company. p. 1-3.
Feynman, Richard; Leighton, Robert; Sands, Matthew (1963). The Feynman Lectures on Physics, Volume 1 (Library of Congress number 63-20717, fourth printing, 1966 ed.). Chapter 4-1, What is energy: Addison-Wesley Publishing Company. p. 4-2.
Feynman, Richard; Leighton, Robert; Sands, Matthew (1963). The Feynman Lectures on Physics, Volume 1 (Library of Congress number 63-20717, fourth printing, 1966 ed.). Chapter 13: Addison-Wesley Publishing Company. p. 13-3.
Bryan, George Hartley (1907). "Thermodynamics, an introductory treatise dealing mainly with first principles and their direct applications". Leipzig, Teubner. Retrieved 23 June 2023.Public Domain This article incorporates text from this source, which is in the public domain.
Carathéodory, C. (1909).
Adkins, C.J. (1968/1983).
Münster, A. (1970).
Pippard, A.B. (1957).
Fowler, R., Guggenheim, E.A. (1939).
Buchdahl, H.A. (1966).
Lieb, E.H., Yngvason, J. (1999), p. 10.
Serrin, J. (1986), p. 5.
Owen, D.R. (1984), pp. 43–45.
Maxwell, J.C. (1871), p.v.
Atkins, P., de Paula, J. (1978/2010), p. 54.
Pippard, A.B. (1957/1966), p. 15.
Planck, M. (1926). 'Über die Begründung des zweiten Hauptsatzes der Thermodynamik', Sitzungsber. Preuss. Akad. Wiss., Phys. Math. Kl., 453—463.
Lieb & Yngvason (1999).
Partington, J.R. (1949), p. 118.
Maxwell, J.C. (1871), p. 10.
Maxwell, J.C. (1871), p. 11.
Chandrasekhar, S. (1961).
Planck, M. (1897/1903), p. viii.
Hintikka, J. (1988), p. 180.
Bailyn, M. (1994), pp. 65, 79.
Born, M.(1949), Lecture V.
Born, M. (1949), p. 44.
De Groot, S.R., Mazur, P. (1962), p. 30.
Denbigh, K.G. (1951), p. 56.
Fitts, D.D. (1962), p. 28.
Gyarmati, I. (1970), p. 68.
Kittel, C. Kroemer, H. (1980).
Bacon, F. (1620).
Partington, J.R. (1949), p. 131.
Partington, J.R. (1949), pp. 132–136.
Reif (1965), pp. 67–68
Maxwell J.C. (1872), p. 54.
Planck (1927), Chapter 3.
Bryan, G.H. (1907), p. 47.
Callen, H.B. (1985), Section 1-8.
Joule J.P. (1884).
Perrot, P. (1998).
Clark, J.O.E. (2004).
Halliday, David; Resnick, Robert (2013). Fundamentals of Physics. Wiley. p. 524.
Denbigh, K. (1981), p. 9.
Adkins, C.J. (1968/1983), p. 55.
Baierlein, R. (1999), p. 349.
Adkins, C.J. (1968/1983), p. 34.
Pippard, A.B. (1957/1966), p. 18.
Haase, R. (1971), p. 7.
Mach, E. (1900), section 5, pp. 48–49, section 22, pp. 60–61.
Truesdell, C. (1980).
Serrin, J. (1986), especially p. 6.
Truesdell, C. (1969), p. 6.

    Lieb, E.H., Yngvason, J. (2003), p. 190.

Quotations

Denbigh states in a footnote that he is indebted to correspondence with Professor E.A. Guggenheim and with Professor N.K. Adam. From this, Denbigh concludes "It seems, however, that when a system is able to exchange both heat and matter with its environment, it is impossible to make an unambiguous distinction between energy transported as heat and by the migration of matter, without already assuming the existence of the 'heat of transport'." Denbigh K.G. (1951), p. 56.

    "Heat must therefore consist of either living force or of attraction through space. In the former case we can conceive the constituent particles of heated bodies to be, either in whole or in part, in a state of motion. In the latter we may suppose the particles to be removed by the process of heating, so as to exert attraction through greater space. I am inclined to believe that both of these hypotheses will be found to hold good,—that in some instances, particularly in the case of sensible heat, or such as is indicated by the thermometer, heat will be found to consist in the living force of the particles of the bodies in which it is induced; whilst in others, particularly in the case of latent heat, the phenomena are produced by the separation of particle from particle, so as to cause them to attract one another through a greater space." Joule, J.P. (1884).

Bibliography of cited references

    Adkins, C.J. (1968/1983). Equilibrium Thermodynamics, (1st edition 1968), third edition 1983, Cambridge University Press, Cambridge UK, ISBN 0-521-25445-0.
    Atkins, P., de Paula, J. (1978/2010). Physical Chemistry, (first edition 1978), ninth edition 2010, Oxford University Press, Oxford UK, ISBN 978-0-19-954337-3.
    Bacon, F. (1620). Novum Organum Scientiarum, translated by Devey, J., P.F. Collier & Son, New York, 1902.
    Baierlein, R. (1999). Thermal Physics. Cambridge University Press. ISBN 978-0-521-65838-6.
    Bailyn, M. (1994). A Survey of Thermodynamics, American Institute of Physics Press, New York, ISBN 0-88318-797-3.
    Born, M. (1949). Natural Philosophy of Cause and Chance, Oxford University Press, London.
    Bryan, G.H. (1907). Thermodynamics. An Introductory Treatise dealing mainly with First Principles and their Direct Applications, B.G. Teubner, Leipzig.
    Buchdahl, H.A. (1966). The Concepts of Classical Thermodynamics, Cambridge University Press, Cambridge UK.
    Callen, H.B. (1960/1985). Thermodynamics and an Introduction to Thermostatistics, (1st edition 1960) 2nd edition 1985, Wiley, New York, ISBN 0-471-86256-8.
    Carathéodory, C. (1909). "Untersuchungen über die Grundlagen der Thermodynamik". Mathematische Annalen. 67 (3): 355–386. doi:10.1007/BF01450409. S2CID 118230148. A translation may be found here. A mostly reliable translation is to be found at Kestin, J. (1976). The Second Law of Thermodynamics, Dowden, Hutchinson & Ross, Stroudsburg PA.
    Chandrasekhar, S. (1961). Hydrodynamic and Hydromagnetic Stability, Oxford University Press, Oxford UK.
    Clark, J.O.E. (2004). The Essential Dictionary of Science. Barnes & Noble Books. ISBN 978-0-7607-4616-5.
    Clausius, R. (1854). Annalen der Physik (Poggendoff's Annalen), Dec. 1854, vol. xciii. p. 481; translated in the Journal de Mathematiques, vol. xx. Paris, 1855, and in the Philosophical Magazine, August 1856, s. 4. vol. xii, p. 81.
    Clausius, R. (1865/1867). The Mechanical Theory of Heat – with its Applications to the Steam Engine and to Physical Properties of Bodies, London: John van Voorst. 1867. Also the second edition translated into English by W.R. Browne (1879) here and here.
    De Groot, S.R., Mazur, P. (1962). Non-equilibrium Thermodynamics, North-Holland, Amsterdam. Reprinted (1984), Dover Publications Inc., New York, ISBN 0486647412.
    Denbigh, K. (1955/1981). The Principles of Chemical Equilibrium, Cambridge University Press, Cambridge ISBN 0-521-23682-7.
    Greven, A., Keller, G., Warnecke (editors) (2003). Entropy, Princeton University Press, Princeton NJ, ISBN 0-691-11338-6.
    Guggenheim, E.A. (1967) [1949], Thermodynamics. An Advanced Treatment for Chemists and Physicists (fifth ed.), Amsterdam: North-Holland Publishing Company.
    Jensen, W.B. (2010). "Why Are q and Q Used to Symbolize Heat?" (PDF). J. Chem. Educ. 87 (11): 1142. Bibcode:2010JChEd..87.1142J. doi:10.1021/ed100769d. Archived from the original (PDF) on 2 April 2015. Retrieved 23 March 2015.
    J.P. Joule (1884), The Scientific Papers of James Prescott Joule, The Physical Society of London, p. 274, Lecture on Matter, Living Force, and Heat. 5 and 12 May 1847.
    Kittel, C. Kroemer, H. (1980). Thermal Physics, second edition, W.H. Freeman, San Francisco, ISBN 0-7167-1088-9.
    Kondepudi, D. (2008), Introduction to Modern Thermodynamics, Chichester UK: Wiley, ISBN 978-0-470-01598-8
    Kondepudi, D., Prigogine, I. (1998). Modern Thermodynamics: From Heat Engines to Dissipative Structures, John Wiley & Sons, Chichester, ISBN 0-471-97393-9.
    Landau, L., Lifshitz, E.M. (1958/1969). Statistical Physics, volume 5 of Course of Theoretical Physics, translated from the Russian by J.B. Sykes, M.J. Kearsley, Pergamon, Oxford.
    Lebon, G., Jou, D., Casas-Vázquez, J. (2008). Understanding Non-equilibrium Thermodynamics: Foundations, Applications, Frontiers, Springer-Verlag, Berlin, e-ISBN 978-3-540-74252-4.
    Lieb, E.H., Yngvason, J. (2003). The Entropy of Classical Thermodynamics, Chapter 8 of Entropy, Greven, A., Keller, G., Warnecke (editors) (2003).
    Maxwell, J.C. (1871), Theory of Heat (first ed.), London: Longmans, Green and Co.
    Partington, J.R. (1949), An Advanced Treatise on Physical Chemistry., vol. 1, Fundamental Principles. The Properties of Gases, London: Longmans, Green and Co.
    Perrot, Pierre (1998). A to Z of Thermodynamics. Oxford University Press. ISBN 978-0-19-856552-9.
    Pippard, A.B. (1957/1966). Elements of Classical Thermodynamics for Advanced Students of Physics, original publication 1957, reprint 1966, Cambridge University Press, Cambridge.
    Planck, M., (1897/1903). Treatise on Thermodynamics, translated by A. Ogg, first English edition, Longmans, Green and Co., London.
    Planck. M. (1914). The Theory of Heat Radiation, a translation by Masius, M. of the second German edition, P. Blakiston's Son & Co., Philadelphia.
    Planck, M., (1923/1927). Treatise on Thermodynamics, translated by A. Ogg, third English edition, Longmans, Green and Co., London.
    Reif, F. (1965). Fundamentals of Statistical and Thermal Physics. New York: McGraw-Hlll, Inc.
    Shavit, A., Gutfinger, C. (1995). Thermodynamics. From Concepts to Applications, Prentice Hall, London, ISBN 0-13-288267-1.
    Truesdell, C. (1969). Rational Thermodynamics: a Course of Lectures on Selected Topics, McGraw-Hill Book Company, New York.
    Truesdell, C. (1980). The Tragicomical History of Thermodynamics 1822–1854, Springer, New York, ISBN 0-387-90403-4.

Further bibliography

    Beretta, G.P.; E.P. Gyftopoulos (1990). "What is heat?" (PDF). Education in Thermodynamics and Energy Systems. AES. 20.
    Gyftopoulos, E.P., & Beretta, G.P. (1991). Thermodynamics: foundations and applications. (Dover Publications)
    Hatsopoulos, G.N., & Keenan, J.H. (1981). Principles of general thermodynamics. RE Krieger Publishing Company.

External links

    Heat on In Our Time at the BBC
    Plasma heat at 2 gigakelvins – Article about extremely high temperature generated by scientists (Foxnews.com)
    Correlations for Convective Heat Transfer – ChE Online Resources

Authority control: National Edit this at Wikidata	

    France BnF data Germany Israel United States Japan Czech Republic

Categories:

    Heat transferThermodynamicsPhysical quantities

    This page was last edited on 8 July 2023, at 22:57 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Overview

    Steady-state conduction
    Transient conduction
    Relativistic conduction
    Quantum conduction

Fourier's law

    Differential form
    Integral form

Conductance

    Intensive-property representation
    Cylindrical shells
    Spherical

Transient thermal conduction

    Interface heat transfer

Thermal conduction applications

    Splat cooling
    Metal quenching

Zeroth law of thermodynamics

Thermal conduction instruments

        Thermal conductivity analyzer
        Gas sensor
    See also
    References
    External links

Thermal conduction

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
For other uses, see Conduction (disambiguation).

Conduction is the process by which heat is transferred from the hotter end to the colder end of an object. The ability of the object to conduct heat is known as its thermal conductivity, and is denoted k.

Heat spontaneously flows along a temperature gradient (i.e. from a hotter body to a colder body). For example, heat is conducted from the hotplate of an electric stove to the bottom of a saucepan in contact with it. In the absence of an opposing external driving energy source, within a body or between bodies, temperature differences decay over time, and thermal equilibrium is approached, temperature becoming more uniform.

In conduction, the heat flow is within and through the body itself. In contrast, in heat transfer by thermal radiation, the transfer is often between bodies, which may be separated spatially. Heat can also be transferred by a combination of conduction and radiation. In solids, conduction is mediated by the combination of vibrations and collisions of molecules, propagation and collisions of phonons, and diffusion and collisions of free electrons. In gases and liquids, conduction is due to the collisions and diffusion of molecules during their random motion. Photons in this context do not collide with one another, and so heat transport by electromagnetic radiation is conceptually distinct from heat conduction by microscopic diffusion and collisions of material particles and phonons. But the distinction is often not easily observed unless the material is semi-transparent.

In the engineering sciences, heat transfer includes the processes of thermal radiation, convection, and sometimes mass transfer.[further explanation needed] Usually, more than one of these processes occurs in a given situation.
Overview
See also: Heat equation

On a microscopic scale, conduction occurs within a body considered as being stationary; this means that the kinetic and potential energies of the bulk motion of the body are separately accounted for. Internal energy diffuses as rapidly moving or vibrating atoms and molecules interact with neighbouring particles, transferring some of their microscopic kinetic and potential energies, these quantities being defined relative to the bulk of the body considered as being stationary. Heat is transferred by conduction when adjacent atoms or molecules collide, or as several electrons move backwards and forwards from atom to atom in a disorganized way so as not to form a macroscopic electric current, or as photons collide and scatter. Conduction is the most significant means of heat transfer within a solid or between solid objects in thermal contact, occurring more readily than in liquids or gases[clarification needed] since the network of relatively close fixed spatial relationships between atoms helps to transfer energy between them by vibration.

Thermal contact conductance is the study of heat conduction between solid bodies in contact. A temperature drop is often observed at the interface between the two surfaces in contact. This phenomenon is said to be a result of a thermal contact resistance existing between the contacting surfaces. Interfacial thermal resistance is a measure of an interface's resistance to thermal flow. This thermal resistance differs from contact resistance, as it exists even at atomically perfect interfaces. Understanding the thermal resistance at the interface between two materials is of primary significance in the study of its thermal properties. Interfaces often contribute significantly to the observed properties of the materials.

The inter-molecular transfer of energy could be primarily by elastic impact, as in fluids, or by free-electron diffusion, as in metals, or phonon vibration, as in insulators. In insulators, the heat flux is carried almost entirely by phonon vibrations.

Metals (e.g., copper, platinum, gold, etc.) are usually good conductors of thermal energy. This is due to the way that metals bond chemically: metallic bonds (as opposed to covalent or ionic bonds) have free-moving electrons that transfer thermal energy rapidly through the metal. The electron fluid of a conductive metallic solid conducts most of the heat flux through the solid. Phonon flux is still present but carries less of the energy. Electrons also conduct electric current through conductive solids, and the thermal and electrical conductivities of most metals have about the same ratio.[clarification needed] A good electrical conductor, such as copper, also conducts heat well. Thermoelectricity is caused by the interaction of heat flux and electric current. Heat conduction within a solid is directly analogous to diffusion of particles within a fluid, in the situation where there are no fluid currents.

In gases, heat transfer occurs through collisions of gas molecules with one another. In the absence of convection, which relates to a moving fluid or gas phase, thermal conduction through a gas phase is highly dependent on the composition and pressure of this phase, and in particular, the mean free path of gas molecules relative to the size of the gas gap, as given by the Knudsen number K n K_{n}.[1]

To quantify the ease with which a particular medium conducts, engineers employ the thermal conductivity, also known as the conductivity constant or conduction coefficient, k. In thermal conductivity, k is defined as "the quantity of heat, Q, transmitted in time (t) through a thickness (L), in a direction normal to a surface of area (A), due to a temperature difference (ΔT) [...]". Thermal conductivity is a material property that is primarily dependent on the medium's phase, temperature, density, and molecular bonding. Thermal effusivity is a quantity derived from conductivity, which is a measure of its ability to exchange thermal energy with its surroundings.
Steady-state conduction

Steady-state conduction is the form of conduction that happens when the temperature difference(s) driving the conduction are constant, so that (after an equilibration time), the spatial distribution of temperatures (temperature field) in the conducting object does not change any further. Thus, all partial derivatives of temperature concerning space may either be zero or have nonzero values, but all derivatives of temperature at any point concerning time are uniformly zero. In steady-state conduction, the amount of heat entering any region of an object is equal to the amount of heat coming out (if this were not so, the temperature would be rising or falling, as thermal energy was tapped or trapped in a region).

For example, a bar may be cold at one end and hot at the other, but after a state of steady-state conduction is reached, the spatial gradient of temperatures along the bar does not change any further, as time proceeds. Instead, the temperature remains constant at any given cross-section of the rod normal to the direction of heat transfer, and this temperature varies linearly in space in the case where there is no heat generation in the rod.[2]

In steady-state conduction, all the laws of direct current electrical conduction can be applied to "heat currents". In such cases, it is possible to take "thermal resistances" as the analog to electrical resistances. In such cases, temperature plays the role of voltage, and heat transferred per unit time (heat power) is the analog of electric current. Steady-state systems can be modeled by networks of such thermal resistances in series and parallel, in exact analogy to electrical networks of resistors. See purely resistive thermal circuits for an example of such a network.
Transient conduction
Main article: Heat equation

During any period in which temperatures changes in time at any place within an object, the mode of thermal energy flow is termed transient conduction. Another term is "non-steady-state" conduction, referring to the time-dependence of temperature fields in an object. Non-steady-state situations appear after an imposed change in temperature at a boundary of an object. They may also occur with temperature changes inside an object, as a result of a new source or sink of heat suddenly introduced within an object, causing temperatures near the source or sink to change in time.

When a new perturbation of temperature of this type happens, temperatures within the system change in time toward a new equilibrium with the new conditions, provided that these do not change. After equilibrium, heat flow into the system once again equals the heat flow out, and temperatures at each point inside the system no longer change. Once this happens, transient conduction is ended, although steady-state conduction may continue if heat flow continues.

If changes in external temperatures or internal heat generation changes are too rapid for the equilibrium of temperatures in space to take place, then the system never reaches a state of unchanging temperature distribution in time, and the system remains in a transient state.

An example of a new source of heat "turning on" within an object, causing transient conduction, is an engine starting in an automobile. In this case, the transient thermal conduction phase for the entire machine is over, and the steady-state phase appears, as soon as the engine reaches steady-state operating temperature. In this state of steady-state equilibrium, temperatures vary greatly from the engine cylinders to other parts of the automobile, but at no point in space within the automobile does temperature increase or decrease. After establishing this state, the transient conduction phase of heat transfer is over.

New external conditions also cause this process: for example, the copper bar in the example steady-state conduction experiences transient conduction as soon as one end is subjected to a different temperature from the other. Over time, the field of temperatures inside the bar reaches a new steady-state, in which a constant temperature gradient along the bar is finally set up, and this gradient then stays constant in time. Typically, such a new steady-state gradient is approached exponentially with time after a new temperature-or-heat source or sink, has been introduced. When a "transient conduction" phase is over, heat flow may continue at high power, so long as temperatures do not change.

An example of transient conduction that does not end with steady-state conduction, but rather no conduction, occurs when a hot copper ball is dropped into oil at a low temperature. Here, the temperature field within the object begins to change as a function of time, as the heat is removed from the metal, and the interest lies in analyzing this spatial change of temperature within the object over time until all gradients disappear entirely (the ball has reached the same temperature as the oil). Mathematically, this condition is also approached exponentially; in theory, it takes infinite time, but in practice, it is over, for all intents and purposes, in a much shorter period. At the end of this process with no heat sink but the internal parts of the ball (which are finite), there is no steady-state heat conduction to reach. Such a state never occurs in this situation, but rather the end of the process is when there is no heat conduction at all.

The analysis of non-steady-state conduction systems is more complex than that of steady-state systems. If the conducting body has a simple shape, then exact analytical mathematical expressions and solutions may be possible (see heat equation for the analytical approach).[3] However, most often, because of complicated shapes with varying thermal conductivities within the shape (i.e., most complex objects, mechanisms or machines in engineering) often the application of approximate theories is required, and/or numerical analysis by computer. One popular graphical method involves the use of Heisler Charts.

Occasionally, transient conduction problems may be considerably simplified if regions of the object being heated or cooled can be identified, for which thermal conductivity is very much greater than that for heat paths leading into the region. In this case, the region with high conductivity can often be treated in the lumped capacitance model, as a "lump" of material with a simple thermal capacitance consisting of its aggregate heat capacity. Such regions warm or cool, but show no significant temperature variation across their extent, during the process (as compared to the rest of the system). This is due to their far higher conductance. During transient conduction, therefore, the temperature across their conductive regions changes uniformly in space, and as a simple exponential in time. An example of such systems is those that follow Newton's law of cooling during transient cooling (or the reverse during heating). The equivalent thermal circuit consists of a simple capacitor in series with a resistor. In such cases, the remainder of the system with a high thermal resistance (comparatively low conductivity) plays the role of the resistor in the circuit.
Relativistic conduction

The theory of relativistic heat conduction is a model that is compatible with the theory of special relativity. For most of the last century, it was recognized that the Fourier equation is in contradiction with the theory of relativity because it admits an infinite speed of propagation of heat signals. For example, according to the Fourier equation, a pulse of heat at the origin would be felt at infinity instantaneously. The speed of information propagation is faster than the speed of light in vacuum, which is physically inadmissible within the framework of relativity.
Quantum conduction

Second sound is a quantum mechanical phenomenon in which heat transfer occurs by wave-like motion, rather than by the more usual mechanism of diffusion. Heat takes the place of pressure in normal sound waves. This leads to a very high thermal conductivity. It is known as "second sound" because the wave motion of heat is similar to the propagation of sound in air.
Fourier's law

The law of heat conduction, also known as Fourier's law, states that the rate of heat transfer through a material is proportional to the negative gradient in the temperature and to the area, at right angles to that gradient, through which the heat flows. We can state this law in two equivalent forms: the integral form, in which we look at the amount of energy flowing into or out of a body as a whole, and the differential form, in which we look at the flow rates or fluxes of energy locally.

Newton's law of cooling is a discrete analogue of Fourier's law, while Ohm's law is the electrical analogue of Fourier's law and Fick's laws of diffusion is its chemical analogue.
Differential form

The differential form of Fourier's law of thermal conduction shows that the local heat flux density q \mathbf {q} is equal to the product of thermal conductivity k k and the negative local temperature gradient − ∇ T -\nabla T. The heat flux density is the amount of energy that flows through a unit area per unit time.
q = − k ∇ T ,
{\displaystyle \mathbf {q} =-k\nabla T,}
where (including the SI units)

    q \mathbf {q} is the local heat flux density, W/m2,
    k k is the material's conductivity, W/(m·K),
    ∇ T {\displaystyle \nabla T} is the temperature gradient, K/m.

The thermal conductivity k k is often treated as a constant, though this is not always true. While the thermal conductivity of a material generally varies with temperature, the variation can be small over a significant range of temperatures for some common materials. In anisotropic materials, the thermal conductivity typically varies with orientation; in this case k k is represented by a second-order tensor. In non-uniform materials, k k varies with spatial location.

For many simple applications, Fourier's law is used in its one-dimensional form, for example, in the x direction:
q x = − k d T d x .
{\displaystyle q_{x}=-k{\frac {dT}{dx}}.}

In an isotropic medium, Fourier's law leads to heat equation
∂ T ∂ t = α ( ∂ 2 T ∂ x 2 + ∂ 2 T ∂ y 2 + ∂ 2 T ∂ z 2 )
{\displaystyle {\frac {\partial T}{\partial t}}=\alpha \left({\frac {\partial ^{2}T}{\partial x^{2}}}+{\frac {\partial ^{2}T}{\partial y^{2}}}+{\frac {\partial ^{2}T}{\partial z^{2}}}\right)}
with a fundamental solution famously known as heat kernel.

Integral form

By integrating the differential form over the material's total surface S S, we arrive at the integral form of Fourier's law:

    ∂ Q ∂ t = − k {\displaystyle {\frac {\partial Q}{\partial t}}=-k} \oiint S \scriptstyle S ∇ T ⋅ d S , {\displaystyle \nabla T\cdot d\mathbf {S} ,}

where (including the SI units):

    ∂ Q ∂ t {\displaystyle {\frac {\partial Q}{\partial t}}} is the amount of heat transferred per unit time (in W),
    d S d\mathbf {S} is an oriented surface area element (in m2).

The above differential equation, when integrated for a homogeneous material of 1-D geometry between two endpoints at constant temperature, gives the heat flow rate as
Q Δ t = − k A Δ T Δ x ,
{\displaystyle {\frac {Q}{\Delta t}}=-kA{\frac {\Delta T}{\Delta x}},}
where

    Δ t \Delta t is the time interval during which the amount of heat Q Q flows through a cross-section of the material,
    A A is the cross-sectional surface area,
    Δ T \Delta T is the temperature difference between the ends,
    Δ x \Delta x is the distance between the ends.

This law forms the basis for the derivation of the heat equation.
Conductance

Writing
U = k Δ x ,
{\displaystyle U={\frac {k}{\Delta x}},}
where U is the conductance, in W/(m2 K).

Fourier's law can also be stated as:
Δ Q Δ t = U A ( − Δ T ) .
{\displaystyle {\frac {\Delta Q}{\Delta t}}=UA\,(-\Delta T).}

The reciprocal of conductance is resistance, R {\displaystyle {\big .}R} is given by:
R = 1 U = Δ x k = A ( − Δ T ) Δ Q Δ t .
{\displaystyle R={\frac {1}{U}}={\frac {\Delta x}{k}}={\frac {A\,(-\Delta T)}{\frac {\Delta Q}{\Delta t}}}.}

Resistance is additive when several conducting layers lie between the hot and cool regions, because A and Q are the same for all layers. In a multilayer partition, the total conductance is related to the conductance of its layers by:
R = R 1 + R 2 + R 3 + ⋯
{\displaystyle R=R_{1}+R_{2}+R_{3}+\cdots }
or equivalently
1 U = 1 U 1 + 1 U 2 + 1 U 3 + ⋯
{\displaystyle {\frac {1}{U}}={\frac {1}{U_{1}}}+{\frac {1}{U_{2}}}+{\frac {1}{U_{3}}}+\cdots }

So, when dealing with a multilayer partition, the following formula is usually used:
Δ Q Δ t = A ( − Δ T ) Δ x 1 k 1 + Δ x 2 k 2 + Δ x 3 k 3 + ⋯ .
{\displaystyle {\frac {\Delta Q}{\Delta t}}={\frac {A\,(-\Delta T)}{{\frac {\Delta x_{1}}{k_{1}}}+{\frac {\Delta x_{2}}{k_{2}}}+{\frac {\Delta x_{3}}{k_{3}}}+\cdots }}.}

For heat conduction from one fluid to another through a barrier, it is sometimes important to consider the conductance of the thin film of fluid that remains stationary next to the barrier. This thin film of fluid is difficult to quantify because its characteristics depend upon complex conditions of turbulence and viscosity—but when dealing with thin high-conductance barriers it can sometimes be quite significant.
Intensive-property representation

The previous conductance equations, written in terms of extensive properties, can be reformulated in terms of intensive properties. Ideally, the formulae for conductance should produce a quantity with dimensions independent of distance, like Ohm's law for electrical resistance, R = V / I R = V/I\,\!, and conductance, G = I / V G = I/V \,\!.

From the electrical formula: R = ρ x / A {\displaystyle R=\rho x/A}, where ρ is resistivity, x is length, and A is cross-sectional area, we have G = k A / x G = k A / x \,\!, where G is conductance, k is conductivity, x is length, and A is cross-sectional area.

For heat,
U = k A Δ x ,
{\displaystyle U={\frac {kA}{\Delta x}},}
where U is the conductance.

Fourier's law can also be stated as:
Q ˙ = U Δ T ,
{\displaystyle {\dot {Q}}=U\,\Delta T,}
analogous to Ohm's law, I = V / R {\displaystyle I=V/R} or I = V G . {\displaystyle I=VG.}

The reciprocal of conductance is resistance, R, given by:
R = Δ T Q ˙ ,
{\displaystyle R={\frac {\Delta T}{\dot {Q}}},}
analogous to Ohm's law, R = V / I . {\displaystyle R=V/I.}

The rules for combining resistances and conductances (in series and parallel) are the same for both heat flow and electric current.
Cylindrical shells

Conduction through cylindrical shells (e.g. pipes) can be calculated from the internal radius, r 1 r_{1}, the external radius, r 2 r_{2}, the length, ℓ \ell , and the temperature difference between the inner and outer wall, T 2 − T 1 T_2 - T_1.

The surface area of the cylinder is A r = 2 π r ℓ {\displaystyle A_{r}=2\pi r\ell }

When Fourier's equation is applied:
Q ˙ = − k A r d T d r = − 2 k π r ℓ d T d r
{\displaystyle {\dot {Q}}=-kA_{r}{\frac {dT}{dr}}=-2k\pi r\ell {\frac {dT}{dr}}}
and rearranged:
Q ˙ ∫ r 1 r 2 1 r d r = − 2 k π ℓ ∫ T 1 T 2 d T
{\displaystyle {\dot {Q}}\int _{r_{1}}^{r_{2}}{\frac {1}{r}}\,dr=-2k\pi \ell \int _{T_{1}}^{T_{2}}dT}
then the rate of heat transfer is:
Q ˙ = 2 k π ℓ T 1 − T 2 ln ⁡ ( r 2 / r 1 )
{\displaystyle {\dot {Q}}=2k\pi \ell {\frac {T_{1}-T_{2}}{\ln(r_{2}/r_{1})}}}
the thermal resistance is:
R c = Δ T Q ˙ = ln ⁡ ( r 2 / r 1 ) 2 π k ℓ
{\displaystyle R_{c}={\frac {\Delta T}{\dot {Q}}}={\frac {\ln(r_{2}/r_{1})}{2\pi k\ell }}}
and Q ˙ = 2 π k ℓ r m T 1 − T 2 r 2 − r 1 {\textstyle {\dot {Q}}=2\pi k\ell r_{m}{\frac {T_{1}-T_{2}}{r_{2}-r_{1}}}}, where r m = r 2 − r 1 ln ⁡ ( r 2 / r 1 ) {\textstyle r_{m}={\frac {r_{2}-r_{1}}{\ln(r_{2}/r_{1})}}}. It is important to note that this is the log-mean radius.

Spherical

The conduction through a spherical shell with internal radius, r 1 r_{1}, and external radius, r 2 r_{2}, can be calculated in a similar manner as for a cylindrical shell.

The surface area of the sphere is: A = 4 π r 2 . A=4\pi r^{2}.

Solving in a similar manner as for a cylindrical shell (see above) produces:
Q ˙ = 4 k π T 1 − T 2 1 / r 1 − 1 / r 2 = 4 k π ( T 1 − T 2 ) r 1 r 2 r 2 − r 1
{\displaystyle {\dot {Q}}=4k\pi {\frac {T_{1}-T_{2}}{1/{r_{1}}-1/{r_{2}}}}=4k\pi {\frac {(T_{1}-T_{2})r_{1}r_{2}}{r_{2}-r_{1}}}}

Transient thermal conduction
Main article: Heat equation
Interface heat transfer
	
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2013) (Learn how and when to remove this template message)

The heat transfer at an interface is considered a transient heat flow. To analyze this problem, the Biot number is important to understand how the system behaves. The Biot number is determined by:
Bi = h L k
{\displaystyle {\textit {Bi}}={\frac {hL}{k}}}
The heat transfer coefficient h h, is introduced in this formula, and is measured in
J m 2 s K
{\displaystyle \mathrm {\frac {J}{m^{2}sK}} }
. If the system has a Biot number of less than 0.1, the material behaves according to Newtonian cooling, i.e. with negligible temperature gradient within the body.[4] If the Biot number is greater than 0.1, the system behaves as a series solution. The temperature profile in terms of time can be derived from the equation
q = − h Δ T ,
{\displaystyle q=-h\,\Delta T,}
which becomes
T − T f T i − T f = exp ⁡ ( − h A t ρ C p V ) .
{\displaystyle {\frac {T-T_{f}}{T_{i}-T_{f}}}=\exp \left({\frac {-hAt}{\rho C_{p}V}}\right).}

The heat transfer coefficient, h, is measured in W m 2 K \mathrm{\frac{W}{m^2 K}} , and represents the transfer of heat at an interface between two materials. This value is different at every interface and is an important concept in understanding heat flow at an interface.

The series solution can be analyzed with a nomogram. A nomogram has a relative temperature as the y coordinate and the Fourier number, which is calculated by
Fo = α t L 2 .
{\displaystyle {\textit {Fo}}={\frac {\alpha t}{L^{2}}}.}

The Biot number increases as the Fourier number decreases. There are five steps to determine a temperature profile in terms of time.

    Calculate the Biot number
    Determine which relative depth matters, either x or L.
    Convert time to the Fourier number.
    Convert T i T_{i} to relative temperature with the boundary conditions.
    Compared required to point to trace specified Biot number on the nomogram.

Thermal conduction applications
Splat cooling

Splat cooling is a method for quenching small droplets of molten materials by rapid contact with a cold surface. The particles undergo a characteristic cooling process, with the heat profile at t = 0 t=0 for initial temperature as the maximum at x = 0 x=0 and T = 0 T=0 at x = − ∞ x = -\infin and x = ∞ x = \infin , and the heat profile at t = ∞ t=\infin for − ∞ ≤ x ≤ ∞ {\displaystyle -\infty \leq x\leq \infty } as the boundary conditions. Splat cooling rapidly ends in a steady state temperature, and is similar in form to the Gaussian diffusion equation. The temperature profile, with respect to the position and time of this type of cooling, varies with:
T ( x , t ) − T i = T i Δ X 2 π α t exp ⁡ ( − x 2 4 α t )
{\displaystyle T(x,t)-T_{i}={\frac {T_{i}\Delta X}{2{\sqrt {\pi \alpha t}}}}\exp \left(-{\frac {x^{2}}{4\alpha t}}\right)}

Splat cooling is a fundamental concept that has been adapted for practical use in the form of thermal spraying. The thermal diffusivity coefficient, represented as α \alpha , can be written as α = k ρ C p \alpha =\frac{k}{\rho C_p} . This varies according to the material.[5][6]
Metal quenching

Metal quenching is a transient heat transfer process in terms of the time temperature transformation (TTT). It is possible to manipulate the cooling process to adjust the phase of a suitable material. For example, appropriate quenching of steel can convert a desirable proportion of its content of austenite to martensite, creating a very hard and strong product. To achieve this, it is necessary to quench at the "nose" (or eutectic) of the TTT diagram. Since materials differ in their Biot numbers, the time it takes for the material to quench, or the Fourier number, varies in practice.[7] In steel, the quenching temperature range is generally from 600 °C to 200 °C. To control the quenching time and to select suitable quenching media, it is necessary to determine the Fourier number from the desired quenching time, the relative temperature drop, and the relevant Biot number. Usually, the correct figures are read from a standard nomogram.[citation needed] By calculating the heat transfer coefficient from this Biot number, one can find a liquid medium suitable for the application.[8]
Zeroth law of thermodynamics

One statement of the so-called zeroth law of thermodynamics is directly focused on the idea of conduction of heat. Bailyn (1994) writes that "the zeroth law may be stated: All diathermal walls are equivalent".[9]

A diathermal wall is a physical connection between two bodies that allows the passage of heat between them. Bailyn is referring to diathermal walls that exclusively connect two bodies, especially conductive walls.

This statement of the "zeroth law" belongs to an idealized theoretical discourse, and actual physical walls may have peculiarities that do not conform to its generality.

For example, the material of the wall must not undergo a phase transition, such as evaporation or fusion, at the temperature at which it must conduct heat. But when only thermal equilibrium is considered and time is not urgent, so that the conductivity of the material does not matter too much, one suitable heat conductor is as good as another. Conversely, another aspect of the zeroth law is that, subject again to suitable restrictions, a given diathermal wall is indifferent to the nature of the heat bath to which it is connected. For example, the glass bulb of a thermometer acts as a diathermal wall whether exposed to a gas or a liquid, provided that they do not corrode or melt it.

These differences are among the defining characteristics of heat transfer. In a sense, they are symmetries of heat transfer.
Thermal conduction instruments
Thermal conductivity analyzer

Thermal conduction property of any gas under standard conditions of pressure and temperature is a fixed quantity. This property of a known reference gas or known reference gas mixtures can, therefore, be used for certain sensory applications, such as the thermal conductivity analyzer.

The working of this instrument is by principle based on the Wheatstone bridge containing four filaments whose resistances are matched. Whenever a certain gas is passed over such network of filaments, their resistance changes due to the altered thermal conductivity of the filaments and thereby changing the net voltage output from the Wheatstone Bridge. This voltage output will be correlated with the database to identify the gas sample.
Gas sensor

The principle of thermal conductivity of gases can also be used to measure the concentration of a gas in a binary mixture of gases.

Working: if the same gas is present around all the Wheatstone bridge filaments, then the same temperature is maintained in all the filaments and hence same resistances are also maintained; resulting in a balanced Wheatstone bridge. However, If the dissimilar gas sample (or gas mixture) is passed over one set of two filaments and the reference gas on the other set of two filaments, then the Wheatstone bridge becomes unbalanced. And the resulting net voltage output of the circuit will be correlated with the database to identify the constituents of the sample gas.

Using this technique many unknown gas samples can be identified by comparing their thermal conductivity with other reference gas of known thermal conductivity. The most commonly used reference gas is nitrogen; as the thermal conductivity of most common gases (except hydrogen and helium) are similar to that of nitrogen.
See also

    List of thermal conductivities
    Electrical conduction
    Convection diffusion equation
    R-value (insulation)
    Heat pipe
    Fick's law of diffusion
    Relativistic heat conduction
    Churchill–Bernstein equation
    Fourier number
    Biot number
    False diffusion
    Heat Conduction
    General equation of heat transfer

References

Dai; et al. (2015). "Effective Thermal Conductivity of Submicron Powders: A Numerical Study". Applied Mechanics and Materials. 846: 500–505. doi:10.4028/www.scientific.net/AMM.846.500. S2CID 114611104.
Bergman, Theodore L.; Lavine, Adrienne S.; Incropera, Frank P.; Dewitt, David P. (2011). Fundamentals of heat and mass transfer (7th ed.). Hoboken, NJ: Wiley. ISBN 9780470501979. OCLC 713621645.
The Exact Analytical Conduction Toolbox contains a variety of transient expressions for heat conduction, along with algorithms and computer code for obtaining precise numerical values.
III, H. Palmour; Spriggs, R. M.; Uskokovic, D. P. (11 November 2013). Science of Sintering: New Directions for Materials Processing and Microstructural Control. Springer Science & Business Media. p. 164. ISBN 978-1-4899-0933-6.
Sam Zhang; Dongliang Zhao (19 November 2012). Aeronautical and Aerospace Materials Handbook. CRC Press. pp. 304–. ISBN 978-1-4398-7329-8. Retrieved 7 May 2013.
Martin Eein (2002). Drop-Surface Interactions. Springer. pp. 174–. ISBN 978-3-211-83692-7. Retrieved 7 May 2013.
Rajiv Asthana; Ashok Kumar; Narendra B. Dahotre (9 January 2006). Materials Processing and Manufacturing Science. Butterworth–Heinemann. pp. 158–. ISBN 978-0-08-046488-6. Retrieved 7 May 2013.
George E. Totten (2002). Handbook of Residual Stress and Deformation of Steel. ASM International. pp. 322–. ISBN 978-1-61503-227-3. Retrieved 7 May 2013.

    Bailyn, M. (1994). A Survey of Thermodynamics, American Institute of Physics, New York, ISBN 0-88318-797-3, page 23.

    Dehghani, F 2007, CHNG2801 – Conservation and Transport Processes: Course Notes, University of Sydney, Sydney
    John H Lienhard IV and John H Lienhard V, 'A Heat Transfer Textbook', Fifth Edition, Dover Pub., Mineola, NY, 2019 [1]

External links

    Media related to Heat conduction at Wikimedia Commons
    Heat conduction – Thermal-FluidsPedia
    Newton's Law of Cooling by Jeff Bryant based on a program by Stephen Wolfram, Wolfram Demonstrations Project.

Authority control: National Edit this at Wikidata	

    Germany Israel United States Japan

Categories:

    Heat conductionHeat transferPhysical quantitiesTransport phenomena

    This page was last edited on 22 August 2023, at 11:31 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki



Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents

    (Top)
    Early history
    Recent research
    European Internet
    Spin-off companies
    Software and languages
    Notable people
    References
    External links

Centrum Wiskunde & Informatica

    Article
    Talk

    Read
    Edit
    View history

Tools

Coordinates: 52°21′23″N 4°57′07″E
From Wikipedia, the free encyclopedia
Centrum Wiskunde & Informatica
CWI logo
Type	National research institute
Established	1946; 77 years ago
President	Prof.dr. A.G. de Kok
Administrative staff
	~200
Location	Amsterdam
, Netherlands
Website	www.cwi.nl

The Centrum Wiskunde & Informatica (abbr. CWI; English: "National Research Institute for Mathematics and Computer Science") is a research centre in the field of mathematics and theoretical computer science. It is part of the institutes organization of the Dutch Research Council (NWO) and is located at the Amsterdam Science Park. This institute is famous as the creation site of the programming language Python. It was a founding member of the European Research Consortium for Informatics and Mathematics (ERCIM).
Early history

The institute was founded in 1946 by Johannes van der Corput, David van Dantzig, Jurjen Koksma, Hendrik Anthony Kramers, Marcel Minnaert and Jan Arnoldus Schouten. It was originally called Mathematical Centre (in Dutch: Mathematisch Centrum). One early mission was to develop mathematical prediction models to assist large Dutch engineering projects, such as the Delta Works. During this early period, the Mathematics Institute also helped with designing the wings of the Fokker F27 Friendship airplane, voted in 2006 as the most beautiful Dutch design of the 20th century.[1][2]

The computer science component developed soon after. Adriaan van Wijngaarden, considered the founder of computer science (or informatica) in the Netherlands, was the director of the institute for almost 20 years. Edsger Dijkstra did most of his early influential work on algorithms and formal methods at CWI. The first Dutch computers, the Electrologica X1 and Electrologica X8, were both designed at the centre, and Electrologica was created as a spinoff to manufacture the machines.

In 1983, the name of the institute was changed to Centrum Wiskunde & Informatica (CWI) to reflect a governmental push for emphasizing computer science research in the Netherlands.[3]
Recent research

The institute is known for its work in fields such as operations research, software engineering, information processing, and mathematical applications in life sciences and logistics. More recent examples of research results from CWI include the development of scheduling algorithms for the Dutch railway system (the Nederlandse Spoorwegen, one of the busiest rail networks in the world) and the development of the Python programming language by Guido van Rossum. Python has played an important role in the development of the Google search platform from the beginning, and it continues to do so as the system grows and evolves.[4] Many information retrieval techniques used by packages such as SPSS were initially developed by Data Distilleries, a CWI spinoff.[5][6]

Work at the institute was recognized by national or international research awards, such as the Lanchester Prize (awarded yearly by INFORMS), the Gödel Prize (awarded by ACM SIGACT) and the Spinoza Prize. Most of its senior researchers hold part-time professorships at other Dutch universities, with the institute producing over 170 full professors during the course of its history. Several CWI researchers have been recognized as members of the Royal Netherlands Academy of Arts and Sciences, the Academia Europaea, or as knights in the Order of the Netherlands Lion.[7]

In February 2017, CWI in association with Google announced a successful collision attack on SHA 1 encryption algorithm.[8]
European Internet

CWI was an early user of the Internet in Europe, in the form of a TCP/IP connection to NSFNET. Piet Beertema at CWI established one of the first two connections outside the United States to the NSFNET (shortly after France's INRIA)[9][10][11] for EUnet on 17 November 1988. The first Dutch country code top-level domain issued was cwi.nl.[12][13][14] When this domain cwi.nl was registered, on 1 May 1986, .nl effectively became the first active ccTLD outside the United States.[15] For the first ten years CWI, or rather Beertema, managed the .nl administration, until in 1996 this task was transferred to its spin-off SIDN.[12]

The Amsterdam Internet Exchange (one of the largest Internet Exchanges in the world, in terms of both members and throughput traffic) is located at the neighbouring SARA (an early CWI spin-off) and Nikhef institutes. The World Wide Web Consortium (W3C) office for the Benelux countries is located at CWI.[16]
Spin-off companies

CWI has demonstrated a continuing effort to put the work of its researchers at the disposal of society, mainly by collaborating with commercial companies and creating spin-off businesses. In 2000 CWI established "CWI Incubator BV", a dedicated company with the aim to generate high tech spin-off companies.[17] Some of the CWI spinoffs include:[18]

    1956: Electrologica, a pioneering Dutch computer manufacturer.
    1971: SARA (now called SURF), founded as a center for data processing activities for Vrije Universiteit Amsterdam, Universiteit van Amsterdam, and the CWI.
    1990: DigiCash, an electronic money corporation founded by David Chaum.
    1994: NLnet, an Internet Service Provider.
    1994: General Design / Satama Amsterdam, a design company, acquired by LBi (then Lost Boys international).
    1995: Data Distilleries, developer of analytical database software aimed at information retrieval, eventually becoming part of SPSS and acquired by IBM.
    1996: Stichting Internet Domeinregistratie Nederland (SIDN), the .nl top-level domain registrar.
    2000: Software Improvement Group (SIG), a software improvement and legacy code analysis company.
    2008: MonetDB, a high-tech database technology company, developer of the MonetDB column-store.
    2008: Vectorwise, an analytical database technology company, founded in cooperation with the Ingres Corporation (now Actian) and eventually acquired by it.
    2010: Spinque, a company providing search technology for information retrieval specialists.
    2013: MonetDB Solutions, a database services company.
    2016: Seita, a technology company providing demand response services for the energy sector.

Software and languages

    ABC programming language
    Algol 60
    Algol 68
    Alma-0, a multi-paradigm computer programming language
    ASF+SDF Meta Environment, programming language specification and prototyping system, IDE generator
    Cascading Style Sheets
    MonetDB
    NetHack
    Python programming language
    RascalMPL, general purpose meta programming language
    RDFa
    SMIL
    van Wijngaarden grammar
    XForms
    XHTML
    XML Events

Notable people

    Richard Askey
    Adrian Baddeley
    Theo Bemelmans
    Piet Beertema
    Jan Bergstra
    Gerrit Blaauw
    Peter Boncz
    Hugo Brandt Corstius
    Stefan Brands
    Andries Brouwer
    Harry Buhrman
    Dick Bulterman
    David Chaum
    Ronald Cramer
    Theodorus Dekker
    Edsger Dijkstra
    Constance van Eeden
    Peter van Emde Boas
    Richard D. Gill
    Jan Friso Groote
    Dick Grune
    Michiel Hazewinkel
    Jan Hemelrijk
    Martin L. Kersten
    Willem Klein
    Jurjen Ferdinand Koksma
    Tom Koornwinder
    Kees Koster
    Monique Laurent
    Gerrit Lekkerkerker
    Arjen Lenstra
    Jan Karel Lenstra
    Gijsbert de Leve
    Barry Mailloux
    Massimo Marchiori
    Lambert Meertens
    Rob Mokken
    Albert Nijenhuis
    Steven Pemberton
    Herman te Riele
    Guido van Rossum
    Alexander Schrijver
    Jan H. van Schuppen
    Marc Stevens
    John Tromp
    John V. Tucker
    Paul Vitányi
    Hans van Vliet
    Marc Voorhoeve
    Adriaan van Wijngaarden
    Ronald de Wolf
    Peter Wynn

References

"Fokker F27 Friendship wins 2006 Best Dutch Design Election".
"Fokker Friendship beste Nederlandse design". 5 May 2006.
Bennie Mols: ERCOM: The Centrum voor Wiskunde en Informatica turns 60. In: Newsletter of the European Mathematical Society, No. 56 (September 2007), p. 43 (online)
"Quotes about Python". Python.org. Retrieved 13 July 2012.
"SPSS and Data Distilleries". Python.org. Archived from the original on 24 February 2015. Retrieved 24 February 2015.
Sumath, S; Sivanandam, S.N. (2006). Introduction to Data Mining and its Applications. Springer Berlin Heidelberg. p. 743. doi:10.1007/978-3-540-34351-6. ISBN 978-3-540-34350-9.
"Lex Schrijver receives EURO Gold Medal 2015". cwi. 25 April 2013. Retrieved 19 February 2018.
Announcing the first SHA1 collision
"The path to digital literacy and network culture in France (1980s to 1990s)". The Routledge Companion to Global Internet Histories. Taylor & Francis. 2017. pp. 84–89. ISBN 978-1317607656.
[Et Dieu crea l'Internet, Christian Huitema, ISBN 2-212-08855-8, 1995, page 10]
Andrianarisoa, Menjanirina (2 March 2012). "A brief history of the internet".
"CWI History: details". CWI. Retrieved 9 February 2020.
(in Dutch) De geschiedenis van SIDN Archived 2013-07-27 at the Wayback Machine (History of SIDN), Official website of SIDN
"Kees Neggers: Global Networking Requires Global Collaboration | Internet Hall of Fame". www.internethalloffame.org. Retrieved 3 April 2020.
"Our milestones". SIDN. Archived from the original on 8 August 2020. Retrieved 17 August 2022.
"The World Wide Web Consortium - Benelux Office". W3C. Retrieved 17 August 2022.
"Spin-off companies' details". CWI Amsterdam. Retrieved 8 July 2014.

    "Spin-off companies". CWI Amsterdam. Retrieved 8 July 2014.

External links

    Official website

    vte

The European Mathematical Society
Authority control Edit this at Wikidata

Categories:

    Amsterdam-OostComputer science institutes in the NetherlandsEdsger W. DijkstraMathematical institutesMembers of the European Research Consortium for Informatics and MathematicsOrganisations based in Amsterdam1946 establishments in the NetherlandsResearch institutes in the NetherlandsScience and technology in the Netherlands

    This page was last edited on 13 June 2023, at 22:15 (UTC).
    Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
History

    Early years
    Reforms and reversal
    Recent years

Controversies

Coverage

    Rail network

Types of train service

Fares and tickets

        Off-peak discount passes
    Logo
    Divisions of NS
    Policy
    Technological assistance for train staff
    Statistics
    See also
    References
    Further reading
    External links

Nederlandse Spoorwegen

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
NS Nederlandse Spoorwegen
Type	State-owned naamloze vennootschap
Industry	Rail transport
Founded	1938
Headquarters	Utrecht
, Netherlands
Key people
	Wouter Koolmees (CEO)[1]
Products	Rail transport, rail construction, services
Revenue	

    Decrease €5,980 million (2021)[2]
    €6,661 million (2019)

Net income
	

    Increase €116 million (2018)[3]
    €47 million (2017)

Total assets	

    Increase €7,030 million (2018)[3]
    €6,214 million (2017)

Total equity	

    Decrease €11,786 million (2021)[3]
    €3,477 million (2017)

Number of employees
	

    Increase 38,600 (2020)
    40,978 (2020)

Parent	State of the Netherlands
Subsidiaries	Abellio
Nedkoleje
NSRegio (99%)
WestfalenBahn
Website	www.ns.nl
Nederlandse Spoorwegen
Railway tracks in the Netherlands
Double decker (DDZ) train near Gouda, South Holland
Overview
Locale	Netherlands
Dates of operation	1938–present
Predecessor	Hollandsche IJzeren Spoorweg-Maatschappij (HSM)
Maatschappij tot Exploitatie van Staatsspoorwegen (SS)
Technical
Track gauge	1,435 mm (4 ft 8+1⁄2 in) standard gauge

Nederlandse Spoorwegen (NS; Dutch: [ˈneːdərlɑntsə ˈspoːrˌʋeːɣə(n)] (listen); English: "Dutch Railways") is the principal passenger railway operator in the Netherlands. It is a Dutch state-owned company founded in 1938. The Dutch rail network is the busiest in the European Union, and the third busiest in the world after Switzerland and Japan.[4]

The rail infrastructure is maintained by network manager ProRail, which was split off from NS in 2003. Freight operator NS Cargo merged with DB Cargo in 2000. NS runs 4,800 scheduled domestic trains a day, serving 1.1 million passengers.[5] The NS also provides international rail services from the Netherlands to other European destinations and carries out concessions on some foreign rail markets through its subsidiary Abellio.
History
See also: History of rail transport in the Netherlands
Early years
The Hoofdgebouw I (Main Building I) complex in Utrecht, former Nederlandse Spoorwegen headquarters and nowadays the office of DB Cargo in the Netherlands

World War I caused an economic downturn in the Netherlands that caused the two largest Dutch railway companies, Hollandsche IJzeren Spoorweg-Maatschappij (HSM) and Maatschappij tot Exploitatie van Staatsspoorwegen (SS), to become unprofitable. The companies avoided bankruptcy by integrating their operations, which occurred by 1917. The cooperation was by economic and ideological reasons. The state provided support by buying shares in both companies. In 1938, the state bought the remaining shares and merged the companies to create NS; NS was not nationalised.
See also: Holocaust trains § Netherlands

During World War II, NS was forced by the Germans to construct railways to Westerbork transit camp and transport almost a hundred thousand Jews to extermination camps. The company's only wartime strike was during the Dutch famine of 1944–45; NS opted not to strike a year earlier.

NS played a pivotal role in the post-war reconstruction of the Netherlands; only it could provide the required logistical services in a time when there was little alternative to rail transport. The company declined in the 1960s – like many other railways – and operated at a loss. There was increased competition from other modes of transport. In addition, national coal distribution from Limburg became less profitable; the discovery of a gas field near Slochteren led to coal losing market share to natural gas in power plants and homes. NS' response, the Spoorslag '70 plan which increased service and introduced intercity service, failed to restore profitability. The company was deemed nationally important and received state subsidies.
Reforms and reversal
Protests against neoliberal policies in 1983

NS was reorganized following the neoliberal reforms of the 1980s and the 1991 EU Directive 91/440; the latter required railway infrastructure and transport activities to be managed independently. Although the state called the process "corporatization" (verzelfstandiging), it really only meant the withdrawal of subsidies. The changes were carried out by Rob den Besten, who became chief executive officer of NS after the retirement of Leo Ploeger.

NS' infrastructure division was split off into NS Railinfratrust. Plans to split the remainder of NS met with limited success due to trade union opposition; the new companies created were NS Reizigers and locomotive maintenance company NedTrain. Passenger transport was to be conducted on a commercial basis, but the state continued to subsidize non-viable routes. Internally, route managers assumed de facto control, but they were dependent on a different organ in the company[clarify] The freight business, NS Cargo, merged with Deutsche Bahn; the resulting company operated as Railion in 2000 and then as DB Cargo.[6] Performance deteriorated after the reforms, and the company suffered multiple unorganized strikes. The entire board of directors resigned in late-2001.

Another change in strategy followed. Karel Noordzij became CEO in 2002 and reversed many of the reforms to restore confidence in the company. The state no longer considered competitive passenger service to be viable, and began granting concessions with the goal of one concession per line. NS received a concession to run main line routes until 2025.
Recent years
Current headquarters in Utrecht

The timetable change on 10 December 2006 saw the most routes to approximate the symmetry minute in clock-face schedules to the one used in most other European countries. The previous symmetry minute 46 led to problems with cross-border trains. As of December 2022 the company's CEO is former minister Wouter Koolmees, after Marjan Rintel left to become CEO of KLM.

NS was heavily impacted by the COVID-19 pandemic, which caused massive drops in passenger numbers. The company received significant financial support from the national government in order to keep the company solvent.[7] In 2022, the company made significant cuts in its timetable, running fewer and shorter trains, as a consequence of personnel shortages.[8]
Controversies

NS has been involved in various controversies.

    Technical problems with the high-speed V250 trains, which started their services on 29 July 2013, and ended on 17 January 2014, led to the resignation of CEO Bert Meerstadt in June 2016[9] and a parliamentary investigation in 2016. The High Speed Alliance (HSA), an NS (80%) / KLM (20%) joint venture almost went bankrupt due to the late introduction of the trains in combination with a too high price for the concession which the company paid to the Dutch government. HSA was liquidated in 2017.
    In 2013, it was revealed that NS had been using a subsidiary in Ireland, NS Financial Services Company (NSFSC), to reduce its tax liability in the Netherlands. The procedure was determined to be lawful, but it was unfavorable for the Dutch taxpayers for a state-owned company to avoid national taxes. From 1998 the NS used the favourable tax climate in Ireland, which resulted in a profit for NS of more than €270 million but a loss to the Dutch state of €21 million in 2012 alone.[10] The corporate tax rate in Ireland was 12.5%, in the Netherlands 25% at that time.[11] NS used its Irish subsidiary to buy new trains, among others the high-speed V250 trains from the Italian firm AnsaldoBreda.[citation needed] The Dutch Minister of Finance, Jeroen Dijsselbloem, wrote to the parliament that NS would stop this tax evasion.[citation needed] Most rolling stock was transferred to the Netherlands-based NS Lease in December 2017.[12] NSFSC was wound up in April 2019.[13]
    In 2015 it became clear that a subsidiary of NS, Abellio, had shown unfair behaviour about a tendering for public transport in the province of Limburg. The company had obtained confidential information from a competitor, Veolia, through a former employee of Veolia who had been hired by Abellio subsidiary Qbuzz.[14] On 5 June 2015, it became clear that CEO Timo Huges of the NS had given incomplete and incorrect information about the tendering procedure. According to Minister Dijsselbloem, Huges had acted "sloppy, inaccurate and in violation of the law."[15] Consequently, Huges resigned from his position.[16]

Coverage
See also: Train routes in the Netherlands and Railway stations in the Netherlands
Top three busiest railway stations in the Netherlands
Utrecht Centraal
Amsterdam Centraal
Rotterdam Centraal

The NS covers most of the country, with almost all cities connected, mostly with a service frequency of two trains an hour or more and at least four trains per hour between all of the largest five cities (Amsterdam, Rotterdam, The Hague, Utrecht and Eindhoven) as well as some smaller cities (Nijmegen, Amersfoort, Arnhem, 's-Hertogenbosch, Dordrecht and Leiden). From December 2008 train frequencies were increased on the following services: Arnhem–Nijmegen (8 trains per hour) and The Hague–Rotterdam (12 trains per hour), Amsterdam Centraal–Hoofddorp (16 trains per hour). A night train service was added between Utrecht, Gouda and Rotterdam.[17] Trains usually run between 5:00 a.m. and 1:00 a.m. although there is also a nightline which connects major cities in the Randstad throughout the night, as well as in weekends also some major cities in North Brabant.

In addition to its domestic services, NS is also a partner (along with Stena Line and its British railway company Abellio Greater Anglia) in the Dutchflyer service. NS has also entered into a partnership with KLM to operate services on the new HSL-Zuid under the name Intercity Direct towards Breda and Brussels. Intercity Direct is part of NS International; other services such as Thalys to France and Intercity-Express to Germany and Switzerland are also part of NS International.
Rail network
NS trains at Arnhem Centraal

The hoofdrailnet is the official core internal passenger rail network of the Netherlands. Currently, NS has a concession until 1 January 2025 to provide all passenger services on this network, except that on some stretches there is an overlap with lines for which other operators have a concession. Some of the most notable of these stretches are those from Elst railway station to Arnhem Centraal railway station, where NS shares tracks with Arriva, and further on to Arnhem Velperpoort. Here the tracks are shared by three operators, as Breng, ultimately part of Transdev, operates there in addition to the two previously mentioned operators. Officially the overlaps do not constitute competition on the same lines.

The concession was free of charge until 2009, and costs an increasing amount since then, up to €30 million for the year 2014. The concession distinguishes the main stations and other stations. Except on New Year's Eve, the main stations have to be served at least twice an hour per direction from 6 a.m. to midnight and the other stations at least once an hour. Exceptions are possible until the start of the next concession.[18]

The next concession period is 2025–2035. For the 2015–2025 concession, requirements include: for every train service where on average more than one-third of the passengers travel longer than 30 minutes, a train with a toilet is used, every newly ordered train has a toilet and in 2025 every train has to have a toilet. The last trains on the hoofdrailnet without a toilet were the NS SGMm (the so-called classical "Sprinter", retired 2018–21) and the Sprinter Lighttrain (SLT, these trains have since had on-board toilets retrofitted).
Types of train service

NS provides three kinds of train service:

    A Sprinter stops at all stations, and is mainly used for local traffic. On some smaller lines, though, it is the only kind of service. The name is derived from the 'Sprinter' (2900 class) rolling stock; however, the service was sometimes operated using older style rolling stock (such as 'Plan V/T': 400, 500, 800, and 900 class).
    Intercity services only stop at larger stations and were introduced in the 1970s to provide fast train connections throughout the country. Intercity services are operated by DDZ, VIRM and ICM class trains. An exception is the service between Den Haag Centraal and Eindhoven, which makes use of the high-speed line between Rotterdam and Breda, and requires Bombardier Traxx-hauled carriages. When a line is not served by Sprinters, Intercity trains stop at all stations. This takes place on the lines between Alkmaar – Den Helder, Bergen op Zoom – Vlissingen, Hoorn – Enkhuizen, Leiden – Woerden, and Deurne – Venlo. See also Intercity services in the Netherlands and List of Dutch Intercity stations (in Dutch).
    The Intercity Direct service, which offers faster service between Amsterdam Centraal and Breda as it makes use of the high-speed line HSL-Zuid and calls at only two intermediate stations (Schiphol Airport and Rotterdam Centraal). Unlike other Intercity trains, the Intercity Direct requires payment of a supplement on top of the regular fare (€2.60 if bought online and swiping on the platform during peak hours and €1,56 when swiping during off-peak hours) if a passenger's journey involves the high-speed line between Schiphol Airport and Rotterdam Centraal. A regular Intercity service that is free of supplements is still offered.

There are also two former train categories, which are now used only by private operators:

    Stoptrein: This is the original name for Sprinter trains. Between 2003 and 2013 NS discharged the Stoptrein formula in favour of Sprinter. Private operators do not use Sprinter so all private services in the Netherlands (except the four Sneltreins of Arriva, see below) are Stoptrein.
    Sneltrein: Sneltrein (in the English section of the old paper time tables, they were translated as "semi fast train" and were a class between Stoptrain and Intercity) was abandoned by NS in 2008. The NS Sneltrein services are now called Intercity, but they stop more often than "real" Intercities. The result is that some stations (like Woerden) are served by some Intercities while others pass it. As of 2015, there are four Sneltrein services by Arriva.

Fares and tickets
A NS Dagretour (one-time chip card), from Rijssen to Almelo and back.

The OV-chipkaart is the common form of fare payment. Single or return tickets, used by incidental travellers and tourists, are available at ticket machines and service counters at a surcharge of €1. They are a disposable use-once only. It is possible to buy e-tickets online on the Dutch Railways website. E-tickets can also be purchased on the Belgian NMBS/SNCB B-Europe website. For long-term use, season tickets are available.[19]

Travelling with these cards and tickets, one has to register starting a journey (check-in) and ending it (check out) at the destination. One always has to travel away from the point of one's latest check-in. Thus, in the case of a voluntary detour, one has to check out and check in to register starting a new journey.

Travellers need to be aware of the various companies other than the Nederlandse Spoorwegen. One needs to check out with one company and check in with another on some stations. There is common tariff system with four smaller passenger train operating companies: Keolis Nederland and Connexxion in the centre and the east, Veolia on the 'Maaslijn' and 'Heuvellandlijn' in the southeast, Arriva in the north and most of the east of the country and on the 'Merwede-Lingelijn' (from Dordrecht to Geldermalsen).

The OV-chipkaart is also used on buses and trams, where hourly tickets are for sale for those who have too little credit to travel but enough cash.[20]
Off-peak discount passes

NS defines off-peak hours as weekdays from 09:00–16:00 and 18:30–06:30, and on Saturdays and Sundays the whole day. Therefore, the full fare is required on weekdays 06:30–09:00 and 16:00–18:30. With an OV-chipkaart that allows for a discount or free travel, one is automatically granted the discount or free travel at the time of checking in. There are several season tickets available that suit individual preferences.[21]
NS ticket and supplement (Amsterdam to Rotterdam, with Intercity supplement)
Logo
Logo at Maastricht station in 2010

The NS corporate logo was designed in 1968 by Gert Dumbar and Gert-Jan Leuvelink, both of the graphic design company Tel Design. Introduced in that same year, it replaced an earlier design which had been used since 1946. The logo, pervasive within trains and railway stations in the Netherlands, plays a significant part in Nederlandse Spoorwegen's signage, promotions, advertising, and graphic design.

The logo usually appears in blue or black on a dark yellow or white background. Since its introduction, NS livery has also had this same distinct dark yellow or white colour. The logo is a widened letter 'N' and a sideways (reversed) 'S'-shape. The two arrows in the logo represent the train's movement, and the two lines in the middle represent the track.
Divisions of NS
Merseyrail Class 508 at Ellesmere Port in June 2012
Abellio Greater Anglia Class 90 at Stratford in October 2014
NS International ICE 3 in June 2014

    Abellio is the subsidiary for operations outside the Netherlands. Abellio has won several franchises in the United Kingdom and Germany.

    In 2003, Abellio commenced operating its first rail franchise in the United Kingdom, through its 50% shareholding in Serco-Abellio.[22] From 2004 until 2016, Serco-Abellio also operated the Northern Rail franchise.[23] In May 2009, the Travel London and Travel Surrey bus businesses were purchased from National Express and rebranded as Abellio London and Abellio Surrey.[24][25]

    In February 2012, Abellio Greater Anglia commenced operating the Greater Anglia franchise,[26] and in April 2015, Abellio ScotRail commenced operating the ScotRail franchise.[27][28][29] In 2016, Abellio successfully bid to retain the renamed East Anglia franchise until 2025.[30] Abellio has partnered with Mitsui for both the East Anglia and the West Midlands franchises, the latter also with JR East. In June 2019 Abellio began operating the East Midlands Railway eight-year franchise.[31]

    NS Reizigers (NSR) – NS Travellers, responsible for passenger train services and for employing train drivers and conductors.
    NS Stations – the result of merging the former :
        NS Stations – in charge of the operation of all 404 railway stations in the Netherlands, i.e., also those served by other railway companies than NS Reizigers; see also station facilities.
        NS Vastgoed – owns 48 km2 of land, often near stations, and develops and operates these areas as public traffic nodes, offices and apartments.
    NedTrain – train maintenance.
    NS Commercie – product- and customer management (business and product development, marketing, sales and customer service).
    NS International – operator, in conjunction with NS Reizigers and foreign partners, of Thalys (from Amsterdam to Paris), ICE (to Cologne and Frankfurt), Intercities (to Berlin) and Intercity Direct services (to Brussels via the HSL-Zuid) and the ÖBB Nightjet (to Vienna and Innsbruck).

In dealing with the general public, these distinctions are not made and the terms Nederlandse Spoorwegen and NS are used.

NS has contracts with Connexxion and BBA, now Veolia Transport for the provision of bus services to replace train services in the case of planned and unplanned cancellations.

On 23 July 2010 NS sold Strukton to the construction company Oranjewoud N.V.. This concluded a long history of planning, designing and executing track development done by the NS.[32][33]
Policy

There is a delay refund scheme entitling passengers to a partial or full refund of the ticket price if a journey is delayed by half an hour or more. The scheme does not apply on short-distance journeys (tickets less than €2.30) and cases in which the delay is the result of planned cancellations that were announced some days in advance. Refunds are, in general, half the ticket price of a one-way trip after a delay of over 30 minutes, and the full ticket price after a delay of one hour or more. That applies to nearly all kinds of tickets. The refund is not considered monetary compensation for lost time but rather as a reduction in charges where poor service has been provided. The system has been improved for holders of some rail passes. Part of the cost of the scheme is paid by ProRail, since they are responsible for part of the delays.

Tobacco smoking is prohibited both on trains and in stations. Smoking on trains has been prohibited since 2004, with smoking in stations permitted in designated smoking zones, until this too was disallowed in October 2020. [34]

Since June 2003, the sale of coffee, soft drinks, beer, sandwiches, candy, etc., has ceased aboard domestic trains. The increasing number of Servex convenience stores at railway stations and the relatively short duration of most train journeys in the Netherlands have lowered the demand for on-train services. In 2005, a much reduced in-train service of drinks and small snacks has been reintroduced on longer journeys. Now, the RailTender service primarily operates in the intercity trains on the trajectory between Utrecht and Zwolle/Eindhoven, Zwolle and Almere, 's-Hertogenbosch and Nijmegen, Apeldoorn and Amersfoort, Rotterdam and Breda/Roosendaal/Antwerp.
Technological assistance for train staff

Conductors have a smartphone with a timetable, fares information, and a separate card reader to read the OV-chipkaart. Train drivers use a tablet with an app called "TimTim" to save energy and keep up with the timetable. The train driver can also see other trains that are in front or behind his train.
Statistics

    14.73 billion passenger km per year (2005), which is 30% of the seat km.

In 2018, NS saw its number of passengers increase by nearly 3 percent. On average, 1.3 million people took the train on a weekday, 100.000 more than in 2016 and the over 250 NS train stations are becoming increasingly crowded.

The top 15 busiest train stations in the Netherlands by travelers (NS only) per working day in 2019:
Station 	Province 	Daily travelers 2019
Utrecht Centraal 	 Utrecht 	207,400
Amsterdam Centraal 	 North Holland 	199,500
Rotterdam Centraal 	 South Holland 	101,700
Den Haag Centraal 	 South Holland 	98,800
Schiphol Airport 	 North Holland 	98,000
Leiden Centraal 	 South Holland 	82,700
Amsterdam Zuid 	 North Holland 	68,700
Eindhoven Centraal 	 North Brabant 	68,200
Amsterdam Sloterdijk 	 North Holland 	50,500
's Hertogenbosch 	 North Brabant 	49,800
Nijmegen 	 Gelderland 	47,500
Arnhem Centraal 	 Gelderland 	45,700
Amersfoort Centraal 	 Utrecht 	44,800
Haarlem 	 North Holland 	43,800
Zwolle 	 Overijssel 	42,100

Also see List of busiest railway stations in The Netherlands[35]
See also

    flagNetherlands portaliconTrains portal

    Dutch railway services
    NS Timetable 2010
    Rail transport by country
    Rail transport in the Netherlands
    Railway stations in the Netherlands
    Train categories in Europe
    Train routes in the Netherlands
    Trains in the Netherlands
    Transport in the Netherlands

References

"Oud-minister Wouter Koolmees nieuwe topman NS". NOS. 10 October 2022. Retrieved 7 November 2022.
"Ns 2020".
"NS Annual Report 2018" (PDF).
International Union of Railways Annual Report, 2015.
"Annual report 2010". Nederlandse Spoorwegen. 1 January 2011. Archived from the original on 30 June 2011. Retrieved 18 July 2011.
DB and NS sign freight merger Railway Gazette International 1 August 1999
"NS maakt flink verlies door coronacrisis, nog steeds minder reizigers". nos.nl (in Dutch). 25 February 2022. Retrieved 17 December 2022.
"NS snijdt vanwege personeelstekort weer fors in dienstregeling". nos.nl (in Dutch). 31 October 2022. Retrieved 17 December 2022.
"NS-topman Bert Meerstadt stapt op". Archived from the original on 25 June 2015. Retrieved 5 June 2015.
"NS stopt met fiscale truc: treinen niet langer gekocht door Ierse dochter". 25 March 2015. Retrieved 5 June 2015.
"NS doet nog steeds zaken via de Ierse route". NRC Q. Retrieved 5 June 2015.
NS repatriates train leasing Railway Gazette International 26 February 2018
NS ends Irish-registered leasing activities Railway Gazette International 29 April 2019
"Limburg gunt concessie openbaar vervoer aan Arriva". Retrieved 5 June 2015.
"Nog meer problemen voor ex-NS-topman Timo Huges". 19 June 2015. Retrieved 19 June 2015.
"'NS-topman Timo Huges stapt op wegens mogelijk machtsmisbruik'". Retrieved 5 June 2015.
www.treinreiziger.nl Archived 11 December 2008 at the Wayback Machine
"Nieuws". ns.nl. Retrieved 23 September 2016.
Find the season ticket that suits you
"OV-chipkaart (for tourists)". Trans Link Systems B.V. Retrieved 21 April 2018.
All 7 season tickets (that allow for discounts)
Dutch and Serco win Merseyrail franchise The Railway Magazine issue 1226 June 2003 page 6
Serco and NedRailways joint bid secures new Northern franchise Rail Express issue 99 August 2004 page 5
National Express Group plc agreement to sell Travel London Archived 2 February 2014 at the Wayback Machine National Express Group 21 May 2009
NedRailways acquisition reinforces long term commitment to UK transport market Archived 18 January 2016 at the Wayback Machine NedRailways 9 June 2009
Greater Anglia rail franchise announcement Department for Transport 20 October 2011
Dutch firms wins ScotRail franchise from FirstGroup BBC News 8 October 2014
Abellio awarded ScotRail franchise Railway Gazette International 8 October 2014
Abellio awarded contract to operate Scotland's National Railway, ScotRail Archived 15 April 2016 at the Wayback Machine Abellio
Better journeys for rail passengers and boost for Derby train industry as new East Anglia franchise announced Department for Transport 10 August 2016
Dutch firm Abellio takes over East Midlands rail franchise BBC News 18 August 2019
"NS agrees to sell Strukton". Railway Gazette International. 29 July 2010. Retrieved 24 March 2012.
Oranjewoud N.V., the holding company that owns Strukton.
"Roken en verkoop van tabak op treinstations in de ban". nos.nl (in Dutch). 30 January 2020. Retrieved 17 December 2022.

    DUTCH RAILWAY HANDLES 1.3 MILLION TRAVELERS PER WORKING DAY

Further reading

    Johnston, Howard (18–31 May 1989). "A brief guide to the NS". Rail. No. 96. EMAP National Publications. NS 150 special supplement. ISSN 0953-4563. OCLC 49953699.
    "NS Annual Report 2017" (PDF). Annual Report in English. Retrieved 26 September 2018.

External links

    Media related to Nederlandse Spoorwegen at Wikimedia Commons
    Nederlandse Spoorwegen, English website

    vte

National railway companies of Europe
Authority control Edit this at Wikidata
Categories:

    Nederlandse SpoorwegenDutch brandsGovernment-owned companies of the NetherlandsRail transport in the NetherlandsRailway companies of the NetherlandsRailway companies established in 1938Dutch companies established in 1938Organisations based in Utrecht (city)Companies based in Utrecht (province)Rail transport in Utrecht (city)

    This page was last edited on 23 August 2023, at 13:28 (UTC).
    Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
History

        Delta law and conceptual framework
        Alterations to the plan during the execution of the Works
            The storm-surge barrier
            Environmental policy implementations
    Environmental effects
    Project costs
    Current status
    Projects
    See also
    References
    External links

Delta Works

    Article
    Talk

    Read
    Edit
    View history

Tools

Coordinates: 51.65°N 3.72°E
From Wikipedia, the free encyclopedia
Not to be confused with Delta Work.
Delta Works is located in Delta Plan
Algerakering
Algerakering
Antwerp
Antwerp
Bathse spuisluis
Bathse spuisluis
Belgium
Brouwersdam
Brouwersdam
Fresh water
Fresh water
Grevelingendam
Grevelingendam
Hartelkering
Hartelkering
Haringvliet
Haringvliet
Haringvliet Bridge
Haringvliet Bridge
Haringvlietdam
Haringvlietdam
Hellegatsdam
Hellegatsdam
Maeslantkering
Maeslantkering
Markiezaatskade
Markiezaatskade
Netherlands
North Sea
North Sea
Oesterdam
Oesterdam
Oosterschelde
Oosterschelde
Oosterscheldekering
Oosterscheldekering
Philipsdam
Philipsdam
Rotterdam
Rotterdam
Scheldt Rhine Canal
Scheldt Rhine Canal
Veerse Gatdam
Veerse Gatdam
Volkerakdam
Volkerakdam
Seawater
Seawater
Western Scheldt
Western Scheldt
Zandkreekdam
Zandkreekdam
Zeeland Bridge
Zeeland Bridge
Delta Works

The Delta Works (Dutch: Deltawerken) is a series of construction projects in the southwest of the Netherlands to protect a large area of land around the Rhine–Meuse–Scheldt delta from the sea. Constructed between 1954 and 1997, the works consist of dams, sluices, locks, dykes, levees, and storm surge barriers located in the provinces of South Holland and Zeeland.

The aim of the dams, sluices, and storm surge barriers was to shorten the Dutch coastline, thus reducing the number of dikes that had to be raised. Along with the Zuiderzee Works, the Delta Works have been declared one of the Seven Wonders of the Modern World by the American Society of Civil Engineers.
History
3:44CC
Newsreel from 1959 on the advantages and disadvantages of the Delta Works

The estuaries of the rivers Rhine, Meuse and Schelde have been subject to flooding over the centuries. After building the Afsluitdijk (1927 – 1932), the Dutch started studying the damming of the Rhine-Meuse Delta. Plans were developed to shorten the coastline and turn the delta into a group of freshwater coastal lakes. By shortening the coastline, fewer dikes would have to be reinforced.

Due to indecision and the Second World War, little action was taken. In 1950 two small estuary mouths, the Brielse Gat near Brielle and the Botlek near Vlaardingen were dammed. After the North Sea flood of 1953, a Delta Works Commission was installed to research the causes and develop measures to prevent such disasters in future. They revised some of the old plans and came up with the "Deltaplan".

Unlike the Zuiderzee Works, the Delta Plan's purpose is largely defensive and not for land reclamation.[1] The Delta Plan is a national programme and demands collaboration between the national government, provincial authorities, municipal authorities and the water boards. The plan consisted of blocking the estuary mouths of the Oosterschelde, the Haringvliet and the Grevelingen. This reduced the length of the dikes exposed to the sea by 700 kilometres (430 mi). The mouths of the Nieuwe Waterweg and the Westerschelde were to remain open because of the important shipping routes to the ports of Rotterdam and Antwerp. The dikes along these waterways were to be heightened and strengthened. The works would be combined with road and waterway infrastructure to stimulate the economy of the province of Zeeland and improve the connection between the ports of Rotterdam and Antwerp.
Delta law and conceptual framework
Zeeland Bridge
Oosterscheldekering

An important part of this project was fundamental research to come up with long-term solutions, protecting the Netherlands against future floods. Instead of analysing past floods and building protection sufficient to deal with those, the Delta Works commission pioneered a conceptual framework to use as norm for investment in flood defences.

The framework is called the 'Delta norm'; it includes the following principles:

    Major areas to be protected from flooding are identified. These are called "dike ring areas" because they are protected by a ring of primary sea defences.
    The cost of flooding is assessed using a statistical model involving damage to property, lost production, and a given amount per human life lost.
    For the purpose of this model, a human life is valued at €2.2 million (2008 data).
    The chances of a significant flood within the given area are calculated. This is done using data from a purpose-built flood simulation lab, as well as empirical statistical data regarding water wave properties and distribution. Storm behaviour and spring tide distribution are also taken into account.

The most important "dike ring area" is the South Holland coast region. It is home to four million people, most of whom live below normal sea level. The loss of human life in a catastrophic flood here can be very large because there is typically little warning time with North Sea storms. Comprehensive evacuation is not a realistic option for the Holland coastal region.

The commission initially set the acceptable risk for complete failure of every "dike ring" in the country at 1 in 125,000 years. But, it found that the cost of building to this level of protection could not be supported. It set "acceptable" risks by region as follows:

    North and South Holland (excluding Wieringermeer): 1 per 10,000 years
    Other areas at risk from sea flooding: 1 per 4,000 years
    Transition areas between high land and low land: 1 per 2,000 years

River flooding causes less damage than salt water flooding, which causes long-term damage to agricultural lands. Areas at risk from river flooding were assigned a higher acceptable risk. River flooding also has a longer warning time, producing a lower estimated death toll per event.

    South Holland at risk from river flooding: 1 per 1,250 years
    Other areas at risk from river flooding: 1 per 250 years.

These acceptable risks were enshrined in the Delta Law (Dutch: Deltawet). This required the government to keep risks of catastrophic flooding within these limits and to upgrade defences should new insights into risks require this. The limits have also been incorporated into the new Water Law (Waterwet), effective from 22 December 2009.

The Delta Project (of which the Delta Works are a part) has been designed with these guidelines in mind. All other primary defences have been upgraded to meet the norm. New data elevating the risk assessment on expected sea level rise due to global warming has identified ten 'weak points.' These have been upgraded to meet future demands. The latest upgrades are made under the High Water Protection Program.
Alterations to the plan during the execution of the Works
Scale model of the Maeslantkering

During the execution of the works, changes were made in response to public pressure. In the Nieuwe Waterweg, the heightening and the associated widening of the dikes proved very difficult because of public opposition to the planned destruction of important historic buildings to achieve this. The plan was changed to the construction of a storm surge barrier (the Maeslantkering) and dikes were only partly built up.
The storm-surge barrier

The Delta Plan originally intended to create a large freshwater lake, the Zeeuwse Meer (Zeeland Lake).[1] This would have caused major environmental destruction in Oosterschelde, with the total loss of the saltwater ecosystem and, consequently, the harvesting of oysters. Environmentalists and fishermen combined their efforts to prevent the closure; they persuaded parliament to amend the original plan. Instead of completely damming the estuary, the government agreed to build a storm surge barrier. This essentially is a long collection of very large valves that can be closed against storm surges.

The storm surge barrier closes only when the sea-level is expected to rise 3 metres above mean sea level. Under normal conditions, the estuary's mouth is open, and salt water flows in and out with the tide. As a result of the change, the weak dikes along the Oosterschelde needed to be strengthened. Over 200 km of the dike needed new revetments. The connections between the Eastern Scheldt and the neighboring Haringvliet had to be dammed to limit the effect of the salt water. Extra dams and locks were needed at the east part of the Oosterschelde to create a shipping route between the ports of Rotterdam and Antwerp. Since operating the barrier has an effect on the environment, fisheries and the water management system, decisions made on opening or closing the gate are carefully considered. Also the safety of the surrounding dykes are affected by barrier operations.
Environmental policy implementations

In an attempt to restore and preserve the natural system surrounded by the dykes and storm-surge barrier, the concept 'building with nature' was introduced in revised Delta Program updates after 2008. The new integrated water management plan not only takes into account protection against flooding, but also covers water quality, leisure industry, economic activities, shipping, environment and nature. Whenever possible, existing engineering constructions would be replaced by more 'nature friendly' options in an attempt to restore natural estuary and tides, while still protecting against flooding.[2] In addition, building components of the reinforcements are designed in a way that they support formation of entire ecosystems.[3] As part of the revision, the Room for the River projects, enabled nature to occupy space by lowering or widening the river bed.[4] In order to establish this, agricultural flood plains are turned into natural parks, excavated farmland is used for wild vegetation and newly excavated lakes and bypasses create habitats for fish and birds.[5] Along the coast, natural sand is added each year to allow sand to blow freely through the dunes instead of having the dunes held in place by planted vegetation or revetments.[6] Although the new plan brought along additional cost, it was received favourably.[citation needed] The re-considerations of the Delta Project indicated the growing importance of integrate environmental impact assessments in policy-making.
Environmental effects

The Delta Project of which the Delta Works are part of was originally designed in a period of time when environmental awareness and ecological effects of engineering projects were barely taken into consideration.[7] Although the level of awareness for the environment grew throughout the years, the Delta Project has caused numerous irreversible[citation needed] effects on the environment in the past. Blocking the estuary mouths did reduce the length of dykes that otherwise would have to be built to protect against floods, but it also led to major changes in the water systems. For example, the tides disappeared, which resulted in a less smooth transition from sea water into fresh water. Flora and fauna suffered from this noticeable change.[8] In addition, rivers got covered up by polluted sludge, since there was no longer an open passage to the sea.
Project costs

The projects of the Delta Plan are financed with the Delta Fund. In 1958, when the Delta law was accepted under the Delta Works Commission, the total costs were estimated at 3.3 billion guilder. This was at that time equal to 20% of national GDP. This amount was spread out over the 25 years that it would take to complete the massive engineering project. The Delta works were mostly financed by the national budget, with a contribution of the Marshall Plan of 400 million guilder. In addition, the Dutch natural gas discovery contributed massively to the finance of the project. At completion in 1997, costs were set on 8.2 billion guilder.[9] Nevertheless, in 2012 the total costs were already set on[clarification needed] around $13 billion.[10]
Current status

The original plan was completed by the Europoortkering which required the construction of the Maeslantkering in the Nieuwe Waterweg between Maassluis and Hook of Holland and the Hartelkering in the Hartel Canal near Spijkenisse. The works were declared finished after almost forty years in 1997.

Due to climate change and relative sea-level rise, the dikes will eventually have to be made higher and wider. This is a long term uphill battle against the sea. The needed level of flood protection and the resulting costs are a recurring subject of debate, and involve a complicated decision-making process. In 1995 it was agreed in the Delta Plan Large Rivers and Room for the River projects that about 500 kilometres of insufficient dyke revetments were reinforced and replaced along the Oosterschelde and Westerschelde between 1995 and 2015. After 2015, under the High Water Protection Program, additional upgrades are made.[11]

In September 2008, the Delta Commission presided by politician Cees Veerman advised in a report that the Netherlands would need a massive new building program to strengthen the country's water defenses against the anticipated effects of global warming for the next 190 years. The plans included drawing up worst-case scenarios for evacuations and included more than €100 billion, or $144 billion, in new spending through the year 2100 for measures, such as broadening coastal dunes and strengthening sea and river dikes. The commission said the country must plan for a rise in the North Sea of 1.3 meters by 2100 and 4 meters by 2200.[12]
Projects

The works that are part of the Delta Works are listed in chronological order with their year of completion:
Delta Works Project 	Beginning 	Inauguration 	Image 	Function 	Watercourse 	Place
Stormvloedkering Hollandse IJssel (Algerakering) 	1954 	1958 		Flood barrier 	Hollandse IJssel (river) 	South Holland near Krimpen aan den IJssel
Zandkreekdam 	1959 	1960 		Dam 	Zandkreek, Veerse Gat (Oosterschelde) 	Between Noord-Beveland and Zuid-Beveland in the east
Veerse Gatdam 	1960 	1961 		Dam 	Veerse Gat (Oosterschelde) 	Between Noord-Beveland and Walcheren in the west
Grevelingendam 	1958 	1965 		Dam 	Grevelingenmeer 	Between Tholen and Schouwen-Duiveland
Volkerakdam 	1957 	1969 		Dam 	Volkerak, Hollands Diep Meuse and Oosterschelde 	Between South Holland and Zeeland
Haringvlietdam 	1958 	1971 		Dam / Flood barrier 	Haringvliet (Rhine and Meuse) 	Between Voorne-Putten and Goeree-Overflakkee
Brouwersdam 	1964 	1971 		Dam 	Grevelingenmeer 	Between Goeree-Overflakkee and Schouwen-Duiveland
Markiezaatskade 	1980 	1983 		Dam 	Scheldt–Rhine Canal, Markiezaatsmeer 	Between Zuid-Beveland and Molenplaat
Oosterscheldekering 	1960 	1986 		Flood barrier 	Oosterschelde 	Between Schouwen-Duiveland and Noord-Beveland
Oesterdam 	1979 	1987 		Dam 	Oosterschelde, Scheldt–Rhine Canal 	Between Tholen and Zuid-Beveland
Philipsdam 	1976 	1987 		Dam 	Oosterschelde 	Between Grevelingendam and Sint Philipsland
Bathse spuisluis 	1980 	1987 		Lock 	Volkerak, Markiezaatsmeer, Oosterschelde 	Bath, Zeeland
Maeslantkering 	1988 	1997 		Flood barrier 	Nieuwe Waterweg (Rhine) 	Downstream Rotterdam South Holland
Hartelkering 	1991 	1997 		Flood barrier 	Hartelkanaal 	Near Spijkenisse, South Holland
See also

    Flood control in the Netherlands
    Lauwerszee Works
    Megaproject
    Thames Barrier
    Zuiderzee Works
    Johan van Veen
    Pieter Jacobus Wemelsfelder

References

Ley, Willy (October 1961). "The Home-Made Land". For Your Information. Galaxy Science Fiction. pp. 92–106.
Kabat, Pavel; Fresco, Louise; Stive, Marcel J.F.; Veerman, Cees P.; van Alphen, Jos S.L.J.; Parmet, Bart W. A. H.; Hazeleger, Wilco; Katsman, Caroline A. (July 2009). "Dutch coasts in transition". Nature Geoscience. 2 (7): 450–451. Bibcode:2009NatGe...2..450K. doi:10.1038/ngeo572.
Deltares (2014). "Bouwen met de natuur in de praktijk". Delta Life. 1: 14–15.
Van Buuren, A; Ellen, G.J.; Warner, J.F. (2016). "Path-dependency and policy learning in the Dutch delta: toward more resilient flood risk management in the Netherlands?". Ecology and Society. 21 (4). doi:10.5751/es-08765-210443.
Rijcken, Ties (2015). "A critical approach to some new ideas about the Dutch flood risk system". Research in Urbanism Series. 3 (1).
DGW. "Nationaal Waterplan". rijksoverheid.nl. Ministerie van Infrastructuur en Milieu.
d'Angremond, K (2003). "From disaster to Delta Project: the storm flood of 1953". Terra Aqua. 90 (3).
de Vos, Art (2006). Nederland: een natte geschiedenis. Schiedam: Scriptum Publishers. p. 96. ISBN 90-5594-487-4.
Aerts, J.C.J.H. (2009). Adaptation cost in the Netherlands: Climate Change and flood risk management. Climate Changes Spatial Planning and Knowledge for Climate. pp. 34–36. ISBN 9789088150159.
Higgins, Andrew. "Lessons for U.S. From a Flood-Prone Land". The New York Times. Retrieved 13 April 2018.
Rijcken, T (2015). "A critical approach to some new ideas about the Dutch flood risk system". Research in Urbanism Series. 3 (1). doi:10.7480/rius.3.842. S2CID 110283338.

    "Dutch draw up drastic measures to defend coast against rising seas". The New York Times. 3 August 2008. Retrieved 20 October 2010.

External links
Wikimedia Commons has media related to Delta Works.
Wikivoyage has a travel guide for Delta Works.

    Delta Works.Org / Deltawerken.Com official website for the Delta Works
    Knowledge Centre Watersnoodmuseum / Flood Museum - Delta Works Archived 2021-03-08 at the Wayback Machine
    Dutch History Pages
    PDF in Dutch explaining the Delta Framework

    vte

Rhine–Meuse–Scheldt delta
Rhine
Rijn
Rhin
	

Current distributaries
    Waal
    Nederrijn
    IJssel
    Lek
    Merwede
    Boven Merwede
    Nieuwe Merwede
    Beneden Merwede
    Oude Maas
    Dordtsche Kil
    Noord
    Nieuwe Maas
    Scheur
    Nieuwe Waterweg

Former distributaries
    Kromme Rijn
    Leidse Rijn
    Oude Rijn
    Hollandse IJssel
    Vecht
    Waaltje
    Brielse Maas
    Spui

Current estuaries
    Nieuwe Waterweg
    IJsselmeer

Former estuaries
    Hollands Diep
    Haringvliet
    Volkerak
    Krammer
    Grevelingen
    Keeten-Mastgat
    Oosterschelde

Associated canals
    Bijlands Kanaal
    Pannerdens Kanaal
    Amsterdam–Rhine Canal
    Vaartse Rijn
    Nieuwe Merwede
    Nieuwe Waterweg
    Scheldt–Rhine Canal
    Maas–Waal Canal

Meuse
Maas	

Current distributaries
    Bergse Maas
    Amer

Former distributaries
    Oude Maasje
    Afgedamde Maas
    Merwede
    Boven Merwede
    Beneden Merwede
    Oude Maas
    Dordtsche Kil
    Noord
    Nieuwe Maas
    Scheur
    Nieuwe Waterweg

Current estuaries

Former estuaries
    Hollands Diep
    Haringvliet
    Volkerak
    Krammer
    Grevelingen
    Keeten-Mastgat
    Oosterschelde

Associated canals
    Heusden Canal
    Bergse Maas
    Maas–Waal Canal 

Scheldt
Schelde
Escaut	

Current distributaries
    Western Scheldt

Former distributaries
    Oosterschelde
    Eendracht

Current estuaries
    Western Scheldt

Former estuaries
    Oosterschelde
    Krammer
    Grevelingen

Associated canals
    Scheldt–Rhine Canal
    Canal through Zuid-Beveland
    Canal through Walcheren

Other rivers
(directly draining
into the delta)	

    Linge Mark Donge Rotte Oude IJssel

Islands and
Peninsulas	

    Rozenburg IJsselmonde Het Eiland van Dordt Voorne and Putten Hoeksche Waard Tiengemeten Goeree-Overflakkee Schouwen-Duiveland Tholen Sint Philipsland Walcheren Noord-Beveland Zuid-Beveland

Towns	

    Rotterdam Antwerp Dordrecht Bergen-op-Zoom Schiedam Vlissingen Vlaardingen Middelburg Spijkenisse

Other topics	

    Delta Works Verdronken Land van Reimerswaal Verdronken Land van Saeftinghe St. Elizabeth's flood (1421) St. Felix's Flood All Saints' Flood (1570)

Categories:

    Delta WorksRhine–Meuse–Scheldt deltaDikes in the NetherlandsDams in the NetherlandsFlood control in the NetherlandsWater resource management in the NetherlandsScience and technology in the Netherlands1954 establishments in the Netherlands20th-century architecture in the Netherlands

    This page was last edited on 4 July 2023, at 08:09 (UTC).
    Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
History

Topics

    Organizations
    Journals and newsletters
    Conferences
    See also
    Notes
    Further reading
    External links

Theoretical computer science

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
This article is about the branch of computer science and mathematics. For the journal, see Theoretical Computer Science (journal).
An artistic representation of a Turing machine. Turing machines are used to model general computing devices.

Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, formal language theory, the lambda calculus and type theory.

It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:[1]

    TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.

History
Main article: History of computer science

While logical inference and mathematical proof had existed previously, in 1931 Kurt Gödel proved with his incompleteness theorem that there are fundamental limitations on what statements could be proved or disproved.

Information theory was added to the field with a 1948 mathematical theory of communication by Claude Shannon. In the same decade, Donald Hebb introduced a mathematical model of learning in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of neural networks and parallel distributed processing were established. In 1971, Stephen Cook and, working independently, Leonid Levin, proved that there exist practically relevant problems that are NP-complete – a landmark result in computational complexity theory[citation needed].

With the development of quantum mechanics in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously. This led to the concept of a quantum computer in the latter half of the 20th century that took off in the 1990s when Peter Shor showed that such methods could be used to factor large numbers in polynomial time, which, if implemented, would render some modern public key cryptography algorithms like RSA insecure.[citation needed]

Modern theoretical computer science research is based on these basic developments, but includes many other mathematical and interdisciplinary problems that have been posed, as shown below:
P → Q P\rightarrow Q\, 					P = NP ?
Mathematical logic 	Automata theory 	Number theory 	Graph theory 	Computability theory 	Computational complexity theory
GNITIRW-TERCES 	Γ ⊢ x : Int \Gamma \vdash x:{\text{Int}} 				
Cryptography 	Type theory 	Category theory 	Computational geometry 	Combinatorial optimization 	Quantum computing theory
Topics
Algorithms
Main article: Algorithm

An algorithm is a step-by-step procedure for calculations. Algorithms are used for calculation, data processing, and automated reasoning.

An algorithm is an effective method expressed as a finite list[2] of well-defined instructions[3] for calculating a function.[4] Starting from an initial state and initial input (perhaps empty),[5] the instructions describe a computation that, when executed, proceeds through a finite[6] number of well-defined successive states, eventually producing "output"[7] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[8]
Automata theory
Main article: Automata theory

Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science, under discrete mathematics (a section of mathematics and also of computer science). Automata comes from the Greek word αὐτόματα meaning "self-acting".

Automata Theory is the study of self-operating virtual machines to help in the logical understanding of input and output process, without or with intermediate stage(s) of computation (or any function/process).
Coding theory
Main article: Coding theory

Coding theory is the study of the properties of codes and their fitness for a specific application. Codes are used for data compression, cryptography, error-correction and more recently also for network coding. Codes are studied by various scientific disciplines—such as information theory, electrical engineering, mathematics, and computer science—for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction (or detection) of errors in the transmitted data.
Computational biology
Main article: Computational biology

Computational biology involves the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.[9] The field is broadly defined and includes foundations in computer science, applied mathematics, animation, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, ecology, evolution, anatomy, neuroscience, and visualization.[10]

Computational biology is different from biological computation, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers, but is similar to bioinformatics, which is an interdisciplinary science using computers to store and process biological data.
Computational complexity theory
Main article: Computational complexity theory

Computational complexity theory is a branch of the theory of computation that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.

A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.
Computational geometry
Main article: Computational geometry

Computational geometry is a branch of computer science devoted to the study of algorithms that can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry.

The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.

Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).
Computational learning theory
Main article: Computational learning theory

Theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning. In supervised learning, an algorithm is given samples that are labeled in some useful way. For example, the samples might be descriptions of mushrooms, and the labels could be whether or not the mushrooms are edible. The algorithm takes these previously labeled samples and uses them to induce a classifier. This classifier is a function that assigns labels to samples including the samples that have never been previously seen by the algorithm. The goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples.
Computational number theory
Main article: Computational number theory

Computational number theory, also known as algorithmic number theory, is the study of algorithms for performing number theoretic computations. The best known problem in the field is integer factorization.
Cryptography
Main article: Cryptography

Cryptography is the practice and study of techniques for secure communication in the presence of third parties (called adversaries).[11] More generally, it is about constructing and analyzing protocols that overcome the influence of adversaries[12] and that are related to various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation.[13] Modern cryptography intersects the disciplines of mathematics, computer science, and electrical engineering. Applications of cryptography include ATM cards, computer passwords, and electronic commerce.

Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.
Data structures
Main article: Data structure

A data structure is a particular way of organizing data in a computer so that it can be used efficiently.[14][15]

Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, databases use B-tree indexes for small percentages of data retrieval and compilers and databases use dynamic hash tables as look up tables.

Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Storing and retrieving can be carried out on data stored in both main memory and in secondary memory.
Distributed computation
Main article: Distributed computation

Distributed computing studies distributed systems. A distributed system is a software system in which components located on networked computers communicate and coordinate their actions by passing messages.[16] The components interact with each other in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.[16] Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications, and blockchain networks like Bitcoin.

A computer program that runs in a distributed system is called a distributed program, and distributed programming is the process of writing such programs.[17] There are many alternatives for the message passing mechanism, including RPC-like connectors and message queues. An important goal and challenge of distributed systems is location transparency.
Information-based complexity
Main article: Information-based complexity

Information-based complexity (IBC) studies optimal algorithms and computational complexity for continuous problems. IBC has studied continuous problems as path integration, partial differential equations, systems of ordinary differential equations, nonlinear equations, integral equations, fixed points, and very-high-dimensional integration.
Formal methods
Main article: Formal methods

Formal methods are a particular kind of mathematics based techniques for the specification, development and verification of software and hardware systems.[18] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.[19]

Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.[20]
Information theory
Main article: Information theory

Information theory is a branch of applied mathematics, electrical engineering, and computer science involving the quantification of information. Information theory was developed by Claude E. Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data. Since its inception it has broadened to find applications in many other areas, including statistical inference, natural language processing, cryptography, neurobiology,[21] the evolution[22] and function[23] of molecular codes, model selection in statistics,[24] thermal physics,[25] quantum computing, linguistics, plagiarism detection,[26] pattern recognition, anomaly detection and other forms of data analysis.[27]

Applications of fundamental topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for Digital Subscriber Line (DSL)). The field is at the intersection of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields. Important sub-fields of information theory are source coding, channel coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, and measures of information.
Machine learning
Main article: Machine learning

Machine learning is a scientific discipline that deals with the construction and study of algorithms that can learn from data.[28] Such algorithms operate by building a model based on inputs[29]: 2  and using that to make predictions or decisions, rather than following only explicitly programmed instructions.

Machine learning can be considered a subfield of computer science and statistics. It has strong ties to artificial intelligence and optimization, which deliver methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit, rule-based algorithms is infeasible. Example applications include spam filtering, optical character recognition (OCR),[30] search engines and computer vision. Machine learning is sometimes conflated with data mining,[31] although that focuses more on exploratory data analysis.[32] Machine learning and pattern recognition "can be viewed as two facets of the same field."[29]: vii 
Parallel computation
Main article: Parallel computation

Parallel computing is a form of computation in which many calculations are carried out simultaneously,[33] operating on the principle that large problems can often be divided into smaller ones, which are then solved "in parallel". There are several different forms of parallel computing: bit-level, instruction level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling.[34] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[35] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[36]

Parallel computer programs are more difficult to write than sequential ones,[37] because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.

The maximum possible speed-up of a single program as a result of parallelization is known as Amdahl's law.
Programming language theory and program semantics
Main articles: Programming language theory and Program semantics

Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of theoretical computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.

In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically legal strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically illegal strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will execute on a certain platform, hence creating a model of computation.
Quantum computation
Main article: Quantum computation

A quantum computer is a computation system that makes direct use of quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data.[38] Quantum computers are different from digital computers based on transistors. Whereas digital computers require data to be encoded into binary digits (bits), each of which is always in one of two definite states (0 or 1), quantum computation uses qubits (quantum bits), which can be in superpositions of states. A theoretical model is the quantum Turing machine, also known as the universal quantum computer. Quantum computers share theoretical similarities with non-deterministic and probabilistic computers; one example is the ability to be in more than one state simultaneously. The field of quantum computing was first introduced by Yuri Manin in 1980[39] and Richard Feynman in 1982.[40][41] A quantum computer with spins as quantum bits was also formulated for use as a quantum space–time in 1968.[42]

As of 2014, quantum computing is still in its infancy but experiments have been carried out in which quantum computational operations were executed on a very small number of qubits.[43] Both practical and theoretical research continues, and many national governments and military funding agencies support quantum computing research to develop quantum computers for both civilian and national security purposes, such as cryptanalysis.[44]
Symbolic computation
Main article: Symbolic computation

Computer algebra, also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although, properly speaking, computer algebra should be a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have not any given value and are thus manipulated as symbols (therefore the name of symbolic computation).

Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.
Very-large-scale integration
Main article: VLSI

Very-large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining thousands of transistors into a single chip. VLSI began in the 1970s when complex semiconductor and communication technologies were being developed. The microprocessor is a VLSI device. Before the introduction of VLSI technology most ICs had a limited set of functions they could perform. An electronic circuit might consist of a CPU, ROM, RAM and other glue logic. VLSI allows IC makers to add all of these circuits into one chip.
Organizations

    European Association for Theoretical Computer Science
    SIGACT
    Simons Institute for the Theory of Computing

Journals and newsletters

    Discrete Mathematics and Theoretical Computer Science
    Information and Computation
    Theory of Computing (open access journal)
    Formal Aspects of Computing
    Journal of the ACM
    SIAM Journal on Computing (SICOMP)
    SIGACT News
    Theoretical Computer Science
    Theory of Computing Systems
    TheoretiCS (open access journal)
    International Journal of Foundations of Computer Science
    Chicago Journal of Theoretical Computer Science (open access journal)
    Foundations and Trends in Theoretical Computer Science
    Journal of Automata, Languages and Combinatorics
    Acta Informatica
    Fundamenta Informaticae
    ACM Transactions on Computation Theory
    Computational Complexity
    Journal of Complexity
    ACM Transactions on Algorithms
    Information Processing Letters
    Open Computer Science (open access journal)

Conferences

    Annual ACM Symposium on Theory of Computing (STOC)[45]
    Annual IEEE Symposium on Foundations of Computer Science (FOCS)[45]
    Innovations in Theoretical Computer Science (ITCS)
    Mathematical Foundations of Computer Science (MFCS)[46]
    International Computer Science Symposium in Russia (CSR)[47]
    ACM–SIAM Symposium on Discrete Algorithms (SODA)[45]
    IEEE Symposium on Logic in Computer Science (LICS)[45]
    Computational Complexity Conference (CCC)[48]
    International Colloquium on Automata, Languages and Programming (ICALP)[48]
    Annual Symposium on Computational Geometry (SoCG)[48]
    ACM Symposium on Principles of Distributed Computing (PODC)[45]
    ACM Symposium on Parallelism in Algorithms and Architectures (SPAA)[48]
    Annual Conference on Learning Theory (COLT)[48]
    Symposium on Theoretical Aspects of Computer Science (STACS)[48]
    European Symposium on Algorithms (ESA)[48]
    Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX)[48]
    Workshop on Randomization and Computation (RANDOM)[48]
    International Symposium on Algorithms and Computation (ISAAC)[48]
    International Symposium on Fundamentals of Computation Theory (FCT)[49]
    International Workshop on Graph-Theoretic Concepts in Computer Science (WG)

See also

    Formal science
    Unsolved problems in computer science
    Sun–Ni law

Notes

"SIGACT". Retrieved 2017-01-19.
"Any classical mathematical algorithm, for example, can be described in a finite number of English words". Rogers, Hartley Jr. (1967). Theory of Recursive Functions and Effective Computability. McGraw-Hill. Page 2.
Well defined with respect to the agent that executes the algorithm: "There is a computing agent, usually human, which can react to the instructions and carry out the computations" (Rogers 1967, p. 2).
"an algorithm is a procedure for computing a function (with respect to some chosen notation for integers) ... this limitation (to numerical functions) results in no loss of generality", (Rogers 1967, p. 1).
"An algorithm has zero or more inputs, i.e., quantities which are given to it initially before the algorithm begins" (Knuth 1973:5).
"A procedure which has all the characteristics of an algorithm except that it possibly lacks finiteness may be called a 'computational method'" (Knuth 1973:5).
"An algorithm has one or more outputs, i.e. quantities which have a specified relation to the inputs" (Knuth 1973:5).
Whether or not a process with random interior processes (not including the input) is an algorithm is debatable. Rogers opines that: "a computation is carried out in a discrete stepwise fashion, without the use of continuous methods or analog devices . . . carried forward deterministically, without resort to random methods or devices, e.g., dice" (Rogers 1967, p. 2).
"NIH working definition of bioinformatics and computational biology" (PDF). Biomedical Information Science and Technology Initiative. 17 July 2000. Archived from the original (PDF) on 5 September 2012. Retrieved 18 August 2012.
"About the CCMB". Center for Computational Molecular Biology. Retrieved 18 August 2012.
Rivest, Ronald L. (1990). "Cryptology". In J. Van Leeuwen (ed.). Handbook of Theoretical Computer Science. Vol. 1. Elsevier.
Bellare, Mihir; Rogaway, Phillip (21 September 2005). "Introduction". Introduction to Modern Cryptography. p. 10.
Menezes, A. J.; van Oorschot, P. C.; Vanstone, S. A. (1997). Handbook of Applied Cryptography. ISBN 978-0-8493-8523-0.
Paul E. Black (ed.), entry for data structure in Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. 15 December 2004. Online version Accessed May 21, 2009.
Entry data structure in the Encyclopædia Britannica (2009) Online entry accessed on May 21, 2009.
Coulouris, George; Jean Dollimore; Tim Kindberg; Gordon Blair (2011). Distributed Systems: Concepts and Design (5th ed.). Boston: Addison-Wesley. ISBN 978-0-132-14301-1.
Andrews (2000). Dolev (2000). Ghosh (2007), p. 10.
R. W. Butler (2001-08-06). "What is Formal Methods?". Retrieved 2006-11-16.
C. Michael Holloway. "Why Engineers Should Consider Formal Methods" (PDF). 16th Digital Avionics Systems Conference (27–30 October 1997). Archived from the original (PDF) on 16 November 2006. Retrieved 2006-11-16.
Monin, pp.3–4
F. Rieke; D. Warland; R Ruyter van Steveninck; W Bialek (1997). Spikes: Exploring the Neural Code. The MIT press. ISBN 978-0262681087.
Huelsenbeck, J. P.; Ronquist, F.; Nielsen, R.; Bollback, J. P. (2001-12-14). "Bayesian Inference of Phylogeny and Its Impact on Evolutionary Biology". Science. American Association for the Advancement of Science (AAAS). 294 (5550): 2310–2314. Bibcode:2001Sci...294.2310H. doi:10.1126/science.1065889. ISSN 0036-8075. PMID 11743192. S2CID 2138288.
Rando Allikmets, Wyeth W. Wasserman, Amy Hutchinson, Philip Smallwood, Jeremy Nathans, Peter K. Rogan, Thomas D. Schneider, Michael Dean (1998) Organization of the ABCR gene: analysis of promoter and splice junction sequences, Gene 215:1, 111–122
Burnham, K. P. and Anderson D. R. (2002) Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach, Second Edition (Springer Science, New York) ISBN 978-0-387-95364-9.
Jaynes, E. T. (1957-05-15). "Information Theory and Statistical Mechanics". Physical Review. American Physical Society (APS). 106 (4): 620–630. Bibcode:1957PhRv..106..620J. doi:10.1103/physrev.106.620. ISSN 0031-899X.
Charles H. Bennett, Ming Li, and Bin Ma (2003) Chain Letters and Evolutionary Histories, Scientific American 288:6, 76–81
David R. Anderson (November 1, 2003). "Some background on why people in the empirical sciences may want to better understand the information-theoretic methods" (PDF). Archived from the original (PDF) on July 23, 2011. Retrieved 2010-06-23.
Ron Kovahi; Foster Provost (1998). "Glossary of terms". Machine Learning. 30: 271–274. doi:10.1023/A:1007411609915.
C. M. Bishop (2006). Pattern Recognition and Machine Learning. Springer. ISBN 978-0-387-31073-2.
Wernick, Yang, Brankov, Yourganov and Strother, Machine Learning in Medical Imaging, IEEE Signal Processing Magazine, vol. 27, no. 4, July 2010, pp. 25–38
Mannila, Heikki (1996). Data mining: machine learning, statistics, and databases. Int'l Conf. Scientific and Statistical Database Management. IEEE Computer Society.
Friedman, Jerome H. (1998). "Data Mining and Statistics: What's the connection?". Computing Science and Statistics. 29 (1): 3–9.
Gottlieb, Allan; Almasi, George S. (1989). Highly parallel computing. Redwood City, Calif.: Benjamin/Cummings. ISBN 978-0-8053-0177-9.
S.V. Adve et al. (November 2008). "Parallel Computing Research at Illinois: The UPCRC Agenda" Archived 2008-12-09 at the Wayback Machine (PDF). Parallel@Illinois, University of Illinois at Urbana-Champaign. "The main techniques for these performance benefits – increased clock frequency and smarter but increasingly complex architectures – are now hitting the so-called power wall. The computer industry has accepted that future performance increases must largely come from increasing the number of processors (or cores) on a die, rather than making a single core go faster."
Asanovic et al. Old [conventional wisdom]: Power is free, but transistors are expensive. New [conventional wisdom] is [that] power is expensive, but transistors are "free".
Asanovic, Krste et al. (December 18, 2006). "The Landscape of Parallel Computing Research: A View from Berkeley" (PDF). University of California, Berkeley. Technical Report No. UCB/EECS-2006-183. "Old [conventional wisdom]: Increasing clock frequency is the primary method of improving processor performance. New [conventional wisdom]: Increasing parallelism is the primary method of improving processor performance ... Even representatives from Intel, a company generally associated with the 'higher clock-speed is better' position, warned that traditional approaches to maximizing performance through maximizing clock speed have been pushed to their limit."
Hennessy, John L.; Patterson, David A.; Larus, James R. (1999). Computer organization and design : the hardware/software interface (2. ed., 3rd print. ed.). San Francisco: Kaufmann. ISBN 978-1-55860-428-5.
"Quantum Computing with Molecules" article in Scientific American by Neil Gershenfeld and Isaac L. Chuang
Manin, Yu. I. (1980). Vychislimoe i nevychislimoe [Computable and Noncomputable] (in Russian). Sov.Radio. pp. 13–15. Archived from the original on 10 May 2013. Retrieved 4 March 2013.
Feynman, R. P. (1982). "Simulating physics with computers". International Journal of Theoretical Physics. 21 (6): 467–488. Bibcode:1982IJTP...21..467F. CiteSeerX 10.1.1.45.9310. doi:10.1007/BF02650179. S2CID 124545445.
Deutsch, David (1992-01-06). "Quantum computation". Physics World. 5 (6): 57–61. doi:10.1088/2058-7058/5/6/38.
Finkelstein, David (1968). "Space-Time Structure in High Energy Interactions". In Gudehus, T.; Kaiser, G. (eds.). Fundamental Interactions at High Energy. New York: Gordon & Breach.
"New qubit control bodes well for future of quantum computing". Retrieved 26 October 2014.
Quantum Information Science and Technology Roadmap for a sense of where the research is heading.
The 2007 Australian Ranking of ICT Conferences Archived 2009-10-02 at the Wayback Machine: tier A+.
MFCS 2017
CSR 2018
The 2007 Australian Ranking of ICT Conferences Archived 2009-10-02 at the Wayback Machine: tier A.

    FCT 2011 (retrieved 2013-06-03)

Further reading

    Martin Davis, Ron Sigal, Elaine J. Weyuker, Computability, complexity, and languages: fundamentals of theoretical computer science, 2nd ed., Academic Press, 1994, ISBN 0-12-206382-1. Covers theory of computation, but also program semantics and quantification theory. Aimed at graduate students.

External links

    SIGACT directory of additional theory links (archived 15 July 2017)
    Theory Matters Wiki Theoretical Computer Science (TCS) Advocacy Wiki
    List of academic conferences in the area of theoretical computer science at confsearch
    Theoretical Computer Science – StackExchange, a Question and Answer site for researchers in theoretical computer science
    Computer Science Animated
    Theory of computation at the Massachusetts Institute of Technology

    vte

Computer science
Authority control: National Edit this at Wikidata	

    Germany

Categories:

    Theoretical computer scienceFormal sciences

    This page was last edited on 23 August 2023, at 11:29 (UTC).
    Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki

Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Definition

Units

Measurement

Experimental values

Influencing factors

Molecular origins

Prediction

See also

References

    External links

Thermal conductivity

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
For thermal conductivity values, see List of thermal conductivities.

The thermal conductivity of a material is a measure of its ability to conduct heat. It is commonly denoted by k k, λ \lambda , or κ \kappa .

Heat transfer occurs at a lower rate in materials of low thermal conductivity than in materials of high thermal conductivity. For instance, metals typically have high thermal conductivity and are very efficient at conducting heat, while the opposite is true for insulating materials like mineral wool or Styrofoam. Correspondingly, materials of high thermal conductivity are widely used in heat sink applications, and materials of low thermal conductivity are used as thermal insulation. The reciprocal of thermal conductivity is called thermal resistivity.

The defining equation for thermal conductivity is q = − k ∇ T {\displaystyle \mathbf {q} =-k\nabla T}, where q \mathbf {q} is the heat flux, k k is the thermal conductivity, and ∇ T {\displaystyle \nabla T} is the temperature gradient. This is known as Fourier's Law for heat conduction. Although commonly expressed as a scalar, the most general form of thermal conductivity is a second-rank tensor. However, the tensorial description only becomes necessary in materials which are anisotropic.
Definition
Simple definition
Thermal conductivity can be defined in terms of the heat flow q q across a temperature difference.

Consider a solid material placed between two environments of different temperatures. Let T 1 T_{1} be the temperature at x = 0 x=0 and T 2 T_{2} be the temperature at x = L x=L, and suppose T 2 > T 1 {\displaystyle T_{2}>T_{1}}. An example of this scenario is a building on a cold winter day: the solid material in this case is the building wall, separating the cold outdoor environment from the warm indoor environment.

According to the second law of thermodynamics, heat will flow from the hot environment to the cold one as the temperature difference is equalized by diffusion. This is quantified in terms of a heat flux q q, which gives the rate, per unit area, at which heat flows in a given direction (in this case minus x-direction). In many materials, q q is observed to be directly proportional to the temperature difference and inversely proportional to the separation distance L L:[1]

    q = − k ⋅ T 2 − T 1 L . {\displaystyle q=-k\cdot {\frac {T_{2}-T_{1}}{L}}.}

The constant of proportionality k k is the thermal conductivity; it is a physical property of the material. In the present scenario, since T 2 > T 1 {\displaystyle T_{2}>T_{1}} heat flows in the minus x-direction and q q is negative, which in turn means that k > 0 k>0. In general, k k is always defined to be positive. The same definition of k k can also be extended to gases and liquids, provided other modes of energy transport, such as convection and radiation, are eliminated or accounted for.

The preceding derivation assumes that the k k does not change significantly as temperature is varied from T 1 T_{1} to T 2 T_{2}. Cases in which the temperature variation of k k is non-negligible must be addressed using the more general definition of k k discussed below.
General definition

Thermal conduction is defined as the transport of energy due to random molecular motion across a temperature gradient. It is distinguished from energy transport by convection and molecular work in that it does not involve macroscopic flows or work-performing internal stresses.

Energy flow due to thermal conduction is classified as heat and is quantified by the vector q ( r , t ) {\displaystyle \mathbf {q} (\mathbf {r} ,t)}, which gives the heat flux at position r \mathbf {r} and time t t. According to the second law of thermodynamics, heat flows from high to low temperature. Hence, it is reasonable to postulate that q ( r , t ) {\displaystyle \mathbf {q} (\mathbf {r} ,t)} is proportional to the gradient of the temperature field T ( r , t ) {\displaystyle T(\mathbf {r} ,t)}, i.e.

    q ( r , t ) = − k ∇ T ( r , t ) , {\displaystyle \mathbf {q} (\mathbf {r} ,t)=-k\nabla T(\mathbf {r} ,t),}

where the constant of proportionality, k > 0 k>0, is the thermal conductivity. This is called Fourier's law of heat conduction. Despite its name, it is not a law but a definition of thermal conductivity in terms of the independent physical quantities q ( r , t ) {\displaystyle \mathbf {q} (\mathbf {r} ,t)} and T ( r , t ) {\displaystyle T(\mathbf {r} ,t)}.[2][3] As such, its usefulness depends on the ability to determine k k for a given material under given conditions. The constant k k itself usually depends on T ( r , t ) {\displaystyle T(\mathbf {r} ,t)} and thereby implicitly on space and time. An explicit space and time dependence could also occur if the material is inhomogeneous or changing with time.[4]

In some solids, thermal conduction is anisotropic, i.e. the heat flux is not always parallel to the temperature gradient. To account for such behavior, a tensorial form of Fourier's law must be used:

    q ( r , t ) = − κ ⋅ ∇ T ( r , t ) {\displaystyle \mathbf {q} (\mathbf {r} ,t)=-{\boldsymbol {\kappa }}\cdot \nabla T(\mathbf {r} ,t)}

where κ {\boldsymbol {\kappa }} is symmetric, second-rank tensor called the thermal conductivity tensor.[5]

An implicit assumption in the above description is the presence of local thermodynamic equilibrium, which allows one to define a temperature field T ( r , t ) {\displaystyle T(\mathbf {r} ,t)}. This assumption could be violated in systems that are unable to attain local equilibrium, as might happen in the presence of strong nonequilibrium driving or long-ranged interactions.
Other quantities

In engineering practice, it is common to work in terms of quantities which are derivative to thermal conductivity and implicitly take into account design-specific features such as component dimensions.

For instance, thermal conductance is defined as the quantity of heat that passes in unit time through a plate of particular area and thickness when its opposite faces differ in temperature by one kelvin. For a plate of thermal conductivity k k, area A A and thickness L L, the conductance is k A / L {\displaystyle kA/L}, measured in W⋅K−1.[6] The relationship between thermal conductivity and conductance is analogous to the relationship between electrical conductivity and electrical conductance.

Thermal resistance is the inverse of thermal conductance.[6] It is a convenient measure to use in multicomponent design since thermal resistances are additive when occurring in series.[7]

There is also a measure known as the heat transfer coefficient: the quantity of heat that passes per unit time through a unit area of a plate of particular thickness when its opposite faces differ in temperature by one kelvin.[8] In ASTM C168-15, this area-independent quantity is referred to as the "thermal conductance".[9] The reciprocal of the heat transfer coefficient is thermal insulance. In summary, for a plate of thermal conductivity k k, area A A and thickness L L,

    thermal conductance = k A / L {\displaystyle kA/L}, measured in W⋅K−1.
        thermal resistance = L / ( k A ) {\displaystyle L/(kA)}, measured in K⋅W−1.
    heat transfer coefficient = k / L {\displaystyle k/L}, measured in W⋅K−1⋅m−2.
        thermal insulance = L / k {\displaystyle L/k}, measured in K⋅m2⋅W−1.

The heat transfer coefficient is also known as thermal admittance in the sense that the material may be seen as admitting heat to flow.[10]

An additional term, thermal transmittance, quantifies the thermal conductance of a structure along with heat transfer due to convection and radiation.[citation needed] It is measured in the same units as thermal conductance and is sometimes known as the composite thermal conductance. The term U-value is also used.

Finally, thermal diffusivity α \alpha combines thermal conductivity with density and specific heat:[11]

    α = k ρ c p {\displaystyle \alpha ={\frac {k}{\rho c_{p}}}}.

As such, it quantifies the thermal inertia of a material, i.e. the relative difficulty in heating a material to a given temperature using heat sources applied at the boundary.[12]
Units

In the International System of Units (SI), thermal conductivity is measured in watts per metre-kelvin (W/(m⋅K)). Some papers report in watts per centimetre-kelvin (W/(cm⋅K)).

In imperial units, thermal conductivity is measured in BTU/(h⋅ft⋅°F).[note 1][13]

The dimension of thermal conductivity is M1L1T−3Θ−1, expressed in terms of the dimensions mass (M), length (L), time (T), and temperature (Θ).

Other units which are closely related to the thermal conductivity are in common use in the construction and textile industries. The construction industry makes use of measures such as the R-value (resistance) and the U-value (transmittance or conductance). Although related to the thermal conductivity of a material used in an insulation product or assembly, R- and U-values are measured per unit area, and depend on the specified thickness of the product or assembly.[note 2]

Likewise the textile industry has several units including the tog and the clo which express thermal resistance of a material in a way analogous to the R-values used in the construction industry.
Measurement
Main article: Thermal conductivity measurement

There are several ways to measure thermal conductivity; each is suitable for a limited range of materials. Broadly speaking, there are two categories of measurement techniques: steady-state and transient. Steady-state techniques infer the thermal conductivity from measurements on the state of a material once a steady-state temperature profile has been reached, whereas transient techniques operate on the instantaneous state of a system during the approach to steady state. Lacking an explicit time component, steady-state techniques do not require complicated signal analysis (steady state implies constant signals). The disadvantage is that a well-engineered experimental setup is usually needed, and the time required to reach steady state precludes rapid measurement.

In comparison with solid materials, the thermal properties of fluids are more difficult to study experimentally. This is because in addition to thermal conduction, convective and radiative energy transport are usually present unless measures are taken to limit these processes. The formation of an insulating boundary layer can also result in an apparent reduction in the thermal conductivity.[14][15]
Experimental values
Experimental values of thermal conductivity[clarification needed]
Main article: List of thermal conductivities

The thermal conductivities of common substances span at least four orders of magnitude.[16] Gases generally have low thermal conductivity, and pure metals have high thermal conductivity. For example, under standard conditions the thermal conductivity of copper is over 10000 times that of air.

Of all materials, allotropes of carbon, such as graphite and diamond, are usually credited with having the highest thermal conductivities at room temperature.[17] The thermal conductivity of natural diamond at room temperature is several times higher than that of a highly conductive metal such as copper (although the precise value varies depending on the diamond type).[18]

Thermal conductivities of selected substances are tabulated below; an expanded list can be found in the list of thermal conductivities. These values are illustrative estimates only, as they do not account for measurement uncertainties or variability in material definitions.
Substance 	Thermal conductivity (W·m−1·K−1) 	Temperature (°C)
Air[19] 	0.026 	25
Styrofoam[20] 	0.033 	25
Water[21] 	0.6089 	26.85
Concrete[21] 	0.92 	–
Copper[21] 	384.1 	18.05
Natural diamond[18] 	895–1350 	26.85
Influencing factors
Temperature

The effect of temperature on thermal conductivity is different for metals and nonmetals. In metals, heat conductivity is primarily due to free electrons. Following the Wiedemann–Franz law, thermal conductivity of metals is approximately proportional to the absolute temperature (in kelvins) times electrical conductivity. In pure metals the electrical conductivity decreases with increasing temperature and thus the product of the two, the thermal conductivity, stays approximately constant. However, as temperatures approach absolute zero, the thermal conductivity decreases sharply.[22] In alloys the change in electrical conductivity is usually smaller and thus thermal conductivity increases with temperature, often proportionally to temperature. Many pure metals have a peak thermal conductivity between 2 K and 10 K.

On the other hand, heat conductivity in nonmetals is mainly due to lattice vibrations (phonons). Except for high-quality crystals at low temperatures, the phonon mean free path is not reduced significantly at higher temperatures. Thus, the thermal conductivity of nonmetals is approximately constant at high temperatures. At low temperatures well below the Debye temperature, thermal conductivity decreases, as does the heat capacity, due to carrier scattering from defects.[22]
Chemical phase

When a material undergoes a phase change (e.g. from solid to liquid), the thermal conductivity may change abruptly. For instance, when ice melts to form liquid water at 0 °C, the thermal conductivity changes from 2.18 W/(m⋅K) to 0.56 W/(m⋅K).[23]

Even more dramatically, the thermal conductivity of a fluid diverges in the vicinity of the vapor-liquid critical point.[24]
Thermal anisotropy

Some substances, such as non-cubic crystals, can exhibit different thermal conductivities along different crystal axes. Sapphire is a notable example of variable thermal conductivity based on orientation and temperature, with 35 W/(m⋅K) along the c axis and 32 W/(m⋅K) along the a axis.[25] Wood generally conducts better along the grain than across it. Other examples of materials where the thermal conductivity varies with direction are metals that have undergone heavy cold pressing, laminated materials, cables, the materials used for the Space Shuttle thermal protection system, and fiber-reinforced composite structures.[26]

When anisotropy is present, the direction of heat flow may differ from the direction of the thermal gradient.
Electrical conductivity

In metals, thermal conductivity is approximately correlated with electrical conductivity according to the Wiedemann–Franz law, as freely moving valence electrons transfer not only electric current but also heat energy. However, the general correlation between electrical and thermal conductance does not hold for other materials, due to the increased importance of phonon carriers for heat in non-metals. Highly electrically conductive silver is less thermally conductive than diamond, which is an electrical insulator but conducts heat via phonons due to its orderly array of atoms.
Magnetic field

The influence of magnetic fields on thermal conductivity is known as the thermal Hall effect or Righi–Leduc effect.
Gaseous phases
Exhaust system components with ceramic coatings having a low thermal conductivity reduce heating of nearby sensitive components

In the absence of convection, air and other gases are good insulators. Therefore, many insulating materials function simply by having a large number of gas-filled pockets which obstruct heat conduction pathways. Examples of these include expanded and extruded polystyrene (popularly referred to as "styrofoam") and silica aerogel, as well as warm clothes. Natural, biological insulators such as fur and feathers achieve similar effects by trapping air in pores, pockets, or voids.

Low density gases, such as hydrogen and helium typically have high thermal conductivity. Dense gases such as xenon and dichlorodifluoromethane have low thermal conductivity. An exception, sulfur hexafluoride, a dense gas, has a relatively high thermal conductivity due to its high heat capacity. Argon and krypton, gases denser than air, are often used in insulated glazing (double paned windows) to improve their insulation characteristics.

The thermal conductivity through bulk materials in porous or granular form is governed by the type of gas in the gaseous phase, and its pressure.[27] At low pressures, the thermal conductivity of a gaseous phase is reduced, with this behaviour governed by the Knudsen number, defined as K n = l / d {\displaystyle K_{n}=l/d}, where l l is the mean free path of gas molecules and d d is the typical gap size of the space filled by the gas. In a granular material d d corresponds to the characteristic size of the gaseous phase in the pores or intergranular spaces.[27]
Isotopic purity

The thermal conductivity of a crystal can depend strongly on isotopic purity, assuming other lattice defects are negligible. A notable example is diamond: at a temperature of around 100 K the thermal conductivity increases from 10,000 W·m−1·K−1 for natural type IIa diamond (98.9% 12C), to 41,000 for 99.9% enriched synthetic diamond. A value of 200,000 is predicted for 99.999% 12C at 80 K, assuming an otherwise pure crystal.[28] The thermal conductivity of 99% isotopically enriched cubic boron nitride is ~ 1400 W·m−1·K−1,[29] which is 90% higher than that of natural boron nitride.
Molecular origins

The molecular mechanisms of thermal conduction vary among different materials, and in general depend on details of the microscopic structure and molecular interactions. As such, thermal conductivity is difficult to predict from first-principles. Any expressions for thermal conductivity which are exact and general, e.g. the Green-Kubo relations, are difficult to apply in practice, typically consisting of averages over multiparticle correlation functions.[30] A notable exception is a monatomic dilute gas, for which a well-developed theory exists expressing thermal conductivity accurately and explicitly in terms of molecular parameters.

In a gas, thermal conduction is mediated by discrete molecular collisions. In a simplified picture of a solid, thermal conduction occurs by two mechanisms: 1) the migration of free electrons and 2) lattice vibrations (phonons). The first mechanism dominates in pure metals and the second in non-metallic solids. In liquids, by contrast, the precise microscopic mechanisms of thermal conduction are poorly understood.[31]
Gases
See also: Kinetic theory of gases

In a simplified model of a dilute monatomic gas, molecules are modeled as rigid spheres which are in constant motion, colliding elastically with each other and with the walls of their container. Consider such a gas at temperature T T and with density ρ \rho , specific heat c v c_{v} and molecular mass m m. Under these assumptions, an elementary calculation yields for the thermal conductivity

    k = β ρ λ c v 2 k B T π m , {\displaystyle k=\beta \rho \lambda c_{v}{\sqrt {\frac {2k_{\text{B}}T}{\pi m}}},}

where β \beta is a numerical constant of order 1 1, k B k_{\text{B}} is the Boltzmann constant, and λ \lambda is the mean free path, which measures the average distance a molecule travels between collisions.[32] Since λ \lambda is inversely proportional to density, this equation predicts that thermal conductivity is independent of density for fixed temperature. The explanation is that increasing density increases the number of molecules which carry energy but decreases the average distance λ \lambda a molecule can travel before transferring its energy to a different molecule: these two effects cancel out. For most gases, this prediction agrees well with experiments at pressures up to about 10 atmospheres.[33] On the other hand, experiments show a more rapid increase with temperature than k ∝ T {\displaystyle k\propto {\sqrt {T}}} (here, λ \lambda is independent of T T). This failure of the elementary theory can be traced to the oversimplified "elastic sphere" model, and in particular to the fact that the interparticle attractions, present in all real-world gases, are ignored.

To incorporate more complex interparticle interactions, a systematic approach is necessary. One such approach is provided by Chapman–Enskog theory, which derives explicit expressions for thermal conductivity starting from the Boltzmann equation. The Boltzmann equation, in turn, provides a statistical description of a dilute gas for generic interparticle interactions. For a monatomic gas, expressions for k k derived in this way take the form

    k = 25 32 π m k B T π σ 2 Ω ( T ) c v , {\displaystyle k={\frac {25}{32}}{\frac {\sqrt {\pi mk_{\text{B}}T}}{\pi \sigma ^{2}\Omega (T)}}c_{v},}

where σ \sigma is an effective particle diameter and Ω ( T ) \Omega(T) is a function of temperature whose explicit form depends on the interparticle interaction law.[34][33] For rigid elastic spheres, Ω ( T ) \Omega(T) is independent of T T and very close to 1 1. More complex interaction laws introduce a weak temperature dependence. The precise nature of the dependence is not always easy to discern, however, as Ω ( T ) \Omega(T) is defined as a multi-dimensional integral which may not be expressible in terms of elementary functions. An alternate, equivalent way to present the result is in terms of the gas viscosity μ \mu , which can also be calculated in the Chapman–Enskog approach:

    k = f μ c v , {\displaystyle k=f\mu c_{v},}

where f f is a numerical factor which in general depends on the molecular model. For smooth spherically symmetric molecules, however, f f is very close to 2.5 2.5, not deviating by more than 1 % {\displaystyle 1\%} for a variety of interparticle force laws.[35] Since k k, μ \mu , and c v c_{v} are each well-defined physical quantities which can be measured independent of each other, this expression provides a convenient test of the theory. For monatomic gases, such as the noble gases, the agreement with experiment is fairly good.[36]

For gases whose molecules are not spherically symmetric, the expression k = f μ c v {\displaystyle k=f\mu c_{v}} still holds. In contrast with spherically symmetric molecules, however, f f varies significantly depending on the particular form of the interparticle interactions: this is a result of the energy exchanges between the internal and translational degrees of freedom of the molecules. An explicit treatment of this effect is difficult in the Chapman–Enskog approach. Alternately, the approximate expression f = ( 1 / 4 ) ( 9 γ − 5 ) {\displaystyle f=(1/4){(9\gamma -5)}} was suggested by Eucken, where γ \gamma is the heat capacity ratio of the gas.[35][37]

The entirety of this section assumes the mean free path λ \lambda is small compared with macroscopic (system) dimensions. In extremely dilute gases this assumption fails, and thermal conduction is described instead by an apparent thermal conductivity which decreases with density. Ultimately, as the density goes to 0 {\displaystyle 0} the system approaches a vacuum, and thermal conduction ceases entirely.
Liquids

The exact mechanisms of thermal conduction are poorly understood in liquids: there is no molecular picture which is both simple and accurate. An example of a simple but very rough theory is that of Bridgman, in which a liquid is ascribed a local molecular structure similar to that of a solid, i.e. with molecules located approximately on a lattice. Elementary calculations then lead to the expression

    k = 3 ( N A / V ) 2 / 3 k B v s , {\displaystyle k=3(N_{\text{A}}/V)^{2/3}k_{\text{B}}v_{\text{s}},}

where N A N_{{\text{A}}} is the Avogadro constant, V V is the volume of a mole of liquid, and v s {\displaystyle v_{\text{s}}} is the speed of sound in the liquid. This is commonly called Bridgman's equation.[38]
Metals
Main article: Free electron model

For metals at low temperatures the heat is carried mainly by the free electrons. In this case the mean velocity is the Fermi velocity which is temperature independent. The mean free path is determined by the impurities and the crystal imperfections which are temperature independent as well. So the only temperature-dependent quantity is the heat capacity c, which, in this case, is proportional to T. So

    k = k 0 T  (metal at low temperature) {\displaystyle k=k_{0}\,T{\text{ (metal at low temperature)}}}

with k0 a constant. For pure metals, k0 is large, so the thermal conductivity is high. At higher temperatures the mean free path is limited by the phonons, so the thermal conductivity tends to decrease with temperature. In alloys the density of the impurities is very high, so l and, consequently k, are small. Therefore, alloys, such as stainless steel, can be used for thermal insulation.
Lattice waves, phonons, in dielectric solids
	
This section may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (January 2019) (Learn how and when to remove this template message)
	
This article relies excessively on references to primary sources. Please improve this article by adding secondary or tertiary sources.
Find sources: "Thermal conductivity" – news · newspapers · books · scholar · JSTOR (February 2021) (Learn how and when to remove this template message)

Heat transport in both amorphous and crystalline dielectric solids is by way of elastic vibrations of the lattice (i.e., phonons). This transport mechanism is theorized to be limited by the elastic scattering of acoustic phonons at lattice defects. This has been confirmed by the experiments of Chang and Jones on commercial glasses and glass ceramics, where the mean free paths were found to be limited by "internal boundary scattering" to length scales of 10−2 cm to 10−3 cm.[39][40]

The phonon mean free path has been associated directly with the effective relaxation length for processes without directional correlation. If Vg is the group velocity of a phonon wave packet, then the relaxation length l l\; is defined as:

    l = V g t {\displaystyle l\;=V_{\text{g}}t}

where t is the characteristic relaxation time. Since longitudinal waves have a much greater phase velocity than transverse waves,[41] Vlong is much greater than Vtrans, and the relaxation length or mean free path of longitudinal phonons will be much greater. Thus, thermal conductivity will be largely determined by the speed of longitudinal phonons.[39][42]

Regarding the dependence of wave velocity on wavelength or frequency (dispersion), low-frequency phonons of long wavelength will be limited in relaxation length by elastic Rayleigh scattering. This type of light scattering from small particles is proportional to the fourth power of the frequency. For higher frequencies, the power of the frequency will decrease until at highest frequencies scattering is almost frequency independent. Similar arguments were subsequently generalized to many glass forming substances using Brillouin scattering.[43][44][45][46]

Phonons in the acoustical branch dominate the phonon heat conduction as they have greater energy dispersion and therefore a greater distribution of phonon velocities. Additional optical modes could also be caused by the presence of internal structure (i.e., charge or mass) at a lattice point; it is implied that the group velocity of these modes is low and therefore their contribution to the lattice thermal conductivity λL ( κ \kappa L) is small.[47]

Each phonon mode can be split into one longitudinal and two transverse polarization branches. By extrapolating the phenomenology of lattice points to the unit cells it is seen that the total number of degrees of freedom is 3pq when p is the number of primitive cells with q atoms/unit cell. From these only 3p are associated with the acoustic modes, the remaining 3p(q − 1) are accommodated through the optical branches. This implies that structures with larger p and q contain a greater number of optical modes and a reduced λL.

From these ideas, it can be concluded that increasing crystal complexity, which is described by a complexity factor CF (defined as the number of atoms/primitive unit cell), decreases λL.[48][failed verification] This was done by assuming that the relaxation time τ decreases with increasing number of atoms in the unit cell and then scaling the parameters of the expression for thermal conductivity in high temperatures accordingly.[47]

Describing anharmonic effects is complicated because an exact treatment as in the harmonic case is not possible, and phonons are no longer exact eigensolutions to the equations of motion. Even if the state of motion of the crystal could be described with a plane wave at a particular time, its accuracy would deteriorate progressively with time. Time development would have to be described by introducing a spectrum of other phonons, which is known as the phonon decay. The two most important anharmonic effects are the thermal expansion and the phonon thermal conductivity.

Only when the phonon number ‹n› deviates from the equilibrium value ‹n›0, can a thermal current arise as stated in the following expression

    Q x = 1 V ∑ q , j ℏ ω ( ⟨ n ⟩ − ⟨ n ⟩ 0 ) v x , Q_{x}={\frac {1}{V}}\sum _{q,j}{\hslash \omega \left(\left\langle n\right\rangle -{\left\langle n\right\rangle }^{0}\right)v_{x}}{\text{,}}

where v is the energy transport velocity of phonons. Only two mechanisms exist that can cause time variation of ‹n› in a particular region. The number of phonons that diffuse into the region from neighboring regions differs from those that diffuse out, or phonons decay inside the same region into other phonons. A special form of the Boltzmann equation

    d ⟨ n ⟩ d t = ( ∂ ⟨ n ⟩ ∂ t ) diff. + ( ∂ ⟨ n ⟩ ∂ t ) decay {\frac {d\left\langle n\right\rangle }{dt}}={\left({\frac {\partial \left\langle n\right\rangle }{\partial t}}\right)}_{\text{diff.}}+{\left({\frac {\partial \left\langle n\right\rangle }{\partial t}}\right)}_{\text{decay}}

states this. When steady state conditions are assumed the total time derivate of phonon number is zero, because the temperature is constant in time and therefore the phonon number stays also constant. Time variation due to phonon decay is described with a relaxation time (τ) approximation

    ( ∂ ⟨ n ⟩ ∂ t ) decay = −   ⟨ n ⟩ − ⟨ n ⟩ 0 τ , {\displaystyle {\left({\frac {\partial \left\langle n\right\rangle }{\partial t}}\right)}_{\text{decay}}=-{\text{ }}{\frac {\left\langle n\right\rangle -{\left\langle n\right\rangle }^{0}}{\tau }},}

which states that the more the phonon number deviates from its equilibrium value, the more its time variation increases. At steady state conditions and local thermal equilibrium are assumed we get the following equation

    ( ∂ ( n ) ∂ t ) diff. = − v x ∂ ( n ) 0 ∂ T ∂ T ∂ x . {\left({\frac {\partial \left(n\right)}{\partial t}}\right)}_{\text{diff.}}=-{v}_{x}{\frac {\partial {\left(n\right)}^{0}}{\partial T}}{\frac {\partial T}{\partial x}}{\text{.}}

Using the relaxation time approximation for the Boltzmann equation and assuming steady-state conditions, the phonon thermal conductivity λL can be determined. The temperature dependence for λL originates from the variety of processes, whose significance for λL depends on the temperature range of interest. Mean free path is one factor that determines the temperature dependence for λL, as stated in the following equation

    λ L = 1 3 V ∑ q , j v ( q , j ) Λ ( q , j ) ∂ ∂ T ϵ ( ω ( q , j ) , T ) , {\displaystyle {\lambda }_{L}={\frac {1}{3V}}\sum _{q,j}v\left(q,j\right)\Lambda \left(q,j\right){\frac {\partial }{\partial T}}\epsilon \left(\omega \left(q,j\right),T\right),}

where Λ is the mean free path for phonon and ∂ ∂ T ϵ {\displaystyle {\frac {\partial }{\partial T}}\epsilon } denotes the heat capacity. This equation is a result of combining the four previous equations with each other and knowing that ⟨ v x 2 ⟩ = 1 3 v 2 \left\langle v_{x}^{2}\right\rangle ={\frac {1}{3}}v^{2} for cubic or isotropic systems and Λ = v τ \Lambda =v\tau .[49]

At low temperatures (< 10 K) the anharmonic interaction does not influence the mean free path and therefore, the thermal resistivity is determined only from processes for which q-conservation does not hold. These processes include the scattering of phonons by crystal defects, or the scattering from the surface of the crystal in case of high quality single crystal. Therefore, thermal conductance depends on the external dimensions of the crystal and the quality of the surface. Thus, temperature dependence of λL is determined by the specific heat and is therefore proportional to T3.[49]

Phonon quasimomentum is defined as ℏq and differs from normal momentum because it is only defined within an arbitrary reciprocal lattice vector. At higher temperatures (10 K < T < Θ), the conservation of energy ℏ ω 1 = ℏ ω 2 + ℏ ω 3 {\displaystyle \hslash {\omega }_{1}=\hslash {\omega }_{2}+\hslash {\omega }_{3}} and quasimomentum q 1 = q 2 + q 3 + G {\displaystyle \mathbf {q} _{1}=\mathbf {q} _{2}+\mathbf {q} _{3}+\mathbf {G} }, where q1 is wave vector of the incident phonon and q2, q3 are wave vectors of the resultant phonons, may also involve a reciprocal lattice vector G complicating the energy transport process. These processes can also reverse the direction of energy transport.

Therefore, these processes are also known as Umklapp (U) processes and can only occur when phonons with sufficiently large q-vectors are excited, because unless the sum of q2 and q3 points outside of the Brillouin zone the momentum is conserved and the process is normal scattering (N-process). The probability of a phonon to have energy E is given by the Boltzmann distribution P ∝ e − E / k T P\propto {e}^{{-E/kT}}. To U-process to occur the decaying phonon to have a wave vector q1 that is roughly half of the diameter of the Brillouin zone, because otherwise quasimomentum would not be conserved.

Therefore, these phonons have to possess energy of ∼ k Θ / 2 \sim k\Theta /2, which is a significant fraction of Debye energy that is needed to generate new phonons. The probability for this is proportional to e − Θ / b T {e}^{{-\Theta /bT}}, with b = 2 b=2. Temperature dependence of the mean free path has an exponential form e Θ / b T {e}^{{\Theta /bT}}. The presence of the reciprocal lattice wave vector implies a net phonon backscattering and a resistance to phonon and thermal transport resulting finite λL,[47] as it means that momentum is not conserved. Only momentum non-conserving processes can cause thermal resistance.[49]

At high temperatures (T > Θ), the mean free path and therefore λL has a temperature dependence T−1, to which one arrives from formula e Θ / b T {e}^{{\Theta /bT}} by making the following approximation e x ∝ x   ,   ( x ) < 1 {\displaystyle {e}^{x}\propto x{\text{ }},{\text{ }}\left(x\right)<1}[clarification needed] and writing x = Θ / b T x=\Theta /bT. This dependency is known as Eucken's law and originates from the temperature dependency of the probability for the U-process to occur.[47][49]

Thermal conductivity is usually described by the Boltzmann equation with the relaxation time approximation in which phonon scattering is a limiting factor. Another approach is to use analytic models or molecular dynamics or Monte Carlo based methods to describe thermal conductivity in solids.

Short wavelength phonons are strongly scattered by impurity atoms if an alloyed phase is present, but mid and long wavelength phonons are less affected. Mid and long wavelength phonons carry significant fraction of heat, so to further reduce lattice thermal conductivity one has to introduce structures to scatter these phonons. This is achieved by introducing interface scattering mechanism, which requires structures whose characteristic length is longer than that of impurity atom. Some possible ways to realize these interfaces are nanocomposites and embedded nanoparticles or structures.
Prediction

Because thermal conductivity depends continuously on quantities like temperature and material composition, it cannot be fully characterized by a finite number of experimental measurements. Predictive formulas become necessary if experimental values are not available under the physical conditions of interest. This capability is important in thermophysical simulations, where quantities like temperature and pressure vary continuously with space and time, and may encompass extreme conditions inaccessible to direct measurement.[50]
In fluids

For the simplest fluids, such as dilute monatomic gases and their mixtures, ab initio quantum mechanical computations can accurately predict thermal conductivity in terms of fundamental atomic properties—that is, without reference to existing measurements of thermal conductivity or other transport properties.[51] This method uses Chapman-Enskog theory to evaluate a low-density expansion of thermal conductivity. Chapman-Enskog theory, in turn, takes fundamental intermolecular potentials as input, which are computed ab initio from a quantum mechanical description.

For most fluids, such high-accuracy, first-principles computations are not feasible. Rather, theoretical or empirical expressions must be fit to existing thermal conductivity measurements. If such an expression is fit to high-fidelity data over a large range of temperatures and pressures, then it is called a "reference correlation" for that material. Reference correlations have been published for many pure materials; examples are carbon dioxide, ammonia, and benzene.[52][53][54] Many of these cover temperature and pressure ranges that encompass gas, liquid, and supercritical phases.

Thermophysical modeling software often relies on reference correlations for predicting thermal conductivity at user-specified temperature and pressure. These correlations may be proprietary. Examples are REFPROP[55] (proprietary) and CoolProp[56] (open-source).

Thermal conductivity can also be computed using the Green-Kubo relations, which express transport coefficients in terms of the statistics of molecular trajectories.[57] The advantage of these expressions is that they are formally exact and valid for general systems. The disadvantage is that they require detailed knowledge of particle trajectories, available only in computationally expensive simulations such as molecular dynamics. An accurate model for interparticle interactions is also required, which may be difficult to obtain for complex molecules.[58]
In solids
[icon]	
This section needs expansion. You can help by adding to it. (January 2022)
See also

    Copper in heat exchangers
    Heat pump
    Heat transfer
    Heat transfer mechanisms
    Insulated pipe
    Interfacial thermal resistance
    Laser flash analysis
    List of thermal conductivities
    Phase-change material
    R-value (insulation)
    Specific heat capacity
    Thermal bridge
    Thermal conductance quantum
    Thermal contact conductance
    Thermal diffusivity
    Thermal effusivity
    Thermal entrance length
    Thermal interface material
    Thermal diode
    Thermal resistance
    Thermistor
    Thermocouple
    Thermodynamics
    Thermal conductivity measurement
    Refractory metals

References
Notes

1 Btu/(h⋅ft⋅°F) = 1.730735 W/(m⋅K)

    R-values and U-values quoted in the US (based on the inch-pound units of measurement) do not correspond with and are not compatible with those used outside the US (based on the SI units of measurement).

Citations

Bird, Stewart & Lightfoot 2006, p. 266.
Bird, Stewart & Lightfoot 2006, pp. 266–267.
Holman, J.P. (1997), Heat Transfer (8th ed.), McGraw Hill, p. 2, ISBN 0-07-844785-2
Bejan, Adrian (1993), Heat Transfer, John Wiley & Sons, pp. 10–11, ISBN 0-471-50290-1
Bird, Stewart & Lightfoot 2006, p. 267.
Bejan, p. 34
Bird, Stewart & Lightfoot 2006, p. 305.
Gray, H.J.; Isaacs, Alan (1975). A New Dictionary of Physics (2nd ed.). Longman Group Limited. p. 251. ISBN 0582322421.
ASTM C168 − 15a Standard Terminology Relating to Thermal Insulation.
"Thermal Performance: Thermal Mass in Buildings". greenspec.co.uk. Retrieved 2022-09-13.
Bird, Stewart & Lightfoot 2006, p. 268.
Incropera, Frank P.; DeWitt, David P. (1996), Fundamentals of heat and mass transfer (4th ed.), Wiley, pp. 50–51, ISBN 0-471-30460-3
Perry, R. H.; Green, D. W., eds. (1997). Perry's Chemical Engineers' Handbook (7th ed.). McGraw-Hill. Table 1–4. ISBN 978-0-07-049841-9.
Daniel V. Schroeder (2000), An Introduction to Thermal Physics, Addison Wesley, p. 39, ISBN 0-201-38027-7
Chapman, Sydney; Cowling, T.G. (1970), The Mathematical Theory of Non-Uniform Gases (3rd ed.), Cambridge University Press, p. 248
Heap, Michael J.; Kushnir, Alexandra R.L.; Vasseur, Jérémie; Wadsworth, Fabian B.; Harlé, Pauline; Baud, Patrick; Kennedy, Ben M.; Troll, Valentin R.; Deegan, Frances M. (2020-06-01). "The thermal properties of porous andesite". Journal of Volcanology and Geothermal Research. 398: 106901. Bibcode:2020JVGR..39806901H. doi:10.1016/j.jvolgeores.2020.106901. ISSN 0377-0273. S2CID 219060797.
An unlikely competitor for diamond as the best thermal conductor, Phys.org news (July 8, 2013).
"Thermal Conductivity in W cm−1 K−1 of Metals and Semiconductors as a Function of Temperature", in CRC Handbook of Chemistry and Physics, 99th Edition (Internet Version 2018), John R. Rumble, ed., CRC Press/Taylor & Francis, Boca Raton, FL.
Lindon C. Thomas (1992), Heat Transfer, Prentice Hall, p. 8, ISBN 978-0133849424
"Thermal Conductivity of common Materials and Gases". www.engineeringtoolbox.com.
Bird, Stewart & Lightfoot 2006, pp. 270–271.
Hahn, David W.; Özişik, M. Necati (2012). Heat conduction (3rd ed.). Hoboken, N.J.: Wiley. p. 5. ISBN 978-0-470-90293-6.
Ramires, M. L. V.; Nieto de Castro, C. A.; Nagasaka, Y.; Nagashima, A.; Assael, M. J.; Wakeham, W. A. (July 6, 1994). "Standard reference data for the thermal conductivity of water". Journal of Physical and Chemical Reference Data. NIST. 24 (3): 1377–1381. doi:10.1063/1.555963. Retrieved 25 May 2017.
Millat, Jürgen; Dymond, J.H.; Nieto de Castro, C.A. (2005). Transport properties of fluids: their correlation, prediction, and estimation. Cambridge New York: IUPAC/Cambridge University Press. ISBN 978-0-521-02290-3.
"Sapphire, Al2O3". Almaz Optics. Retrieved 2012-08-15.
Hahn, David W.; Özişik, M. Necati (2012). Heat conduction (3rd ed.). Hoboken, N.J.: Wiley. p. 614. ISBN 978-0-470-90293-6.
Dai, W.; et al. (2017). "Influence of gas pressure on the effective thermal conductivity of ceramic breeder pebble beds". Fusion Engineering and Design. 118: 45–51. doi:10.1016/j.fusengdes.2017.03.073.
Wei, Lanhua; Kuo, P. K.; Thomas, R. L.; Anthony, T. R.; Banholzer, W. F. (16 February 1993). "Thermal conductivity of isotopically modified single crystal diamond". Physical Review Letters. 70 (24): 3764–3767. Bibcode:1993PhRvL..70.3764W. doi:10.1103/PhysRevLett.70.3764. PMID 10053956.
Chen, Ke; Song, Bai; Ravichandran, Navaneetha K.; Zheng, Qiye; Chen, Xi; Lee, Hwijong; Sun, Haoran; Li, Sheng; Gamage, Geethal Amila Gamage Udalamatta; Tian, Fei; Ding, Zhiwei (2020-01-31). "Ultrahigh thermal conductivity in isotope-enriched cubic boron nitride". Science. 367 (6477): 555–559. Bibcode:2020Sci...367..555C. doi:10.1126/science.aaz6149. hdl:1721.1/127819. ISSN 0036-8075. PMID 31919128. S2CID 210131908.
see, e.g., Balescu, Radu (1975), Equilibrium and Nonequilibrium Statistical Mechanics, John Wiley & Sons, pp. 674–675, ISBN 978-0-471-04600-4
Incropera, Frank P.; DeWitt, David P. (1996), Fundamentals of heat and mass transfer (4th ed.), Wiley, p. 47, ISBN 0-471-30460-3
Chapman, Sydney; Cowling, T.G. (1970), The Mathematical Theory of Non-Uniform Gases (3rd ed.), Cambridge University Press, pp. 100–101
Bird, Stewart & Lightfoot 2006, p. 275.
Chapman & Cowling, p. 167
Chapman & Cowling, p. 247
Chapman & Cowling, pp. 249-251
Bird, Stewart & Lightfoot 2006, p. 276.
Bird, Stewart & Lightfoot 2006, p. 279.
Klemens, P.G. (1951). "The Thermal Conductivity of Dielectric Solids at Low Temperatures". Proceedings of the Royal Society of London A. 208 (1092): 108. Bibcode:1951RSPSA.208..108K. doi:10.1098/rspa.1951.0147. S2CID 136951686.
Chang, G. K.; Jones, R. E. (1962). "Low-Temperature Thermal Conductivity of Amorphous Solids". Physical Review. 126 (6): 2055. Bibcode:1962PhRv..126.2055C. doi:10.1103/PhysRev.126.2055.
Crawford, Frank S. (1968). Berkeley Physics Course: Vol. 3: Waves. McGraw-Hill. p. 215. ISBN 9780070048607.
Pomeranchuk, I. (1941). "Thermal conductivity of the paramagnetic dielectrics at low temperatures". Journal of Physics USSR. 4: 357. ISSN 0368-3400.
Zeller, R. C.; Pohl, R. O. (1971). "Thermal Conductivity and Specific Heat of Non-crystalline Solids". Physical Review B. 4 (6): 2029. Bibcode:1971PhRvB...4.2029Z. doi:10.1103/PhysRevB.4.2029.
Love, W. F. (1973). "Low-Temperature Thermal Brillouin Scattering in Fused Silica and Borosilicate Glass". Physical Review Letters. 31 (13): 822. Bibcode:1973PhRvL..31..822L. doi:10.1103/PhysRevLett.31.822.
Zaitlin, M. P.; Anderson, M. C. (1975). "Phonon thermal transport in noncrystalline materials". Physical Review B. 12 (10): 4475. Bibcode:1975PhRvB..12.4475Z. doi:10.1103/PhysRevB.12.4475.
Zaitlin, M. P.; Scherr, L. M.; Anderson, M. C. (1975). "Boundary scattering of phonons in noncrystalline materials". Physical Review B. 12 (10): 4487. Bibcode:1975PhRvB..12.4487Z. doi:10.1103/PhysRevB.12.4487.
Pichanusakorn, P.; Bandaru, P. (2010). "Nanostructured thermoelectrics". Materials Science and Engineering: R: Reports. 67 (2–4): 19–63. doi:10.1016/j.mser.2009.10.001. S2CID 46456426.
Roufosse, Micheline; Klemens, P. G. (1973-06-15). "Thermal Conductivity of Complex Dielectric Crystals". Physical Review B. 7 (12): 5379–5386. Bibcode:1973PhRvB...7.5379R. doi:10.1103/PhysRevB.7.5379.
Ibach, H.; Luth, H. (2009). Solid-State Physics: An Introduction to Principles of Materials Science. Springer. ISBN 978-3-540-93803-3.
Puligheddu, Marcello; Galli, Giulia (2020-05-11). "Atomistic simulations of the thermal conductivity of liquids". Physical Review Materials. American Physical Society (APS). 4 (5): 053801. Bibcode:2020PhRvM...4e3801P. doi:10.1103/physrevmaterials.4.053801. ISSN 2475-9953. OSTI 1631591. S2CID 219408529.
Sharipov, Felix; Benites, Victor J. (2020-07-01). "Transport coefficients of multi-component mixtures of noble gases based on ab initio potentials: Viscosity and thermal conductivity". Physics of Fluids. AIP Publishing. 32 (7): 077104. arXiv:2006.08687. Bibcode:2020PhFl...32g7104S. doi:10.1063/5.0016261. ISSN 1070-6631. S2CID 219708359.
Huber, M. L.; Sykioti, E. A.; Assael, M. J.; Perkins, R. A. (2016). "Reference Correlation of the Thermal Conductivity of Carbon Dioxide from the Triple Point to 1100 K and up to 200 MPa". Journal of Physical and Chemical Reference Data. AIP Publishing. 45 (1): 013102. Bibcode:2016JPCRD..45a3102H. doi:10.1063/1.4940892. ISSN 0047-2689. PMC 4824315. PMID 27064300.
Monogenidou, S. A.; Assael, M. J.; Huber, M. L. (2018). "Reference Correlation for the Thermal Conductivity of Ammonia from the Triple-Point Temperature to 680 K and Pressures up to 80 MPa". Journal of Physical and Chemical Reference Data. AIP Publishing. 47 (4): 043101. Bibcode:2018JPCRD..47d3101M. doi:10.1063/1.5053087. ISSN 0047-2689. S2CID 105753612.
Assael, M. J.; Mihailidou, E. K.; Huber, M. L.; Perkins, R. A. (2012). "Reference Correlation of the Thermal Conductivity of Benzene from the Triple Point to 725 K and up to 500 MPa". Journal of Physical and Chemical Reference Data. AIP Publishing. 41 (4): 043102. Bibcode:2012JPCRD..41d3102A. doi:10.1063/1.4755781. ISSN 0047-2689.
"NIST Reference Fluid Thermodynamic and Transport Properties Database (REFPROP): Version 10". NIST. 2018-01-01. Retrieved 2021-12-23.
Bell, Ian H.; Wronski, Jorrit; Quoilin, Sylvain; Lemort, Vincent (2014-01-27). "Pure and Pseudo-pure Fluid Thermophysical Property Evaluation and the Open-Source Thermophysical Property Library CoolProp". Industrial & Engineering Chemistry Research. American Chemical Society (ACS). 53 (6): 2498–2508. doi:10.1021/ie4033999. ISSN 0888-5885. PMC 3944605. PMID 24623957.
Evans, Denis J.; Morriss, Gary P. (2007). Statistical Mechanics of Nonequilibrium Liquids. ANU Press. ISBN 9781921313226. JSTOR j.ctt24h99q.

    Maginn, Edward J.; Messerly, Richard A.; Carlson, Daniel J.; Roe, Daniel R.; Elliott, J. Richard (2019). "Best Practices for Computing Transport Properties 1. Self-Diffusivity and Viscosity from Equilibrium Molecular Dynamics [Article v1.0]". Living Journal of Computational Molecular Science. University of Colorado at Boulder. 1 (1). doi:10.33011/livecoms.1.1.6324. ISSN 2575-6524. S2CID 104357320.

Sources

    Bird, R.B.; Stewart, W.E.; Lightfoot, E.N. (2006). Transport Phenomena. Transport Phenomena. Vol. 1. Wiley. ISBN 978-0-470-11539-8.

Further reading
Undergraduate-level texts (engineering)

    Bird, R. Byron; Stewart, Warren E.; Lightfoot, Edwin N. (2007), Transport Phenomena (2nd ed.), John Wiley & Sons, Inc., ISBN 978-0-470-11539-8. A standard, modern reference.
    Incropera, Frank P.; DeWitt, David P. (1996), Fundamentals of heat and mass transfer (4th ed.), Wiley, ISBN 0-471-30460-3
    Bejan, Adrian (1993), Heat Transfer, John Wiley & Sons, ISBN 0-471-50290-1
    Holman, J.P. (1997), Heat Transfer (8th ed.), McGraw Hill, ISBN 0-07-844785-2
    Callister, William D. (2003), "Appendix B", Materials Science and Engineering - An Introduction, John Wiley & Sons, ISBN 0-471-22471-5

Undergraduate-level texts (physics)

    Halliday, David; Resnick, Robert; & Walker, Jearl (1997). Fundamentals of Physics (5th ed.). John Wiley and Sons, New York ISBN 0-471-10558-9. An elementary treatment.
    Daniel V. Schroeder (1999), An Introduction to Thermal Physics, Addison Wesley, ISBN 978-0-201-38027-9. A brief, intermediate-level treatment.
    Reif, F. (1965), Fundamentals of Statistical and Thermal Physics, McGraw-Hill. An advanced treatment.

Graduate-level texts

    Balescu, Radu (1975), Equilibrium and Nonequilibrium Statistical Mechanics, John Wiley & Sons, ISBN 978-0-471-04600-4
    Chapman, Sydney; Cowling, T.G. (1970), The Mathematical Theory of Non-Uniform Gases (3rd ed.), Cambridge University Press. A very advanced but classic text on the theory of transport processes in gases.
    Reid, C. R., Prausnitz, J. M., Poling B. E., Properties of gases and liquids, IV edition, Mc Graw-Hill, 1987
    Srivastava G. P (1990), The Physics of Phonons. Adam Hilger, IOP Publishing Ltd, Bristol

External links

    Thermopedia THERMAL CONDUCTIVITY
    Contribution of Interionic Forces to the Thermal Conductivity of Dilute Electrolyte Solutions The Journal of Chemical Physics 41, 3924 (1964)
    The importance of Soil Thermal Conductivity for power companies
    Thermal Conductivity of Gas Mixtures in Chemical Equilibrium. II The Journal of Chemical Physics 32, 1005 (1960)

Authority control: National Edit this at Wikidata	

    Germany Israel United States Czech Republic

Categories:

    Heat conductionHeat transferPhysical quantitiesThermodynamic properties

    This page was last edited on 20 August 2023, at 10:12 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki



Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
History

Informal definition

Formalization

Expressing algorithms

Design

Computer algorithms

Examples

Algorithmic analysis

Classification

Algorithm = Logic + Control

Legal issues

History: Development of the notion of "algorithm"

    See also
    Notes
    Bibliography
    Further reading
    External links

Algorithm

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
"Algorithms" redirects here. For the subfield of computer science, see Analysis of algorithms. For other uses, see Algorithm (disambiguation).
	
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Algorithm" – news · newspapers · books · scholar · JSTOR (July 2022) (Learn how and when to remove this template message)
Flow-chart of an algorithm Euclides algorithm's) for calculating the greatest common divisor (g.c.d.) of two numbers a and b in locations named A and B. The algorithm proceeds by successive subtractions in two loops: IF the test B ≥ A yields "yes" or "true" (more accurately, the number b in location B is greater than or equal to the number a in location A) THEN, the algorithm specifies B ← B − A (meaning the number b − a replaces the old b). Similarly, IF A > B, THEN A ← A − B. The process terminates when (the contents of) B is 0, yielding the g.c.d. in A. (Algorithm derived from Scott 2009:13; symbols and drawing style from Tausworthe 1977).
Ada Lovelace's diagram from "note G", the first published computer algorithm

In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ (listen)) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation.[1] Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as "memory", "search" and "stimulus".[2]

In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.[3]

As an effective method, an algorithm can be expressed within a finite amount of space and time,[4] and in a well-defined formal language[5] for calculating a function.[6] Starting from an initial state and initial input (perhaps empty),[7] the instructions describe a computation that, when executed, proceeds through a finite[8] number of well-defined successive states, eventually producing "output"[9] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[10]
History
Ancient algorithms

Since antiquity, step-by-step procedures for solving mathematical problems have been attested. This includes Babylonian mathematics (around 2500 BC),[11] Egyptian mathematics (around 1550 BC),[11] Indian mathematics (around 800 BC and later; e.g. Shulba Sutras, Kerala School, and Brāhmasphuṭasiddhānta),[12][13] The Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC, e.g. sieve of Eratosthenes and Euclidean algorithm),[14] and Arabic mathematics (9th century, e.g. cryptographic algorithms for code-breaking based on frequency analysis).[15]
Al-khwarizmi and the term algorithm

Around 825, Muhammad ibn Musa al-Khwarizmi wrote kitāb al-ḥisāb al-hindī ("Book of Indian computation") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī ("Addition and subtraction in Indian arithmetic"). Both of these texts are lost in the original Arabic at this time. (However, his other book on algebra remains.)[16]

In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared: Liber Alghoarismi de practica arismetrice (attributed to John of Seville) and Liber Algorismi de numero Indorum (attributed to Adelard of Bath).[17] Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi ("Thus spoke Al-Khwarizmi").[18]

In 1240, Alexander of Villedieu writes a Latin text titled Carmen de Algorismo. It begins with:

    Haec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.

which translates to:

    Algorism is the art by which at present we use those Indian figures, which number two times five.

The poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.[19]
English evolution of the word

Around 1230, the English word algorism is attested and then by Chaucer in 1391. English adopted the French term.[20][21]

In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, "number"; cf. "arithmetic"), the Latin word was altered to algorithmus.

In 1656, in the English dictionary Glossographia, it says:[22]

    Algorism ([Latin] algorismus) the Art or use of Cyphers, or of numbering by Cyphers; skill in accounting.

    Augrime ([Latin] algorithmus) skil in accounting or numbring.

In 1658, in the first edition of The New World of English Words, it says:[23]

    Algorithme, (a word compounded of Arabick and Spanish,) the art of reckoning by Cyphers.

In 1706, in the sixth edition of The New World of English Words, it says:[24]

    Algorithm, the Art of computing or reckoning by numbers, which contains the five principle Rules of Arithmetick, viz. Numeration, Addition, Subtraction, Multiplication and Division; to which may be added Extraction of Roots: It is also call'd Logistica Numeralis.

    Algorism, the practical Operation in the several Parts of Specious Arithmetick or Algebra; sometimes it is taken for the Practice of Common Arithmetick by the ten Numeral Figures.

In 1751, in the Young Algebraist's Companion, Daniel Fenning contrasts the terms algorism and algorithm as follows:[25]

    Algorithm signifies the first Principles, and Algorism the practical Part, or knowing how to put the Algorithm in Practice.

Since at least 1811, the term algorithm is attested to mean a "step-by-step procedure" in English.[26][27]

In 1842, in the Dictionary of Science, Literature and Art, it says:

    ALGORITHM, signifies the art of computing in reference to some particular subject, or in some particular way; as the algorithm of numbers; the algorithm of the differential calculus.[28]

Machine usage

In 1928, a partial formalization of the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define "effective calculability"[29] or "effective method".[30] Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.
Informal definition
For a detailed presentation of the various points of view on the definition of "algorithm", see Algorithm characterizations.

An informal definition could be "a set of rules that precisely defines a sequence of operations",[31][need quotation to verify] which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure[32] or cook-book recipe.[33]

In general, a program is only an algorithm if it stops eventually[34]—even though infinite loops may sometimes prove desirable.

A prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.

Boolos, Jeffrey & 1974, 1999 offer an informal meaning of the word "algorithm" in the following quotation:

    No human being can write fast enough, or long enough, or small enough† ( †"smaller and smaller without limit ... you'd be trying to write on molecules, on atoms, on electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.[35]

An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary "input variables" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):

    Precise instructions (in a language understood by "the computer")[36] for a fast, efficient, "good"[37] process that specifies the "moves" of "the computer" (machine or human, equipped with the necessary internally contained information and capabilities)[38] to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = ... and "effectively"[39] produce, in a "reasonable" time,[40] output-integer y at a specified place and in a specified format.

The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.

Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.
Formalization

Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987), and Gurevich (2000):

    Minsky: "But we will also maintain, with Turing ... that any procedure which could "naturally" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute".[41] Gurevich: "… Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine … according to Savage [1987], an algorithm is a computational process defined by a Turing machine".[42]

Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.

Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.

For some of these computational processes, the algorithm must be rigorously defined: and specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).

Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom"—an idea that is described more formally by flow of control.

So far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of "memory" as a scratchpad. An example of such an assignment can be found below.

For some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.
Expressing algorithms

Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.

There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see Turing machine for more).

Representations of algorithms can be classed into three accepted levels of Turing machine description, as follows:[43]

1 High-level description
    "...prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head."
2 Implementation description
    "...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function."
3 Formal description
    Most detailed, "lowest level", gives the Turing machine's "state table".

For an example of the simple algorithm "Add m+n" described in all three levels, see Examples.
Design
See also: Algorithm § By design paradigm

Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns,[44] with examples including the template method pattern and the decorator pattern.

One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.

Typical steps in the development of algorithms:

    Problem definition
    Development of a model
    Specification of the algorithm
    Designing an algorithm
    Checking the correctness of the algorithm
    Analysis of algorithm
    Implementation of algorithm
    Program testing
    Documentation preparation[clarification needed]

Computer algorithms
Logical NAND algorithm implemented electronically in 7400 chip
Flowchart examples of the canonical Böhm-Jacopini structures: the SEQUENCE (rectangles descending the page), the WHILE-DO and the IF-THEN-ELSE. The three structures are made of the primitive conditional GOTO (IF test THEN GOTO step xxx, shown as diamond), the unconditional GOTO (rectangle), various assignment operators (rectangle), and HALT (rectangle). Nesting of these structures inside assignment-blocks results in complex diagrams (cf. Tausworthe 1977:100, 114).

"Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:

    Knuth: " ... we want good algorithms in some loosely defined aesthetic sense. One criterion ... is the length of time taken to perform the algorithm .... Other criteria are adaptability of the algorithm to computers, its simplicity, and elegance, etc."[45]

    Chaitin: " ... a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does"[46]

Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant'"—such a proof would solve the Halting problem (ibid).

Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".[47]

Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.

Computers (and computors), models of computation: A computer (or human "computer"[48]) is a restricted type of machine, a "discrete deterministic mechanical device"[49] that blindly follows its instructions.[50] Melzak's and Lambek's primitive models[51] reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters[52] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[53]

Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability".[54] Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution)[55] operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1).[56] Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT. However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is convenient; it can be constructed by initializing a dedicated location to zero e.g. the instruction " Z ← 0 "; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.

Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example".[57] But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computer must know how to take a square root. If they do not, then the algorithm, to be effective, must provide a set of rules for extracting a square root.[58]

This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).

But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, the arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters".[59] When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").

Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language".[60] Tausworthe augments the three Böhm-Jacopini canonical structures:[61] SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE.[62] An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.[63]

Canonical flowchart symbols[64]: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are shown in the diagram.
Examples
Further information: List of algorithms
Algorithm example

One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:

High-level description:

    If there are no numbers in the set, then there is no highest number.
    Assume the first number in the set is the largest number in the set.
    For each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.
    When there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.

(Quasi-)formal description: Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:

Algorithm LargestNumber
Input: A list of numbers L.
Output: The largest number in the list L.

if L.size = 0 return null
largest ← L[0]
for each item in L, do
    if item > largest, then
        largest ← item
return largest

    "←" denotes assignment. For instance, "largest ← item" means that the value of largest changes to the value of item.
    "return" terminates the algorithm and outputs the following value.

Euclid's algorithm
Further information: Euclid's algorithm

In mathematics, the Euclidean algorithm or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (c. 300 BC).[65] It is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.
The example-diagram of Euclid's algorithm from T.L. Heath (1908), with more detail added. Euclid does not go beyond a third measuring and gives no numerical examples. Nicomachus gives the example of 49 and 21: "I subtract the less from the greater; 28 is left; then again I subtract from this the same 21 (for this is possible); 7 is left; I subtract this from 21, 14 is left; from which I again subtract 7 (for this is possible); 7 is left, but 7 cannot be subtracted from 7." Heath comments that "The last phrase is curious, but the meaning of it is obvious enough, as also the meaning of the phrase about ending 'at one and the same number'."(Heath 1908:300).

Euclid poses the problem thus: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed of units": a counting number, a positive integer not including zero. To "measure" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s.[66] In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the "modulus", the integer-fractional part left over after the division.[67]

For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be "proper"; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).

Euclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest.[68] While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.
A graphical expression of Euclid's algorithm to find the greatest common divisor for 1599 and 650

 1599 = 650×2 + 299
 650 = 299×2 + 52
 299 = 52×5 + 39
 52 = 39×1 + 13
 39 = 13×3 + 0

Computer language for Euclid's algorithm

Only a few instruction types are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.

    A location is symbolized by upper case letter(s), e.g. S, A, etc.
    The varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might contain the number l = 3009.

An inelegant program for Euclid's algorithm
"Inelegant" is a translation of Knuth's version of the algorithm with a subtraction-based remainder-loop replacing his use of division (or a "modulus" instruction). Derived from Knuth 1973:2–4. Depending on the two numbers "Inelegant" may compute the g.c.d. in fewer steps than "Elegant".

The following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:

INPUT:

1 [Into two locations L and S put the numbers l and s that represent the two lengths]:
INPUT L, S
2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:
R ← L

E0: [Ensure r ≥ s.]

3 [Ensure the smaller of the two numbers is in S and the larger in R]:
IF R > S THEN
the contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:
GOTO step 7
ELSE
swap the contents of R and S.
4 L ← R (this first step is redundant, but is useful for later discussion).
5 R ← S
6 S ← L

E1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.

7 IF S > R THEN
done measuring so
GOTO 10
ELSE
measure again,
8 R ← R − S
9 [Remainder-loop]:
GOTO 7.

E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.

10 IF R = 0 THEN
done so
GOTO step 15
ELSE
CONTINUE TO step 11,

E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.

11 L ← R
12 R ← S
13 S ← L
14 [Repeat the measuring process]:
GOTO 7

OUTPUT:

15 [Done. S contains the greatest common divisor]:
PRINT S

DONE:

16 HALT, END, STOP.

An elegant program for Euclid's algorithm

The following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by "Inelegant"; worse, "Inelegant" requires more types of instructions.[clarify] The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.

  5 REM Euclid's algorithm for greatest common divisor
  6 PRINT "Type two integers greater than 0"
  10 INPUT A,B
  20 IF B=0 THEN GOTO 80
  30 IF A > B THEN GOTO 60
  40 LET B=B-A
  50 GOTO 20
  60 LET A=A-B
  70 GOTO 20
  80 PRINT A
  90 END

How "Elegant" works: In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co-loops", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S (Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the "sense" of the subtraction reverses.

The following version can be used with programming languages from the C-family:

// Euclid's algorithm for greatest common divisor
int euclidAlgorithm (int A, int B) {
     A = abs(A);
     B = abs(B);
     while (B != 0) {
          while (A > B) {
               A = A-B;
          }
          B = B-A;
     }
     return A;
}

Testing the Euclid algorithms

Does an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source[69] uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.

But "exceptional cases"[70] must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).

Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".[71] Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.[72]
Measuring and improving the Euclid algorithms

Elegance (compactness) versus goodness (speed): With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis[73] indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.

Can the algorithms be improved?: Once the programmer judges a program "fit" and "effective"—that is, it computes the function intended by its author—then the question becomes, can it be improved?

The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm;[74] rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.

The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.
Algorithmic analysis
Main article: Analysis of algorithms

It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation. At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.

Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.
Formal versus empirical
Main articles: Empirical algorithmics, Profiling (computer programming), and Program optimization

The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.

Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization. Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.[75]
Execution efficiency
Main article: Algorithmic efficiency

To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[76] In general, speed improvements depend on special properties of the problem, which are very common in practical applications.[77] Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.
Classification

There are various ways to classify algorithms, each with its own merits.
By implementation

One way to classify algorithms is by implementation means.

int gcd(int A, int B) {
    if (B == 0)
        return A;
    else if (A > B)
        return gcd(A-B,B);
    else
        return gcd(A,B-A);
}

Recursive C implementation of Euclid's algorithm from the above flowchart

Recursion
    A recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.
Serial, parallel or distributed
    Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms are algorithms that take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms are algorithms that use multiple machines connected with a computer network. Parallel and distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. For example, a CPU would be an example of a parallel algorithm. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.
Deterministic or non-deterministic
    Deterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.
Exact or approximate
    While many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.[78]
Quantum algorithm
    They run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.

By design paradigm

Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:

Brute-force or exhaustive search
    Brute force is a method of problem-solving that involves systematically trying every possible option until the optimal solution is found. This approach can be very time consuming, as it requires going through every possible combination of variables. However, it is often used when other methods are not available or too complex. Brute force can be used to solve a variety of problems, including finding the shortest path between two points and cracking passwords.
Divide and conquer
    A divide-and-conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.
Search and enumeration
    Many problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.
Randomized algorithm
    Such algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness.[79] Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:

    Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.
    Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.

Reduction of complexity
    This technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.
Back tracking
    In this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.

Optimization problems

For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:

Linear programming
    When searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm.[80] Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.
Dynamic programming
    When a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.
The greedy method
    A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.
The heuristic method
    In optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.

By field of study
	
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2023) (Learn how and when to remove this template message)
See also: List of algorithms

Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.

Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.
By complexity
	
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2023) (Learn how and when to remove this template message)
See also: Complexity class and Parameterized complexity

Algorithms can be classified by the amount of time they need to complete compared to their input size:

    Constant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.
    Logarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.
    Linear time: if the time is proportional to the input size. E.g. the traverse of a list.
    Polynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.
    Exponential time: if the time is an exponential function of the input size. E.g. Brute-force search.

Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.
Continuous algorithms

The adjective "continuous" when applied to the word "algorithm" can mean:

    An algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis; or
    An algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.[81]

Algorithm = Logic + Control

In logic programming, an algorithm is viewed as having both "a logic component, which specifies the knowledge to be used in solving problems, and a control component, which determines the problem-solving strategies by means of which that knowledge is used."[82]

The Euclidean algorithm illustrates this view of an algorithm. [83][84] Here, in typical logic programming style, the function gcd(A, B) = C is represented as a relation gcd(A, B, C):

gcd(A, A, A) :- A > 0.

gcd(A, B, C) :- A > B, gcd(A-B, B, C).

gcd(A, B, C) :- B > A, gcd(A, B-A, C).

These sentences have a purely logical (and "declarative") reading, as a recursive (or inductive) definition, which is independent of how the logic is used to solve problems:

    The gcd of A and A is A.
    The gcd of A and B is C, if A > B, and the gcd of A-B and B is C.
    The gcd of A and B is C, if B > A, and the gcd of A and B-A is C.

Different problem-solving strategies turn the logic into different algorithms. In particular, backward reasoning using SLD resolution, turns the logic into the following version of the Euclidean algorithm:

To find the gcd C of two given numbers A and B:

If A = B, then C = A.

If A is greater than B, then find the gcd of A-B and B, which is C.

If B is greater than A, then find the gcd of A and B-A, which is C.

However, to implement the algorithm in the logic programming language Prolog, the embedded subtractions, A-B and B-A, have to be extracted and written as separate conditions:

gcd(A, A, A) :- A > 0.

gcd(A, B, C) :- A > B, A' is A - B, gcd(A', B, C).

gcd(A, B, C) :- B > A, B' is B - A, gcd(A, B, C).

Other problem solving strategies can also be used for the same logic. In theory, given a pair of integers A and B, forward reasoning could be used to generate all instances of the gcd relation, terminating when the desired gcd of A and B is generated. Of course, forward reasoning is entirely useless in this example. But in other cases, as in the definition of the Fibonacci sequence[82] and as in Datalog, forward reasoning can be an efficient problem solving strategy.

One of the advantages of the logic programming representation of the algorithm is that its purely logical reading makes it easier to verify that the algorithm is correct relative to the standard non-recursive definition of gcd.[83] Here is the standard definition written in Prolog:

gcd(A, B, C) :-
    divides(C, A),
    divides(C, B),
    forall((divides(D, A), divides(D, B)), D =< C).
           
 divides(C, Number) :- 
           between(1, Number, C),
           0 is Number mod C.

This definition, which is the specification of the Euclidean algorithm, is also executable in Prolog: Backward reasoning treats the specification as the brute-force algorithm that iterates through all of the integers C between 1 and A, checking whether C divides both A and B, and then for each such C iterates again through all of the integers D between 1 and A, until it finds a C such that C is greater than or equal to all of the D that also divide both A and B. Although this algorithm is hopelessly inefficient, it shows that formal specifications can often be written in logic programming form, and they can be executed by Prolog, to check that they correctly represent informal requirements.
Legal issues
	
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2023) (Learn how and when to remove this template message)
See also: Software patent

Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.

Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).
History: Development of the notion of "algorithm"
Ancient Near East

The earliest evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC described the earliest division algorithm.[11] During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas.[85] Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.[86]

Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC.[11] Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,[87][14]: Ch 9.2  and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).[14]: Ch 9.1 
Discrete and distinguishable symbols

Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.
Manipulation of symbols as "place holders" for numbers: algebra

Muhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms "algorism" and "algorithm" are derived from the name al-Khwārizmī, while the term "algebra" is derived from the book Al-jabr. In Europe, the word "algorithm" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques.[88] This eventually culminated in Leibniz's notion of the calculus ratiocinator (c. 1680):

    A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.[89]

Cryptographic algorithms

The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.[15]
Mechanical contrivances with discrete states

The clock: Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular, the verge escapement[90] that provides us with the tick and tock of a mechanical clock. "The accurate automatic machine"[91] led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century.[92] Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.

Logical machines 1870 – Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc.] ...". With this machine he could analyze a "syllogism or any other simple logical argument".[93]

This machine he displayed in 1870 before the Fellows of the Royal Society.[94] Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".[95]

Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers.[96] By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a common sound. By the late 19th century the ticker tape (c. 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape.

Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".[97]

The mathematician Martin Davis observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):

    It was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned."[98]

Mathematics during the 19th century up to the mid-20th century

Symbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".[99]

But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a "'formula language', that is a lingua characterica, a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules".[100] The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).

The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox.[101] The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.

Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus[102] a finely honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene.[103] Church's proof[104] that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction.[105] Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine"[106]—in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine".[107] Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I",[108] and a few years later Kleene's renaming his Thesis "Church's Thesis"[109] and proposing "Turing's Thesis".[110]
Emil Post (1936) and Alan Turing (1936–37, 1939)

Emil Post (1936) described the actions of a "computer" (human being) as follows:

    "...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.

His symbol space would be

    "a two-way infinite sequence of spaces or boxes ... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time. ... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.

    "One box is to be singled out and called the starting point. ... a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes...

    "A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]".[111] See more at Post–Turing machine

Alan Turing's statue at Bletchley Park

Alan Turing's work[112] preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'".[113] Given the prevalence at the time of Morse code, telegraphy, ticker tape machines, and teletypewriters, it is quite possible that all were influences on Turing during his youth.

Turing—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.[114]

    "Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite...

    "The behavior of the computer at any moment is determined by the symbols which he is observing, and his "state of mind" at that moment. We may suppose that there is a bound B to the number of symbols or squares that the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...

    "Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided."[115]

Turing's reduction yields the following:

    "The simple operations must therefore include:

        "(a) Changes of the symbol on one of the observed squares
        "(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.

"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:

        "(A) A possible change (a) of symbol together with a possible change of state of mind.
        "(B) A possible change (b) of observed squares, together with a possible change of state of mind"

    "We may now construct a machine to do the work of this computer."[115]

A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:

    "A function is said to be "effectively calculable" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to Gödel, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability † with effective calculability...

        "† We shall use the expression "computable function" to mean a function calculable by a machine, and we let "effectively calculable" refer to the intuitive idea without particular identification with any one of these definitions".[116]

J. B. Rosser (1939) and S. C. Kleene (1943)

J. Barkley Rosser defined an "effective [mathematical] method" in the following manner (italicization added):

    "'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one." (Rosser 1939:225–226)

Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular, Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion, in particular, Gödel's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.

Stephen C. Kleene defined as his now-famous "Thesis I" known as the Church–Turing thesis. But he did this in the following context (boldface in original):

    "12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, "yes" or "no," to the question, "is the predicate value true?"" (Kleene 1943:273)

History after 1950

A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.
See also

    iconMathematics portal

    Abstract machine
    ALGOL
    Algorithm engineering
    Algorithm characterizations
    Algorithmic bias
    Algorithmic composition
    Algorithmic entities
    Algorithmic synthesis
    Algorithmic technique
    Algorithmic topology
    Garbage in, garbage out
    Introduction to Algorithms (textbook)
    Government by algorithm
    List of algorithms
    List of algorithm general topics
    Regulation of algorithms
    Theory of computation
        Computability theory
        Computational complexity theory
    Computational mathematics

Notes

"Definition of ALGORITHM". Merriam-Webster Online Dictionary. Archived from the original on February 14, 2020. Retrieved November 14, 2019.
Blair, Ann, Duguid, Paul, Goeing, Anja-Silvia and Grafton, Anthony. Information: A Historical Companion, Princeton: Princeton University Press, 2021. p. 247
David A. Grossman, Ophir Frieder, Information Retrieval: Algorithms and Heuristics, 2nd edition, 2004, ISBN 1402030045
"Any classical mathematical algorithm, for example, can be described in a finite number of English words" (Rogers 1987:2).
Well defined with respect to the agent that executes the algorithm: "There is a computing agent, usually human, which can react to the instructions and carry out the computations" (Rogers 1987:2).
"an algorithm is a procedure for computing a function (with respect to some chosen notation for integers) ... this limitation (to numerical functions) results in no loss of generality", (Rogers 1987:1).
"An algorithm has zero or more inputs, i.e., quantities which are given to it initially before the algorithm begins" (Knuth 1973:5).
"A procedure which has all the characteristics of an algorithm except that it possibly lacks finiteness may be called a 'computational method'" (Knuth 1973:5).
"An algorithm has one or more outputs, i.e. quantities which have a specified relation to the inputs" (Knuth 1973:5).
Whether or not a process with random interior processes (not including the input) is an algorithm is debatable. Rogers opines that: "a computation is carried out in a discrete stepwise fashion, without the use of continuous methods or analogue devices ... carried forward deterministically, without resort to random methods or devices, e.g., dice" (Rogers 1987:2).
Chabert, Jean-Luc (2012). A History of Algorithms: From the Pebble to the Microchip. Springer Science & Business Media. pp. 7–8. ISBN 9783642181924.
Sriram, M. S. (2005). "Algorithms in Indian Mathematics". In Emch, Gerard G.; Sridharan, R.; Srinivas, M. D. (eds.). Contributions to the History of Indian Mathematics. Springer. p. 153. ISBN 978-93-86279-25-5.
Hayashi, T. (2023, January 1). Brahmagupta. Encyclopedia Britannica. https://www.britannica.com/biography/Brahmagupta
Cooke, Roger L. (2005). The History of Mathematics: A Brief Course. John Wiley & Sons. ISBN 978-1-118-46029-0.
Dooley, John F. (2013). A Brief History of Cryptology and Cryptographic Algorithms. Springer Science & Business Media. pp. 12–3. ISBN 9783319016283.
Burnett, Charles (2017). "Arabic Numerals". In Thomas F. Glick (ed.). Routledge Revivals: Medieval Science, Technology and Medicine (2006): An Encyclopedia. Taylor & Francis. p. 39. ISBN 978-1-351-67617-5. Archived from the original on March 28, 2023. Retrieved May 5, 2019.
"algorism". Oxford English Dictionary (Online ed.). Oxford University Press. (Subscription or participating institution membership required.)
Brezina, Corona (2006). Al-Khwarizmi: The Inventor Of Algebra. The Rosen Publishing Group. ISBN 978-1-4042-0513-0.
"Abu Jafar Muhammad ibn Musa al-Khwarizmi". members.peak.org. Archived from the original on August 21, 2019. Retrieved November 14, 2019.
Mehri, Bahman (2017). "From Al-Khwarizmi to Algorithm". Olympiads in Informatics. 11 (2): 71–74. doi:10.15388/ioi.2017.special.11.
"algorismic". The Free Dictionary. Archived from the original on December 21, 2019. Retrieved November 14, 2019.
Blount, Thomas (1656). Glossographia or a Dictionary... London: Humphrey Moseley and George Sawbridge.
Phillips, Edward (1658). The new world of English words, or, A general dictionary containing the interpretations of such hard words as are derived from other languages...
Phillips, Edward; Kersey, John (1706). The new world of words: or, Universal English dictionary. Containing an account of the original or proper sense, and various significations of all hard words derived from other languages ... Together with a brief and plain explication of all terms relating to any of the arts and sciences ... to which is added, the interpretation of proper names. Printed for J. Phillips etc.
Fenning, Daniel (1751). The young algebraist's companion, or, A new & easy guide to algebra; introduced by the doctrine of vulgar fractions: designed for the use of schools ... illustrated with variety of numerical & literal examples ... Printed for G. Keith & J. Robinson. p. xi.
The Electric Review 1811-07: Vol 7. Open Court Publishing Co. July 1811. p. [1]. "Yet it wants a new algorithm, a compendious method by which the theorems may be established without ambiguity and circumlocution, [...]"
"algorithm". Oxford English Dictionary (Online ed.). Oxford University Press. (Subscription or participating institution membership required.)
Already 1684, in Nova Methodus pro Maximis et Minimis, Leibnitz used the Latin term "algorithmo".
Kleene 1943 in Davis 1965:274
Rosser 1939 in Davis 1965:225
Stone 1973:4
Simanowski, Roberto (2018). The Death Algorithm and Other Digital Dilemmas. Untimely Meditations. Vol. 14. Translated by Chase, Jefferson. Cambridge, Massachusetts: MIT Press. p. 147. ISBN 9780262536370. Archived from the original on December 22, 2019. Retrieved May 27, 2019. "[...] the next level of abstraction of central bureaucracy: globally operating algorithms."
Dietrich, Eric (1999). "Algorithm". In Wilson, Robert Andrew; Keil, Frank C. (eds.). The MIT Encyclopedia of the Cognitive Sciences. MIT Cognet library. Cambridge, Massachusetts: MIT Press (published 2001). p. 11. ISBN 9780262731447. Retrieved July 22, 2020. "An algorithm is a recipe, method, or technique for doing something."
Stone requires that "it must terminate in a finite number of steps" (Stone 1973:7–8).
Boolos and Jeffrey 1974,1999:19
cf Stone 1972:5
Knuth 1973:7 states: "In practice, we not only want algorithms, but we also want good algorithms ... one criterion of goodness is the length of time taken to perform the algorithm ... other criteria are the adaptability of the algorithm to computers, its simplicity, and elegance, etc."
cf Stone 1973:6
Stone 1973:7–8 states that there must be, "...a procedure that a robot [i.e., computer] can follow in order to determine precisely how to obey the instruction". Stone adds finiteness of the process, and definiteness (having no ambiguity in the instructions) to this definition.
Knuth, loc. cit
Minsky 1967, p. 105
Gurevich 2000:1, 3
Sipser 2006:157
Goodrich, Michael T.; Tamassia, Roberto (2002). Algorithm Design: Foundations, Analysis, and Internet Examples. John Wiley & Sons, Inc. ISBN 978-0-471-38365-9. Archived from the original on April 28, 2015. Retrieved June 14, 2018.
Knuth 1973:7
Chaitin 2005:32
Rogers 1987:1–2
In his essay "Calculations by Man and Machine: Conceptual Analysis" Seig 2002:390 credits this distinction to Robin Gandy, cf Wilfred Seig, et al., 2002 Reflections on the foundations of mathematics: Essays in honor of Solomon Feferman, Association for Symbolic Logic, A.K. Peters Ltd, Natick, MA.
cf Gandy 1980:126, Robin Gandy Church's Thesis and Principles for Mechanisms appearing on pp. 123–148 in J. Barwise et al. 1980 The Kleene Symposium, North-Holland Publishing Company.
A "robot": "A computer is a robot that performs any task that can be described as a sequence of instructions." cf Stone 1972:3
Lambek's "abacus" is a "countably infinite number of locations (holes, wires, etc.) together with an unlimited supply of counters (pebbles, beads, etc.). The locations are distinguishable, the counters are not". The holes have unlimited capacity, and standing by is an agent who understands and is able to carry out the list of instructions" (Lambek 1961:295). Lambek references Melzak who defines his Q-machine as "an indefinitely large number of locations ... an indefinitely large supply of counters distributed among these locations, a program, and an operator whose sole purpose is to carry out the program" (Melzak 1961:283). B-B-J (loc. cit.) add the stipulation that the holes are "capable of holding any number of stones" (p. 46). Both Melzak and Lambek appear in The Canadian Mathematical Bulletin, vol. 4, no. 3, September 1961.
If no confusion results, the word "counters" can be dropped, and a location can be said to contain a single "number".
"We say that an instruction is effective if there is a procedure that the robot can follow in order to determine precisely how to obey the instruction." (Stone 1972:6)
cf Minsky 1967: Chapter 11 "Computer models" and Chapter 14 "Very Simple Bases for Computability" pp. 255–281, in particular,
cf Knuth 1973:3.
But always preceded by IF-THEN to avoid improper subtraction.
Knuth 1973:4
Stone 1972:5. Methods for extracting roots are not trivial: see Methods of computing square roots.
Leeuwen, Jan (1990). Handbook of Theoretical Computer Science: Algorithms and complexity. Volume A. Elsevier. p. 85. ISBN 978-0-444-88071-0.
John G. Kemeny and Thomas E. Kurtz 1985 Back to Basic: The History, Corruption, and Future of the Language, Addison-Wesley Publishing Company, Inc. Reading, MA, ISBN 0-201-13433-0.
Tausworthe 1977:101
Tausworthe 1977:142
Knuth 1973 section 1.2.1, expanded by Tausworthe 1977 at pages 100ff and Chapter 9.1
cf Tausworthe 1977
Heath 1908:300; Hawking's Dover 2005 edition derives from Heath.
" 'Let CD, measuring BF, leave FA less than itself.' This is a neat abbreviation for saying, measure along BA successive lengths equal to CD until a point F is reached such that the length FA remaining is less than CD; in other words, let BF be the largest exact multiple of CD contained in BA" (Heath 1908:297)
For modern treatments using division in the algorithm, see Hardy and Wright 1979:180, Knuth 1973:2 (Volume 1), plus more discussion of Euclid's algorithm in Knuth 1969:293–297 (Volume 2).
Euclid covers this question in his Proposition 1.
"Euclid's Elements, Book VII, Proposition 2". Aleph0.clarku.edu. Archived from the original on May 24, 2012. Retrieved May 20, 2012.
While this notion is in widespread use, it cannot be defined precisely.
Knuth 1973:13–18. He credits "the formulation of algorithm-proving in terms of assertions and induction" to R W. Floyd, Peter Naur, C.A.R. Hoare, H.H. Goldstine and J. von Neumann. Tausworth 1977 borrows Knuth's Euclid example and extends Knuth's method in section 9.1 Formal Proofs (pp. 288–298).
Tausworthe 1997:294
cf Knuth 1973:7 (Vol. I), and his more-detailed analyses on pp. 1969:294–313 (Vol II).
Breakdown occurs when an algorithm tries to compact itself. Success would solve the Halting problem.
Kriegel, Hans-Peter; Schubert, Erich; Zimek, Arthur (2016). "The (black) art of run-time evaluation: Are we comparing algorithms or implementations?". Knowledge and Information Systems. 52 (2): 341–378. doi:10.1007/s10115-016-1004-2. ISSN 0219-1377. S2CID 40772241.
Gillian Conahan (January 2013). "Better Math Makes Faster Data Networks". discovermagazine.com. Archived from the original on May 13, 2014. Retrieved May 13, 2014.
Haitham Hassanieh, Piotr Indyk, Dina Katabi, and Eric Price, "ACM-SIAM Symposium On Discrete Algorithms (SODA) Archived July 4, 2013, at the Wayback Machine, Kyoto, January 2012. See also the sFFT Web Page Archived February 21, 2012, at the Wayback Machine.
Kellerer, Hans; Pferschy, Ulrich; Pisinger, David (2004). Knapsack Problems | Hans Kellerer | Springer. Springer. doi:10.1007/978-3-540-24777-7. ISBN 978-3-540-40286-2. S2CID 28836720. Archived from the original on October 18, 2017. Retrieved September 19, 2017.
For instance, the volume of a convex polytope (described using a membership oracle) can be approximated to high accuracy by a randomized polynomial time algorithm, but not by a deterministic one: see Dyer, Martin; Frieze, Alan; Kannan, Ravi (January 1991). "A Random Polynomial-time Algorithm for Approximating the Volume of Convex Bodies". J. ACM. 38 (1): 1–17. CiteSeerX 10.1.1.145.4600. doi:10.1145/102782.102783. S2CID 13268711.
George B. Dantzig and Mukund N. Thapa. 2003. Linear Programming 2: Theory and Extensions. Springer-Verlag.
Tsypkin (1971). Adaptation and learning in automatic systems. Academic Press. p. 54. ISBN 978-0-08-095582-7.
Kowalski, Robert (1979). "Algorithm=Logic+Control". Communications of the ACM. 22 (7): 424–436. doi:10.1145/359131.359136. S2CID 2509896.
Warren, D.S., 2023. Writing correct prolog programs. In Prolog: The Next 50 Years (pp. 62-70). Cham: Springer Nature Switzerland.
Kowalski, R., Dávila, J., Sartor, G. and Calejo, M., 2023. Logical English for law and education. In Prolog: The Next 50 Years (pp. 287–299). Cham: Springer Nature Switzerland.
Knuth, Donald E. (1972). "Ancient Babylonian Algorithms" (PDF). Commun. ACM. 15 (7): 671–677. doi:10.1145/361454.361514. ISSN 0001-0782. S2CID 7829945. Archived from the original (PDF) on December 24, 2012.
Aaboe, Asger (2001). Episodes from the Early History of Astronomy. New York: Springer. pp. 40–62. ISBN 978-0-387-95136-2.
Ast, Courtney. "Eratosthenes". Wichita State University: Department of Mathematics and Statistics. Archived from the original on February 27, 2015. Retrieved February 27, 2015.
Chabert, Jean-Luc (2012). A History of Algorithms: From the Pebble to the Microchip. Springer Science & Business Media. p. 2. ISBN 9783642181924.
Davis 2000:18
Bolter 1984:24
Bolter 1984:26
Bolter 1984:33–34, 204–206.
All quotes from W. Stanley Jevons 1880 Elementary Lessons in Logic: Deductive and Inductive, Macmillan and Co., London and New York. Republished as a googlebook; cf Jevons 1880:199–201. Louis Couturat 1914 the Algebra of Logic, The Open Court Publishing Company, Chicago and London. Republished as a googlebook; cf Couturat 1914:75–76 gives a few more details; he compares this to a typewriter as well as a piano. Jevons states that the account is to be found at January 20, 1870 The Proceedings of the Royal Society.
Jevons 1880:199–200
All quotes from John Venn 1881 Symbolic Logic, Macmillan and Co., London. Republished as a googlebook. cf Venn 1881:120–125. The interested reader can find a deeper explanation in those pages.
Bell and Newell diagram 1971:39, cf. Davis 2000
* Melina Hill, Valley News Correspondent, A Tinkerer Gets a Place in History, Valley News West Lebanon NH, Thursday, March 31, 1983, p. 13.
Davis 2000:14
van Heijenoort 1967:81ff
van Heijenoort's commentary on Frege's Begriffsschrift, a formula language, modeled upon that of arithmetic, for pure thought in van Heijenoort 1967:1
Dixon 1906, cf. Kleene 1952:36–40
cf. footnote in Alonzo Church 1936a in Davis 1965:90 and 1936b in Davis 1965:110
Kleene 1935–6 in Davis 1965:237ff, Kleene 1943 in Davis 1965:255ff
Church 1936 in Davis 1965:88ff
cf. "Finite Combinatory Processes – formulation 1", Post 1936 in Davis 1965:289–290
Turing 1936–37 in Davis 1965:116ff
Rosser 1939 in Davis 1965:226
Kleene 1943 in Davis 1965:273–274
Kleene 1952:300, 317
Kleene 1952:376
Turing 1936–37 in Davis 1965:289–290
Turing 1936 in Davis 1965, Turing 1939 in Davis 1965:160
Hodges, p. 96
Turing 1936–37:116
Turing 1936–37 in Davis 1965:136

    Turing 1939 in Davis 1965:160

Bibliography

    Axt, P (1959). "On a Subrecursive Hierarchy and Primitive Recursive Degrees". Transactions of the American Mathematical Society. 92 (1): 85–105. doi:10.2307/1993169. JSTOR 1993169.
    Bell, C. Gordon and Newell, Allen (1971), Computer Structures: Readings and Examples, McGraw–Hill Book Company, New York. ISBN 0-07-004357-4.
    Blass, Andreas; Gurevich, Yuri (2003). "Algorithms: A Quest for Absolute Definitions" (PDF). Bulletin of European Association for Theoretical Computer Science. 81. Archived (PDF) from the original on October 9, 2022. Includes a bibliography of 56 references.
    Bolter, David J. (1984). Turing's Man: Western Culture in the Computer Age (1984 ed.). Chapel Hill, NC: The University of North Carolina Press. ISBN 978-0-8078-1564-9., ISBN 0-8078-4108-0
    Boolos, George; Jeffrey, Richard (1999) [1974]. Computability and Logic (4th ed.). Cambridge University Press, London. ISBN 978-0-521-20402-6.: cf. Chapter 3 Turing machines where they discuss "certain enumerable sets not effectively (mechanically) enumerable".
    Burgin, Mark (2004). Super-Recursive Algorithms. Springer. ISBN 978-0-387-95569-8.
    Campagnolo, M.L., Moore, C., and Costa, J.F. (2000) An analog characterization of the subrecursive functions. In Proc. of the 4th Conference on Real Numbers and Computers, Odense University, pp. 91–109
    Church, Alonzo (1936). "An Unsolvable Problem of Elementary Number Theory". The American Journal of Mathematics. 58 (2): 345–363. doi:10.2307/2371045. JSTOR 2371045. Reprinted in The Undecidable, p. 89ff. The first expression of "Church's Thesis". See in particular page 100 (The Undecidable) where he defines the notion of "effective calculability" in terms of "an algorithm", and he uses the word "terminates", etc.
    Church, Alonzo (1936). "A Note on the Entscheidungsproblem". The Journal of Symbolic Logic. 1 (1): 40–41. doi:10.2307/2269326. JSTOR 2269326. S2CID 42323521. Church, Alonzo (1936). "Correction to a Note on the Entscheidungsproblem". The Journal of Symbolic Logic. 1 (3): 101–102. doi:10.2307/2269030. JSTOR 2269030. S2CID 5557237. Reprinted in The Undecidable, p. 110ff. Church shows that the Entscheidungsproblem is unsolvable in about 3 pages of text and 3 pages of footnotes.
    Daffa', Ali Abdullah al- (1977). The Muslim contribution to mathematics. London: Croom Helm. ISBN 978-0-85664-464-1.
    Davis, Martin (1965). The Undecidable: Basic Papers On Undecidable Propositions, Unsolvable Problems and Computable Functions. New York: Raven Press. ISBN 978-0-486-43228-1. Davis gives commentary before each article. Papers of Gödel, Alonzo Church, Turing, Rosser, Kleene, and Emil Post are included; those cited in the article are listed here by author's name.
    Davis, Martin (2000). Engines of Logic: Mathematicians and the Origin of the Computer. New York: W.W. Nortion. ISBN 978-0-393-32229-3. Davis offers concise biographies of Leibniz, Boole, Frege, Cantor, Hilbert, Gödel and Turing with von Neumann as the show-stealing villain. Very brief bios of Joseph-Marie Jacquard, Babbage, Ada Lovelace, Claude Shannon, Howard Aiken, etc.
    Public Domain This article incorporates public domain material from Paul E. Black. "algorithm". Dictionary of Algorithms and Data Structures. NIST.
    Dean, Tim (2012). "Evolution and moral diversity". Baltic International Yearbook of Cognition, Logic and Communication. 7. doi:10.4148/biyclc.v7i0.1775.
    Dennett, Daniel (1995). Darwin's Dangerous Idea. pp. 32–36. Bibcode:1996Cmplx...2a..32M. doi:10.1002/(SICI)1099-0526(199609/10)2:1<32::AID-CPLX8>3.0.CO;2-H. ISBN 978-0-684-80290-9. {{cite book}}: |journal= ignored (help)
    Dilson, Jesse (2007). The Abacus ((1968, 1994) ed.). St. Martin's Press, NY. ISBN 978-0-312-10409-2., ISBN 0-312-10409-X
    Yuri Gurevich, Sequential Abstract State Machines Capture Sequential Algorithms, ACM Transactions on Computational Logic, Vol 1, no 1 (July 2000), pp. 77–111. Includes bibliography of 33 sources.
    van Heijenoort, Jean (2001). From Frege to Gödel, A Source Book in Mathematical Logic, 1879–1931 ((1967) ed.). Harvard University Press, Cambridge. ISBN 978-0-674-32449-7., 3rd edition 1976[?], ISBN 0-674-32449-8 (pbk.)
    Hodges, Andrew (1983). Alan Turing: The Enigma. pp. 107–108. Bibcode:1984PhT....37k.107H. doi:10.1063/1.2915935. ISBN 978-0-671-49207-6. {{cite book}}: |journal= ignored (help), ISBN 0-671-49207-1. Cf. Chapter "The Spirit of Truth" for a history leading to, and a discussion of, his proof.
    Kleene, Stephen C. (1936). "General Recursive Functions of Natural Numbers". Mathematische Annalen. 112 (5): 727–742. doi:10.1007/BF01565439. S2CID 120517999. Archived from the original on September 3, 2014. Retrieved September 30, 2013. Presented to the American Mathematical Society, September 1935. Reprinted in The Undecidable, p. 237ff. Kleene's definition of "general recursion" (known now as mu-recursion) was used by Church in his 1935 paper An Unsolvable Problem of Elementary Number Theory that proved the "decision problem" to be "undecidable" (i.e., a negative result).
    Kleene, Stephen C. (1943). "Recursive Predicates and Quantifiers". Transactions of the American Mathematical Society. 53 (1): 41–73. doi:10.2307/1990131. JSTOR 1990131. Reprinted in The Undecidable, p. 255ff. Kleene refined his definition of "general recursion" and proceeded in his chapter "12. Algorithmic theories" to posit "Thesis I" (p. 274); he would later repeat this thesis (in Kleene 1952:300) and name it "Church's Thesis"(Kleene 1952:317) (i.e., the Church thesis).
    Kleene, Stephen C. (1991) [1952]. Introduction to Metamathematics (Tenth ed.). North-Holland Publishing Company. ISBN 978-0-7204-2103-3.
    Knuth, Donald (1997). Fundamental Algorithms, Third Edition. Reading, Massachusetts: Addison–Wesley. ISBN 978-0-201-89683-1.
    Knuth, Donald (1969). Volume 2/Seminumerical Algorithms, The Art of Computer Programming First Edition. Reading, Massachusetts: Addison–Wesley.
    Kosovsky, N.K. Elements of Mathematical Logic and its Application to the theory of Subrecursive Algorithms, LSU Publ., Leningrad, 1981
    Kowalski, Robert (1979). "Algorithm=Logic+Control". Communications of the ACM. 22 (7): 424–436. doi:10.1145/359131.359136. S2CID 2509896.
    A.A. Markov (1954) Theory of algorithms. [Translated by Jacques J. Schorr-Kon and PST staff] Imprint Moscow, Academy of Sciences of the USSR, 1954 [i.e., Jerusalem, Israel Program for Scientific Translations, 1961; available from the Office of Technical Services, U.S. Dept. of Commerce, Washington] Description 444 p. 28 cm. Added t.p. in Russian Translation of Works of the Mathematical Institute, Academy of Sciences of the USSR, v. 42. Original title: Teoriya algerifmov. [QA248.M2943 Dartmouth College library. U.S. Dept. of Commerce, Office of Technical Services, number OTS 60-51085.]
    Minsky, Marvin (1967). Computation: Finite and Infinite Machines (First ed.). Prentice-Hall, Englewood Cliffs, NJ. ISBN 978-0-13-165449-5. Minsky expands his "...idea of an algorithm – an effective procedure..." in chapter 5.1 Computability, Effective Procedures and Algorithms. Infinite machines.
    Post, Emil (1936). "Finite Combinatory Processes, Formulation I". The Journal of Symbolic Logic. 1 (3): 103–105. doi:10.2307/2269031. JSTOR 2269031. S2CID 40284503. Reprinted in The Undecidable, pp. 289ff. Post defines a simple algorithmic-like process of a man writing marks or erasing marks and going from box to box and eventually halting, as he follows a list of simple instructions. This is cited by Kleene as one source of his "Thesis I", the so-called Church–Turing thesis.
    Rogers, Hartley Jr. (1987). Theory of Recursive Functions and Effective Computability. The MIT Press. ISBN 978-0-262-68052-3.
    Rosser, J.B. (1939). "An Informal Exposition of Proofs of Godel's Theorem and Church's Theorem". Journal of Symbolic Logic. 4 (2): 53–60. doi:10.2307/2269059. JSTOR 2269059. S2CID 39499392. Reprinted in The Undecidable, p. 223ff. Herein is Rosser's famous definition of "effective method": "...a method each step of which is precisely predetermined and which is certain to produce the answer in a finite number of steps... a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer" (p. 225–226, The Undecidable)
    Santos-Lang, Christopher (2014). "Moral Ecology Approaches to Machine Ethics" (PDF). In van Rysewyk, Simon; Pontier, Matthijs (eds.). Machine Medical Ethics. Intelligent Systems, Control and Automation: Science and Engineering. Vol. 74. Switzerland: Springer. pp. 111–127. doi:10.1007/978-3-319-08108-3_8. ISBN 978-3-319-08107-6. Archived (PDF) from the original on October 9, 2022.
    Scott, Michael L. (2009). Programming Language Pragmatics (3rd ed.). Morgan Kaufmann Publishers/Elsevier. ISBN 978-0-12-374514-9.
    Sipser, Michael (2006). Introduction to the Theory of Computation. PWS Publishing Company. ISBN 978-0-534-94728-6.
    Sober, Elliott; Wilson, David Sloan (1998). Unto Others: The Evolution and Psychology of Unselfish Behavior. Cambridge: Harvard University Press. ISBN 9780674930469.
    Stone, Harold S. (1972). Introduction to Computer Organization and Data Structures (1972 ed.). McGraw-Hill, New York. ISBN 978-0-07-061726-1. Cf. in particular the first chapter titled: Algorithms, Turing Machines, and Programs. His succinct informal definition: "...any sequence of instructions that can be obeyed by a robot, is called an algorithm" (p. 4).
    Tausworthe, Robert C (1977). Standardized Development of Computer Software Part 1 Methods. Englewood Cliffs NJ: Prentice–Hall, Inc. ISBN 978-0-13-842195-3.
    Turing, Alan M. (1936–37). "On Computable Numbers, With An Application to the Entscheidungsproblem". Proceedings of the London Mathematical Society. Series 2. 42: 230–265. doi:10.1112/plms/s2-42.1.230. S2CID 73712.. Corrections, ibid, vol. 43(1937) pp. 544–546. Reprinted in The Undecidable, p. 116ff. Turing's famous paper completed as a Master's dissertation while at King's College Cambridge UK.
    Turing, Alan M. (1939). "Systems of Logic Based on Ordinals". Proceedings of the London Mathematical Society. 45: 161–228. doi:10.1112/plms/s2-45.1.161. hdl:21.11116/0000-0001-91CE-3. Reprinted in The Undecidable, pp. 155ff. Turing's paper that defined "the oracle" was his PhD thesis while at Princeton.
    United States Patent and Trademark Office (2006), 2106.02 **>Mathematical Algorithms: 2100 Patentability, Manual of Patent Examining Procedure (MPEP). Latest revision August 2006

    Zaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76–99. https://doi.org/10.2307/3027363

Further reading

    Bellah, Robert Neelly (1985). Habits of the Heart: Individualism and Commitment in American Life. Berkeley: University of California Press. ISBN 978-0-520-25419-0.
    Berlinski, David (2001). The Advent of the Algorithm: The 300-Year Journey from an Idea to the Computer. Harvest Books. ISBN 978-0-15-601391-8.
    Chabert, Jean-Luc (1999). A History of Algorithms: From the Pebble to the Microchip. Springer Verlag. ISBN 978-3-540-63369-3.
    Thomas H. Cormen; Charles E. Leiserson; Ronald L. Rivest; Clifford Stein (2009). Introduction To Algorithms (3rd ed.). MIT Press. ISBN 978-0-262-03384-8.
    Harel, David; Feldman, Yishai (2004). Algorithmics: The Spirit of Computing. Addison-Wesley. ISBN 978-0-321-11784-7.
    Hertzke, Allen D.; McRorie, Chris (1998). "The Concept of Moral Ecology". In Lawler, Peter Augustine; McConkey, Dale (eds.). Community and Political Thought Today. Westport, CT: Praeger.
    Knuth, Donald E. (2000). Selected Papers on Analysis of Algorithms. Stanford, California: Center for the Study of Language and Information.
    Knuth, Donald E. (2010). Selected Papers on Design of Algorithms. Stanford, California: Center for the Study of Language and Information.
    Wallach, Wendell; Allen, Colin (November 2008). Moral Machines: Teaching Robots Right from Wrong. US: Oxford University Press. ISBN 978-0-19-537404-9.
    Bleakley, Chris (2020). Poems that Solve Puzzles: The History and Science of Algorithms. Oxford University Press. ISBN 978-0-19-885373-2.

External links
Look up algorithm in Wiktionary, the free dictionary.
Wikibooks has a book on the topic of: Algorithms
At Wikiversity, you can learn more and teach others about Algorithm at the Department of Algorithm
Wikimedia Commons has media related to Algorithms.

    "Algorithm". Encyclopedia of Mathematics. EMS Press. 2001 [1994].
    Algorithms at Curlie
    Weisstein, Eric W. "Algorithm". MathWorld.
    Dictionary of Algorithms and Data Structures – National Institute of Standards and Technology

Algorithm repositories

    The Stony Brook Algorithm Repository – State University of New York at Stony Brook
    Collected Algorithms of the ACM – Associations for Computing Machinery
    The Stanford GraphBase – Stanford University


    vte

Industrial and applied mathematics

    vte

Well-known computer science algorithms
Authority control Edit this at Wikidata
Categories:

    AlgorithmsMathematical logicTheoretical computer science

    This page was last edited on 23 August 2023, at 10:21 (UTC).
    Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki

Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Fourier's law

        Fourier's law in one dimension
        Multi-dimensional extension
    Measurement
    Science and engineering
    See also
    Notes

Heat flux

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
	
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Heat flux" – news · newspapers · books · scholar · JSTOR (March 2021) (Learn how and when to remove this template message)
Heat flux
Heat flux ϕ → q {\displaystyle {\vec {\phi }}_{\mathrm {q} }} through a surface.
Common symbols
	ϕ → q {\displaystyle {\vec {\phi }}_{\mathrm {q} }}
SI unit	W/m2
Other units
	Btu/(h⋅ft2)
In SI base units	kg⋅s−3
Dimension	M T − 3 {\displaystyle {\mathsf {M}}{\mathsf {T}}^{-3}}

In physics and engineering, heat flux or thermal flux, sometimes also referred to as heat flux density[1], heat-flow density or heat flow rate intensity, is a flow of energy per unit area per unit time. Its SI units are watts per square metre (W/m2). It has both a direction and a magnitude, and so it is a vector quantity. To define the heat flux at a certain point in space, one takes the limiting case where the size of the surface becomes infinitesimally small.

Heat flux is often denoted ϕ → q {\displaystyle {\vec {\phi }}_{\mathrm {q} }}, the subscript q specifying heat flux, as opposed to mass or momentum flux. Fourier's law is an important application of these concepts.
Fourier's law
Main article: Thermal conduction § Fourier's law

For most solids in usual conditions, heat is transported mainly by conduction and the heat flux is adequately described by Fourier's law.
Fourier's law in one dimension

ϕ q = − k d T ( x ) d x
{\displaystyle \phi _{\text{q}}=-k{\frac {\mathrm {d} T(x)}{\mathrm {d} x}}}

where k k is the thermal conductivity. The negative sign shows that heat flux moves from higher temperature regions to lower temperature regions.
Multi-dimensional extension
Diagram depicting heat flux through a thermal insulation material with thermal conductivity, k, and thickness, x. Heat flux can be determined using two surface temperature measurements on either side of the material using temperature sensors if k and x of the material are also known.
Diagram depicting heat flux through a thermal insulation material with thermal conductivity, k, and thickness, x. Heat flux can be directly measured using a single heat flux sensor located on either surface or embedded within the material. Using this method, knowing the values of k and x of the material are not required.

The multi-dimensional case is similar, the heat flux goes "down" and hence the temperature gradient has the negative sign:

ϕ → q = − k ∇ T
{\displaystyle {\vec {\phi }}_{\mathrm {q} }=-k\nabla T}
where ∇ {\displaystyle {\nabla }} is the gradient operator.

Measurement
Main article: Heat flux sensor

The measurement of heat flux can be performed in a few different manners. A commonly known, but often impractical, method is performed by measuring a temperature difference over a piece of material with known thermal conductivity. This method is analogous to a standard way to measure an electric current, where one measures the voltage drop over a known resistor. Usually this method is difficult to perform since the thermal resistance of the material being tested is often not known. Accurate values for the material's thickness and thermal conductivity would be required in order to determine thermal resistance. Using the thermal resistance, along with temperature measurements on either side of the material, heat flux can then be indirectly calculated.

A second method of measuring heat flux is by using a heat flux sensor, or heat flux transducer, to directly measure the amount of heat being transferred to/from the surface that the heat flux sensor is mounted to. The most common type of heat flux sensor is a differential temperature thermopile which operates on essentially the same principle as the first measurement method that was mentioned except it has the advantage in that the thermal resistance/conductivity does not need to be a known parameter. These parameters do not have to be known since the heat flux sensor enables an in-situ measurement of the existing heat flux by using the Seebeck effect. However, differential thermopile heat flux sensors have to be calibrated in order to relate their output signals [μV] to heat flux values [W/(m2⋅K)]. Once the heat flux sensor is calibrated it can then be used to directly measure heat flux without requiring the rarely known value of thermal resistance or thermal conductivity.
Science and engineering

One of the tools in a scientist's or engineer's toolbox is the energy balance. Such a balance can be set up for any physical system, from chemical reactors to living organisms, and generally takes the following form

    ∂ E i n ∂ t − ∂ E o u t ∂ t − ∂ E a c c u m u l a t e d ∂ t = 0 {\big .}{\frac {\partial E_{{\mathrm {in}}}}{\partial t}}-{\frac {\partial E_{{\mathrm {out}}}}{\partial t}}-{\frac {\partial E_{{\mathrm {accumulated}}}}{\partial t}}=0

where the three ∂ E ∂ t {\big .}{\frac {\partial E}{\partial t}} terms stand for the time rate of change of respectively the total amount of incoming energy, the total amount of outgoing energy and the total amount of accumulated energy.

Now, if the only way the system exchanges energy with its surroundings is through heat transfer, the heat rate can be used to calculate the energy balance, since

    ∂ E i n ∂ t − ∂ E o u t ∂ t = ∮ S ϕ → q ⋅ d S → {\displaystyle {\frac {\partial E_{\mathrm {in} }}{\partial t}}-{\frac {\partial E_{\mathrm {out} }}{\partial t}}=\oint _{S}{\vec {\phi }}_{\mathrm {q} }\cdot \,\mathrm {d} {\vec {S}}}

where we have integrated the heat flux ϕ → q {\displaystyle {\vec {\phi }}_{\mathrm {q} }} over the surface S S of the system.

In real-world applications one cannot know the exact heat flux at every point on the surface, but approximation schemes can be used to calculate the integral, for example Monte Carlo integration.
See also

    Radiant flux
    Latent heat flux
    Rate of heat flow
    Insolation
    Heat flux sensor
    Relativistic heat conduction

Notes

    The word "flux" is used in most physical disciplines to refer to the flow of a quantity (mass, heat, momentum, etc.) across a surface per unit time per unit area, with the primary exception being in electromagnetism, where it refer to the integral of a vector quantity through a surface. Refer to the Flux article for more detail.

Authority control: National Edit this at Wikidata	

    Germany Israel United States

Categories:

    Thermodynamic propertiesCustomary units of measurement in the United States

    This page was last edited on 3 May 2023, at 05:16 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
History

Fick's first law

    Variations of the first law
    Derivation of Fick's first law for gases

Fick's second law

    Derivation of Fick's second law

Example solutions and generalization

    Example solution 1: constant concentration source and diffusion length
    Example solution 2: Brownian particle and Mean squared displacement
    Generalizations

Applications

        Fick's flow in liquids
        Sorption rate and collision frequency of diluted solute
        Biological perspective
        Semiconductor fabrication applications
            CVD method of fabricate semiconductor
            Invalidity of Fickian diffusion
        Food production and cooking
    See also
    Citations
    Further reading
    External links

Fick's laws of diffusion

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
For the technique of measuring cardiac output, see Fick principle.
Molecular diffusion from a microscopic and macroscopic point of view. Initially, there are solute molecules on the left side of a barrier (purple line) and none on the right. The barrier is removed, and the solute diffuses to fill the whole container. Top: A single molecule moves around randomly. Middle: With more molecules, there is a clear trend where the solute fills the container more and more uniformly. Bottom: With an enormous number of solute molecules, randomness becomes undetectable: The solute appears to move smoothly and systematically from high-concentration areas to low-concentration areas. This smooth flow is described by Fick's laws.

Fick's laws of diffusion describe diffusion and were first posited by Adolf Fick in 1855 on the basis of largely experimental results. They can be used to solve for the diffusion coefficient, D. Fick's first law can be used to derive his second law which in turn is identical to the diffusion equation.

A diffusion process that obeys Fick's laws is called normal or Fickian diffusion; otherwise, it is called anomalous diffusion or non-Fickian diffusion.
History

In 1855, physiologist Adolf Fick first reported[1] his now well-known laws governing the transport of mass through diffusive means. Fick's work was inspired by the earlier experiments of Thomas Graham, which fell short of proposing the fundamental laws for which Fick would become famous. Fick's law is analogous to the relationships discovered at the same epoch by other eminent scientists: Darcy's law (hydraulic flow), Ohm's law (charge transport), and Fourier's Law (heat transport).

Fick's experiments (modeled on Graham's) dealt with measuring the concentrations and fluxes of salt, diffusing between two reservoirs through tubes of water. It is notable that Fick's work primarily concerned diffusion in fluids, because at the time, diffusion in solids was not considered generally possible.[2] Today, Fick's Laws form the core of our understanding of diffusion in solids, liquids, and gases (in the absence of bulk fluid motion in the latter two cases). When a diffusion process does not follow Fick's laws (which happens in cases of diffusion through porous media and diffusion of swelling penetrants, among others),[3][4] it is referred to as non-Fickian.
Fick's first law

Fick's first law relates the diffusive flux to the gradient of the concentration. It postulates that the flux goes from regions of high concentration to regions of low concentration, with a magnitude that is proportional to the concentration gradient (spatial derivative), or in simplistic terms the concept that a solute will move from a region of high concentration to a region of low concentration across a concentration gradient. In one (spatial) dimension, the law can be written in various forms, where the most common form (see[5][6]) is in a molar basis:

    J = − D d φ d x {\displaystyle J=-D{\frac {d\varphi }{dx}}}

where

    J is the diffusion flux, of which the dimension is the amount of substance per unit area per unit time. J measures the amount of substance that will flow through a unit area during a unit time interval.
    D is the diffusion coefficient or diffusivity. Its dimension is area per unit time.
    φ (for ideal mixtures) is the concentration, of which the dimension is the amount of substance per unit volume.
    x is position, the dimension of which is length.

D is proportional to the squared velocity of the diffusing particles, which depends on the temperature, viscosity of the fluid and the size of the particles according to the Stokes–Einstein relation. In dilute aqueous solutions the diffusion coefficients of most ions are similar and have values that at room temperature are in the range of (0.6–2)×10−9 m2/s. For biological molecules the diffusion coefficients normally range from 10−10 to 10−11 m2/s.

In two or more dimensions we must use ∇, the del or gradient operator, which generalises the first derivative, obtaining

    J = − D ∇ φ {\displaystyle \mathbf {J} =-D\nabla \varphi }

where J denotes the diffusion flux vector.

The driving force for the one-dimensional diffusion is the quantity −∂φ/∂x, which for ideal mixtures is the concentration gradient.
Variations of the first law

Another form for the first law is to write it with the primary variable as mass fraction (yi, given for example in kg/kg), then the equation changes to:

    J i = − ρ D M i ∇ y i {\displaystyle \mathbf {J_{i}} =-{\frac {\rho D}{M_{i}}}\nabla y_{i}}

where

    the index i denotes the ith species,
    Ji is the diffusion flux vector of the ith species (for example in mol/m2-s),
    Mi is the molar mass of the ith species, and
    ρ is the mixture density (for example in kg/m3).

The ρ \rho is outside the gradient operator. This is because:

    y i = ρ s i ρ {\displaystyle y_{i}={\frac {\rho _{si}}{\rho }}}

where ρsi is the partial density of the ith species.

Beyond this, in chemical systems other than ideal solutions or mixtures, the driving force for diffusion of each species is the gradient of chemical potential of this species. Then Fick's first law (one-dimensional case) can be written

    J i = − D c i R T ∂ μ i ∂ x J_i = - \frac{D c_i}{RT} \frac{\partial \mu_i}{\partial x}

where

    the index i denotes the ith species.
    c is the concentration (mol/m3).
    R is the universal gas constant (J/K/mol).
    T is the absolute temperature (K).
    μ is the chemical potential (J/mol).

The driving force of Fick's law can be expressed as a fugacity difference:

    J i = − D R T ∂ f i ∂ x {\displaystyle J_{i}=-{\frac {D}{RT}}{\frac {\partial f_{i}}{\partial x}}}

Fugacity f i f_{i} has Pa units. f i f_{i} is a partial pressure of component i in a vapor f i G {\displaystyle f_{i}^{G}} or liquid f i L {\displaystyle f_{i}^{L}} phase. At vapor liquid equilibrium the evaporation flux is zero because f i G = f i L {\displaystyle f_{i}^{G}=f_{i}^{L}}.
Derivation of Fick's first law for gases

Four versions of Fick's law for binary gas mixtures are given below. These assume: thermal diffusion is negligible; the body force per unit mass is the same on both species; and either pressure is constant or both species have the same molar mass. Under these conditions, Ref.[7] shows in detail how the diffusion equation from the kinetic theory of gases reduces to this version of Fick's law:

V i = − D ∇ ln ⁡ y i ,
{\displaystyle \mathbf {V_{i}} =-D\nabla \ln y_{i},}

where Vi is the diffusion velocity of species i. In terms of species flux this is

J i = − ρ D M i ∇ y i .
{\displaystyle \mathbf {J_{i}} =-{\frac {\rho D}{M_{i}}}\nabla y_{i}.}

If, additionally, ∇ ρ = 0 {\displaystyle \nabla \rho =0}, this reduces to the most common form of Fick's law,

J i = − D ∇ φ .
{\displaystyle \mathbf {J_{i}} =-D\nabla \varphi .}

If (instead of or in addition to ∇ ρ = 0 {\displaystyle \nabla \rho =0}) both species have the same molar mass, Fick's law becomes

J i = − ρ D M i ∇ x i ,
{\displaystyle \mathbf {J_{i}} =-{\frac {\rho D}{M_{i}}}\nabla x_{i},}

where x i x_{i} is the mole fraction of species i.
Fick's second law

Fick's second law predicts how diffusion causes the concentration to change with respect to time. It is a partial differential equation which in one dimension reads:

    ∂ φ ∂ t = D ∂ 2 φ ∂ x 2 {\displaystyle {\frac {\partial \varphi }{\partial t}}=D\,{\frac {\partial ^{2}\varphi }{\partial x^{2}}}}

where

    φ is the concentration in dimensions of [ N L − 3 ] {\displaystyle [{\mathsf {N}}{\mathsf {L}}^{-3}]}, example mol/m3; φ = φ(x,t) is a function that depends on location x and time t
    t is time, example s
    D is the diffusion coefficient in dimensions of [ L 2 T − 1 ] {\displaystyle [{\mathsf {L}}^{2}{\mathsf {T}}^{-1}]}, example m2/s
    x is the position, example m

In two or more dimensions we must use the Laplacian Δ = ∇2, which generalises the second derivative, obtaining the equation

    ∂ φ ∂ t = D Δ φ {\displaystyle {\frac {\partial \varphi }{\partial t}}=D\Delta \varphi }

Fick's second law has the same mathematical form as the Heat equation and its fundamental solution is the same as the Heat kernel, except switching thermal conductivity k k with diffusion coefficient D D:

φ ( x , t ) = 1 4 π D t exp ⁡ ( − x 2 4 D t ) .
{\displaystyle \varphi (x,t)={\frac {1}{\sqrt {4\pi Dt}}}\exp \left(-{\frac {x^{2}}{4Dt}}\right).}

Derivation of Fick's second law

Fick's second law can be derived from Fick's first law and the mass conservation in absence of any chemical reactions:

    ∂ φ ∂ t + ∂ ∂ x J = 0 ⇒ ∂ φ ∂ t − ∂ ∂ x ( D ∂ ∂ x φ ) = 0 {\displaystyle {\frac {\partial \varphi }{\partial t}}+{\frac {\partial }{\partial x}}J=0\Rightarrow {\frac {\partial \varphi }{\partial t}}-{\frac {\partial }{\partial x}}\left(D{\frac {\partial }{\partial x}}\varphi \right)\,=0}

Assuming the diffusion coefficient D to be a constant, one can exchange the orders of the differentiation and multiply by the constant:

    ∂ ∂ x ( D ∂ ∂ x φ ) = D ∂ ∂ x ∂ ∂ x φ = D ∂ 2 φ ∂ x 2 {\displaystyle {\frac {\partial }{\partial x}}\left(D{\frac {\partial }{\partial x}}\varphi \right)=D{\frac {\partial }{\partial x}}{\frac {\partial }{\partial x}}\varphi =D{\frac {\partial ^{2}\varphi }{\partial x^{2}}}}

and, thus, receive the form of the Fick's equations as was stated above.

For the case of diffusion in two or more dimensions Fick's second law becomes

    ∂ φ ∂ t = D ∇ 2 φ , {\displaystyle {\frac {\partial \varphi }{\partial t}}=D\,\nabla ^{2}\varphi ,}

which is analogous to the heat equation.

If the diffusion coefficient is not a constant, but depends upon the coordinate or concentration, Fick's second law yields

    ∂ φ ∂ t = ∇ ⋅ ( D ∇ φ ) . {\displaystyle {\frac {\partial \varphi }{\partial t}}=\nabla \cdot (D\,\nabla \varphi ).}

An important example is the case where φ is at a steady state, i.e. the concentration does not change by time, so that the left part of the above equation is identically zero. In one dimension with constant D, the solution for the concentration will be a linear change of concentrations along x. In two or more dimensions we obtain

    ∇ 2 φ = 0 {\displaystyle \nabla ^{2}\varphi =0}

which is Laplace's equation, the solutions to which are referred to by mathematicians as harmonic functions.
Example solutions and generalization

Fick's second law is a special case of the convection–diffusion equation in which there is no advective flux and no net volumetric source. It can be derived from the continuity equation:

    ∂ φ ∂ t + ∇ ⋅ j = R , {\displaystyle {\frac {\partial \varphi }{\partial t}}+\nabla \cdot \mathbf {j} =R,}

where j is the total flux and R is a net volumetric source for φ. The only source of flux in this situation is assumed to be diffusive flux:

    j diffusion = − D ∇ φ {\displaystyle \mathbf {j} _{\text{diffusion}}=-D\nabla \varphi }

Plugging the definition of diffusive flux to the continuity equation and assuming there is no source (R = 0), we arrive at Fick's second law:

    ∂ φ ∂ t = D ∂ 2 φ ∂ x 2 {\displaystyle {\frac {\partial \varphi }{\partial t}}=D{\frac {\partial ^{2}\varphi }{\partial x^{2}}}}

If flux were the result of both diffusive flux and advective flux, the convection–diffusion equation is the result.
Example solution 1: constant concentration source and diffusion length

A simple case of diffusion with time t in one dimension (taken as the x-axis) from a boundary located at position x = 0, where the concentration is maintained at a value n0 is

    n ( x , t ) = n 0 erfc ⁡ ( x 2 D t ) . {\displaystyle n\left(x,t\right)=n_{0}\operatorname {erfc} \left({\frac {x}{2{\sqrt {Dt}}}}\right).}

where erfc is the complementary error function. This is the case when corrosive gases diffuse through the oxidative layer towards the metal surface (if we assume that concentration of gases in the environment is constant and the diffusion space – that is, the corrosion product layer – is semi-infinite, starting at 0 at the surface and spreading infinitely deep in the material). If, in its turn, the diffusion space is infinite (lasting both through the layer with n(x, 0) = 0, x > 0 and that with n(x, 0) = n0, x ≤ 0), then the solution is amended only with coefficient 1/2 in front of n0 (as the diffusion now occurs in both directions). This case is valid when some solution with concentration n0 is put in contact with a layer of pure solvent. (Bokstein, 2005) The length 2√Dt is called the diffusion length and provides a measure of how far the concentration has propagated in the x-direction by diffusion in time t (Bird, 1976).

As a quick approximation of the error function, the first two terms of the Taylor series can be used:

    n ( x , t ) = n 0 [ 1 − 2 ( x 2 D t π ) ] {\displaystyle n(x,t)=n_{0}\left[1-2\left({\frac {x}{2{\sqrt {Dt\pi }}}}\right)\right]}

If D is time-dependent, the diffusion length becomes

    2 ∫ 0 t D τ d τ . {\displaystyle 2{\sqrt {\int _{0}^{t}D\tau \,d\tau }}.}

This idea is useful for estimating a diffusion length over a heating and cooling cycle, where D varies with temperature.
Example solution 2: Brownian particle and Mean squared displacement

Another simple case of diffusion is the Brownian motion of one particle. The particle's Mean squared displacement from its original position is:

MSD ≡ ⟨ ( x − x 0 ) 2 ⟩ = 2 n D t
{\displaystyle {\text{MSD}}\equiv \langle (\mathbf {x} -\mathbf {x_{0}} )^{2}\rangle =2nDt}

where n n is the dimension of the particle's Brownian motion. For example, the diffusion of a molecule across a cell membrane 8 nm thick is 1-D diffusion because of the spherical symmetry; However, the diffusion of a molecule from the membrane to the center of a eukaryotic cell is a 3-D diffusion. For a cylindrical cactus, the diffusion from photosynthetic cells on its surface to its center (the axis of its cylindrical symmetry) is a 2-D diffusion.

The square root of MSD, 2 n D t {\displaystyle {\sqrt {2nDt}}}, is often used as a characterization of how far has the particle moved after time t t has elapsed. The MSD is symmetrically distributed over the 1D, 2D, and 3D space. Thus, the probability distribution of the magnitude of MSD in 1D is Gaussian and 3D is a Maxwell-Boltzmann distribution.
Generalizations

    In non-homogeneous media, the diffusion coefficient varies in space, D = D(x). This dependence does not affect Fick's first law but the second law changes:
    ∂ φ ( x , t ) ∂ t = ∇ ⋅ ( D ( x ) ∇ φ ( x , t ) ) = D ( x ) Δ φ ( x , t ) + ∑ i = 1 3 ∂ D ( x ) ∂ x i ∂ φ ( x , t ) ∂ x i
    {\displaystyle {\frac {\partial \varphi (x,t)}{\partial t}}=\nabla \cdot {\bigl (}D(x)\nabla \varphi (x,t){\bigr )}=D(x)\Delta \varphi (x,t)+\sum _{i=1}^{3}{\frac {\partial D(x)}{\partial x_{i}}}{\frac {\partial \varphi (x,t)}{\partial x_{i}}}}
    In anisotropic media, the diffusion coefficient depends on the direction. It is a symmetric tensor Dji = Dij. Fick's first law changes to
    J = − D ∇ φ ,
    {\displaystyle J=-D\nabla \varphi ,}
    it is the product of a tensor and a vector:
    J i = − ∑ j = 1 3 D i j ∂ φ ∂ x j .
    {\displaystyle J_{i}=-\sum _{j=1}^{3}D_{ij}{\frac {\partial \varphi }{\partial x_{j}}}.}
    For the diffusion equation this formula gives
    ∂ φ ( x , t ) ∂ t = ∇ ⋅ ( D ∇ φ ( x , t ) ) = ∑ i = 1 3 ∑ j = 1 3 D i j ∂ 2 φ ( x , t ) ∂ x i ∂ x j .
    {\displaystyle {\frac {\partial \varphi (x,t)}{\partial t}}=\nabla \cdot {\bigl (}D\nabla \varphi (x,t){\bigr )}=\sum _{i=1}^{3}\sum _{j=1}^{3}D_{ij}{\frac {\partial ^{2}\varphi (x,t)}{\partial x_{i}\partial x_{j}}}.}
    The symmetric matrix of diffusion coefficients Dij should be positive definite. It is needed to make the right hand side operator elliptic.
    For inhomogeneous anisotropic media these two forms of the diffusion equation should be combined in
    ∂ φ ( x , t ) ∂ t = ∇ ⋅ ( D ( x ) ∇ φ ( x , t ) ) = ∑ i , j = 1 3 ( D i j ( x ) ∂ 2 φ ( x , t ) ∂ x i ∂ x j + ∂ D i j ( x ) ∂ x i ∂ φ ( x , t ) ∂ x i ) .
    {\displaystyle {\frac {\partial \varphi (x,t)}{\partial t}}=\nabla \cdot {\bigl (}D(x)\nabla \varphi (x,t){\bigr )}=\sum _{i,j=1}^{3}\left(D_{ij}(x){\frac {\partial ^{2}\varphi (x,t)}{\partial x_{i}\partial x_{j}}}+{\frac {\partial D_{ij}(x)}{\partial x_{i}}}{\frac {\partial \varphi (x,t)}{\partial x_{i}}}\right).}
    The approach based on Einstein's mobility and Teorell formula gives the following generalization of Fick's equation for the multicomponent diffusion of the perfect components:
    ∂ φ i ∂ t = ∑ j ∇ ⋅ ( D i j φ i φ j ∇ φ j ) .
    {\displaystyle {\frac {\partial \varphi _{i}}{\partial t}}=\sum _{j}\nabla \cdot \left(D_{ij}{\frac {\varphi _{i}}{\varphi _{j}}}\nabla \,\varphi _{j}\right).}
    where φi are concentrations of the components and Dij is the matrix of coefficients. Here, indices i and j are related to the various components and not to the space coordinates.

The Chapman–Enskog formulae for diffusion in gases include exactly the same terms. These physical models of diffusion are different from the test models ∂tφi = Σj Dij Δφj which are valid for very small deviations from the uniform equilibrium. Earlier, such terms were introduced in the Maxwell–Stefan diffusion equation.

For anisotropic multicomponent diffusion coefficients one needs a rank-four tensor, for example Dij,αβ, where i, j refer to the components and α, β = 1, 2, 3 correspond to the space coordinates.
Applications

Equations based on Fick's law have been commonly used to model transport processes in foods, neurons, biopolymers, pharmaceuticals, porous soils, population dynamics, nuclear materials, plasma physics, and semiconductor doping processes. The theory of voltammetric methods is based on solutions of Fick's equation. On the other hand, in some cases a "Fickian (another common approximation of the transport equation is that of the diffusion theory)[8]" description is inadequate. For example, in polymer science and food science a more general approach is required to describe transport of components in materials undergoing a glass transition. One more general framework is the Maxwell–Stefan diffusion equations[9] of multi-component mass transfer, from which Fick's law can be obtained as a limiting case, when the mixture is extremely dilute and every chemical species is interacting only with the bulk mixture and not with other species. To account for the presence of multiple species in a non-dilute mixture, several variations of the Maxwell–Stefan equations are used. See also non-diagonal coupled transport processes (Onsager relationship).
Fick's flow in liquids

When two miscible liquids are brought into contact, and diffusion takes place, the macroscopic (or average) concentration evolves following Fick's law. On a mesoscopic scale, that is, between the macroscopic scale described by Fick's law and molecular scale, where molecular random walks take place, fluctuations cannot be neglected. Such situations can be successfully modeled with Landau-Lifshitz fluctuating hydrodynamics. In this theoretical framework, diffusion is due to fluctuations whose dimensions range from the molecular scale to the macroscopic scale.[10]

In particular, fluctuating hydrodynamic equations include a Fick's flow term, with a given diffusion coefficient, along with hydrodynamics equations and stochastic terms describing fluctuations. When calculating the fluctuations with a perturbative approach, the zero order approximation is Fick's law. The first order gives the fluctuations, and it comes out that fluctuations contribute to diffusion. This represents somehow a tautology, since the phenomena described by a lower order approximation is the result of a higher approximation: this problem is solved only by renormalizing the fluctuating hydrodynamics equations.
Sorption rate and collision frequency of diluted solute
Scheme of molecular diffusion in the solution. Orange dots are solute molecules, solvent molecules are not drawn, black arrow is an example random walk trajectory, and the red curve is the diffusive Gaussian broadening probability function from the Fick's law of diffusion.[11]:Fig. 9

The adsorption or absorption rate of a dilute solute to a surface or interface in a (gas or liquid) solution can be calculated using Fick's laws of diffusion. The accumulated number of molecules adsorbed on the surface is expressed by the Langmuir-Schaefer equation at the short-time limit by integrating the diffusion flux equation over time:[12]

    Γ = 2 A C D t π {\displaystyle \Gamma =2AC{\sqrt {\frac {Dt}{\pi }}}}

    Γ \Gamma is number of molecules in unit # molecules adsorbed during the time t t.
    A is the surface area in unit m 2 {\displaystyle m^{2}}.
    C is the number concentration of the adsorber molecules in the bulk solution in unit # molecules/ m 3 {\displaystyle m^{3}}.
    D is diffusion coefficient of the adsorber in unit m 2 / s {\displaystyle m^{2}/s}.
    t is elapsed time in unit s s.

The equation is named after American chemists Irving Langmuir and Vincent Schaefer.

The Langmuir-Schaefer equation can be extended to the Ward-Tordai Equation to account for the "back-diffusion" of rejected molecules from the surface:[13]

    Γ = 2 A C D t π − A D π ∫ 0 t C b ( τ ) t − τ d τ {\displaystyle \Gamma =2AC{\sqrt {\frac {Dt}{\pi }}}-A{\sqrt {\frac {D}{\pi }}}\int _{0}^{\sqrt {t}}{\frac {C_{b}(\tau )}{\sqrt {t-\tau }}}\,d\tau }

where C C is the bulk concentration, C b C_{b} is the sub-surface concentration (which is a function of time depending on the reaction model of the adsorption), and τ \tau is a dummy variable.

Monte Carlo simulations show that these two equations work to predict the adsorption rate of systems that form predictable concentration gradients near the surface but have troubles for systems without or with unpredictable concentration gradients, such as typical biosensing systems or when flow and convection are significant.[14]
A brief history of the theories on diffusive adsorption.[14]

A brief history of diffusive adsorption is shown in the right figure.[14] A noticeable challenge of understanding the diffusive adsorption at the single-molecule level is the fractal nature of diffusion. Most computer simulations pick a time step for diffusion which ignores the fact that there are self-similar finer diffusion events (fractal) within each step. Simulating the fractal diffusion shows that a factor of two corrections should be introduced for the result of a fixed time-step adsorption simulation, bringing it to be consistent with the above two equations.[14]

A more problematic result of the above equations is they predict the lower limit of adsorption under ideal situations but is very difficult to predict the actual adsorption rates. The equations are derived at the long-time-limit condition when a stable concentration gradient has been formed near the surface. But real adsorption is often done much faster than this infinite time limit, i.e., the concentration gradient, decay of concentration at the sub-surface, is only partially formed before the surface has been saturated, thus the adsorption rate measured is almost always faster than the equations have predicted for low or none energy barrier adsorption (unless there is a significant adsorption energy barrier that slows down the absorption significantly), for example, thousands to millions time faster in the self-assembly of monolayers at the water-air or water-substrate interfaces.[12] As such, it is necessary to calculate the evolution of the concentration gradient near the surface and find out a proper time to stop the imagined infinite evolution for practical applications. While it is hard to predict when to stop but it is reasonably easy to calculate the shortest time that matters, the critical time when the first nearest neighbor from the substrate surface feels the building-up of the concentration gradient. This yields the upper limit of the adsorption rate under an ideal situation when there are no other factors than diffusion that affect the absorber dynamics:[14]

    < r >= 4 π A c b 4 / 3 D {\displaystyle <r>={\frac {4}{\pi }}Ac_{b}^{4/3}D}

    < r > {\displaystyle <r>} is the adsorption rate assuming under adsorption energy barrier-free situation, in unit #/s.
    A A is the area of the surface of interest on an "infinite and flat" substrate in unit m2.
    C b {\displaystyle C_{b}} is the concentration of the absorber molecule in the bulk solution in unit #/m3.
    D D is the diffusion constant of the absorber in the solution in unit m2/s.
    Dimensional analysis of these units is satisfied.

This equation can be used to predict the initial adsorption rate of any system; It can be used to predict the steady-state adsorption rate of a typical biosensing system when the binding site is just a very small fraction of the substrate surface and a near-surface concentration gradient is never formed; It can also be used to predict the adsorption rate of molecules on the surface when there is a significant flow to push the concentration gradient very shallowly in the sub-surface.

In the ultrashort time limit, in the order of the diffusion time a2/D, where a is the particle radius, the diffusion is described by the Langevin equation. At a longer time, the Langevin equation merges into the Stokes–Einstein equation. The latter is appropriate for the condition of the diluted solution, where long-range diffusion is considered. According to the fluctuation-dissipation theorem based on the Langevin equation in the long-time limit and when the particle is significantly denser than the surrounding fluid, the time-dependent diffusion constant is:[15]

    D ( t ) = μ k B T ( 1 − e − t / ( m μ ) ) {\displaystyle D(t)=\mu \,k_{\rm {B}}T\left(1-e^{-t/(m\mu )}\right)}

where (all in SI units)

    kB is Boltzmann's constant.
    T is the absolute temperature.
    μ is the mobility of the particle in the fluid or gas, which can be calculated using the Einstein relation (kinetic theory).
    m is the mass of the particle.
    t is time.

For a single molecule such as organic molecules or biomolecules (e.g. proteins) in water, the exponential term is negligible due to the small product of mμ in the picosecond region.

When the area of interest is the size of a molecule (specifically, a long cylindrical molecule such as DNA), the adsorption rate equation represents the collision frequency of two molecules in a diluted solution, with one molecule a specific side and the other no steric dependence, i.e., a molecule (random orientation) hit one side of the other. The diffusion constant need to be updated to the relative diffusion constant between two diffusing molecules. This estimation is especially useful in studying the interaction between a small molecule and a larger molecule such as a protein. The effective diffusion constant is dominated by the smaller one whose diffusion constant can be used instead.

The above hitting rate equation is also useful to predict the kinetics of molecular self-assembly on a surface. Molecules are randomly oriented in the bulk solution. Assuming 1/6 of the molecules has the right orientation to the surface binding sites, i.e. 1/2 of the z-direction in x, y, z three dimensions, thus the concentration of interest is just 1/6 of the bulk concentration. Put this value into the equation one should be able to calculate the theoretical adsorption kinetic curve using the Langmuir adsorption model. In a more rigid picture, 1/6 can be replaced by the steric factor of the binding geometry.
Comparing collision theory and diffusive collision theory.[16]

The bimolecular collision frequency related to many reactions including protein coagulation/aggregation is initially described by Smoluchowski coagulation equation proposed by Marian Smoluchowski in a seminal 1916 publication,[17] derived from Brownian motion and Fick's laws of diffusion. Under an idealized reaction condition for A+B->Product in a diluted solution, Smoluchovski suggested that the molecular flux at the infinite time limit can be calculated from Fick's laws of diffusion yielding a fixed/stable concentration gradient from the target molecule, e.g. B is the target molecule holding fixed relatively, and A is the moving molecule that creates a concentration gradient near the target molecule B due to the coagulation reaction between A and B. Smoluchowski calculated the collision frequency between A and B in the solution with unit #/s/ m 3 {\displaystyle m^{3}}:

    Z A B = 4 π R D r C A C B {\displaystyle Z_{AB}=4{\pi }RD_{r}C_{A}C_{B}}

where,

    R R is the radius of the collision.
    D r = D A + D B {\displaystyle D_{r}=D_{A}+D_{B}} is the relative diffusion constant between A and B, unit m 2 / s {\displaystyle m^{2}/s}.
    C A C_A and C B C_B are number concentrations of A and B respectively, unit # / m 3 {\displaystyle m^{3}}.

The reaction order of this bimolecular reaction is 2 which is the analogy to the result from collision theory by replacing the moving speed of the molecule with diffusive flux. In the collision theory, the traveling time between A and B is proportional to the distance which is a similar relationship for the diffusion case if the flux is fixed.

However, under a practical condition, the concentration gradient near the target molecule is evolving over time with the molecular flux evolving as well,[14] and on average the flux is much bigger than the infinite time limit flux Smoluchowski has proposed. Thus, this Smoluchowski frequency represents the lower limit of the real collision frequency.

In 2022, Chen calculates the upper limit of the collision frequency between A and B in a solution assuming the bulk concentration of the moving molecule is fixed after the first nearest neighbor of the target molecule.[16] Thus the concentration gradient evolution stops at the first nearest neighbor layer given a stop-time to calculate the actual flux. He named this the critical time and derive the diffusive collision frequency in unit #/s/ m 3 {\displaystyle m^{3}}:[16]

    Z A B = 8 π σ D r C A C B C A + C B 3 {\displaystyle Z_{AB}={\frac {8}{\pi }}{\sigma }D_{r}C_{A}C_{B}{\sqrt[{3}]{C_{A}+C_{B}}}}

where,

    σ {\sigma } is the area of the cross-section of the collision, unit m 2 m^{2}.
    D r = D A + D B {\displaystyle D_{r}=D_{A}+D_{B}} is the relative diffusion constant between A and B, unit m 2 / s {\displaystyle m^{2}/s}.
    C A C_A and C B C_B are number concentrations of A and B respectively, unit # / m 3 {\displaystyle m^{3}}.

This equation assumes the upper limit of a diffusive collision frequency between A and B is when the first neighbor layer starts to feel the evolution of the concentration gradient, whose reaction order is 2 1 3 {\displaystyle 2{\frac {1}{3}}} instead of 2. Both the Smoluchowski equation and the JChen equation satisfy dimensional checks with SI units. But the former is dependent on the radius and the latter is on the area of the collision sphere. The actual reaction order for a bimolecular unit reaction could be between 2 and 2 1 3 {\displaystyle 2{\frac {1}{3}}}, which makes sense because the diffusive collision time is squarely dependent on the distance between the two molecules.
Biological perspective

The first law gives rise to the following formula:[18]

    flux = − P ( c 2 − c 1 ) {\displaystyle {\text{flux}}={-P\left(c_{2}-c_{1}\right)}}

in which

    P is the permeability, an experimentally determined membrane "conductance" for a given gas at a given temperature.
    c2 − c1 is the difference in concentration of the gas across the membrane for the direction of flow (from c1 to c2).

Fick's first law is also important in radiation transfer equations. However, in this context, it becomes inaccurate when the diffusion constant is low and the radiation becomes limited by the speed of light rather than by the resistance of the material the radiation is flowing through. In this situation, one can use a flux limiter.

The exchange rate of a gas across a fluid membrane can be determined by using this law together with Graham's law.

Under the condition of a diluted solution when diffusion takes control, the membrane permeability mentioned in the above section can be theoretically calculated for the solute using the equation mentioned in the last section (use with particular care because the equation is derived for dense solutes, while biological molecules are not denser than water):[11]

    P = 2 A p η t m D / ( π t ) {\displaystyle P=2A_{p}\eta _{tm}{\sqrt {D/(\pi t)}}}

where

    A P {\displaystyle A_{P}} is the total area of the pores on the membrane (unit m2).
    η t m {\displaystyle \eta _{tm}} transmembrane efficiency (unitless), which can be calculated from the stochastic theory of chromatography.
    D is the diffusion constant of the solute unit m2s−1.
    t is time unit s.
    c2, c1 concentration should use unit mol m−3, so flux unit becomes mol s−1.

The flux is decay over the square root of time because a concentration gradient builds up near the membrane over time under ideal conditions. When there is flow and convection, the flux can be significantly different than the equation predicts and show an effective time t with a fixed value,[14] which makes the flux stable instead of decay over time. A critical time has been estimated under idealized flow conditions when there is no gradient formed.[14][16] This strategy is adopted in biology such as blood circulation.
Semiconductor fabrication applications

The semiconductor is a collective term for a series of devices. It mainly includes three categories：two-terminal devices, three-terminal devices, and four-terminal devices. The combination of the semiconductors is called an integrated circuit.

The relationship between Fick's law and semiconductors: the principle of the semiconductor is transferring chemicals or dopants from a layer to a layer. Fick's law can be used to control and predict the diffusion by knowing how much the concentration of the dopants or chemicals move per meter and second through mathematics.

Therefore, different types and levels of semiconductors can be fabricated.

Integrated circuit fabrication technologies, model processes like CVD, thermal oxidation, wet oxidation, doping, etc. use diffusion equations obtained from Fick's law.
CVD method of fabricate semiconductor

The wafer is a kind of semiconductor whose silicon substrate is coated with a layer of CVD-created polymer chain and films. This film contains n-type and p-type dopants and takes responsibility for dopant conductions. The principle of CVD relies on the gas phase and gas-solid chemical reaction to create thin films.

The viscous flow regime of CVD is driven by a pressure gradient. CVD also includes a diffusion component distinct from the surface diffusion of adatoms. In CVD, reactants and products must also diffuse through a boundary layer of stagnant gas that exists next to the substrate. The total number of steps required for CVD film growth are gas phase diffusion of reactants through the boundary layer, adsorption and surface diffusion of adatoms, reactions on the substrate, and gas phase diffusion of products away through the boundary layer.

The velocity profile for gas flow is:

δ ( x ) = ( 5 x R e 1 / 2 ) R e = v ρ L η
{\displaystyle \delta (x)=\left({\frac {5x}{\mathrm {Re} ^{1/2}}}\right)\mathrm {Re} ={\frac {v\rho L}{\eta }}}
where

    δ \delta is the thickness
    R e {\mathrm {Re}} is the Reynolds number
    x is the length of the subtrate.
    v = 0 at any surface
    η \eta is viscosity
    ρ \rho is density.

Integrated the x from 0 to L, it gives the average thickness:

δ = 10 L 3 R e 1 / 2
{\displaystyle \delta ={\frac {10L}{3\mathrm {Re} ^{1/2}}}}

To keep the reaction balanced, reactants must diffuse through the stagnant boundary layer to reach the substrate. So a thin boundary layer is desirable. According to the equations, increasing vo would result in more wasted reactants. The reactants will not reach the substrate uniformly if the flow becomes turbulent. Another option is to switch to a new carrier gas with lower viscosity or density.

The Fick's first law describes diffusion through the boundary layer. As a function of pressure (P) and temperature (T) in a gas, diffusion is determined.

D = D 0 ( P 0 P ) ( T T 0 ) 3 / 2
{\displaystyle D=D_{0}\left({\frac {P_{0}}{P}}\right)\left({\frac {T}{T_{0}}}\right)^{3/2}}
where

    P 0 P_{0} is the standard pressure.
    T 0 T_{0} is the standard temperature.
    D 0 D_{0} is the standard diffusitivity.

The equation tells that increasing the temperature or decreasing the pressure can increase the diffusivity.

Fick's first law predicts the flux of the reactants to the substrate and product away from the substrate:
J = − D i ( d c i d x )
{\displaystyle J=-D_{i}\left({\frac {dc_{i}}{dx}}\right)}
where

    x x is the thickness δ \delta
    d c i {\displaystyle dc_{i}} is the first reactant's concentration.

In ideal gas law P V = n R T PV=nRT, the concentration of the gas is expressed by partial pressure.

J = − D i ( P i − P 0 δ R T )
{\displaystyle J=-D_{i}\left({\frac {P_{i}-P_{0}}{\delta RT}}\right)}
where

    R R is the gas constant.
    P i − P 0 δ {\displaystyle {\frac {P_{i}-P_{0}}{\delta }}} is the partial pressure gradient.

As a result, Fick's first law tells us we can use a partial pressure gradient to control the diffusivity and control the growth of thin films of semiconductors.

In many realistic situations, the simple Fick's law is not an adequate formulation for the semiconductor problem. It only applies to certain conditions, for example, given the semiconductor boundary conditions: constant source concentration diffusion, limited source concentration, or moving boundary diffusion (where junction depth keeps moving into the substrate).
Invalidity of Fickian diffusion

It is important to note that, even though Fickian diffusion has been used to model diffusion processes in semiconductor manufacturing (including CVD reactors) in early days, it often fails to validate the diffusion in advanced semiconductor nodes (< 90 nm). This mostly stems from the inability of Fickian diffusion to model diffusion processes accurately at molecular level and smaller. In advanced semiconductor manufacturing, it is important to understand the movement at atomic scales, which is failed by continuum diffusion. Today, most semiconductor manufacturers use random walk to study and model diffusion processes. This allows us to study the effects of diffusion in a discrete manner to understand the movement of individual atoms, molecules, plasma etc.

In such a process, the movements of diffusing species (atoms, molecules, plasma etc.) are treated as a discrete entity, following a random walk through the CVD reactor, boundary layer, material structures etc. Sometimes, the movements might follow a biased-random walk depending on the processing conditions. Statistical analysis is done to understand variation/stochasticity arising from the random walk of the species, which in-turn affects the overall process and electrical variations.
Food production and cooking

The formulation of Fick's first law can explain a variety of complex phenomena in the context of food and cooking: Diffusion of molecules such as ethylene promotes plant growth and ripening, salt and sugar molecules promotes meat brining and marinating, and water molecules promote dehydration. Fick's first law can also be used to predict the changing moisture profiles across a spaghetti noodle as it hydrates during cooking. These phenomena are all about the spontaneous movement of particles of solutes driven by the concentration gradient. In different situations, there is different diffusivity which is a constant.[19]

By controlling the concentration gradient, the cooking time, shape of the food, and salting can be controlled.
See also

    Advection
    Churchill–Bernstein equation
    Diffusion
    False diffusion
    Gas exchange
    Mass flux
    Maxwell–Stefan diffusion
    Nernst–Planck equation
    Osmosis

Citations

* Fick A (1855). "Ueber Diffusion". Annalen der Physik (in German). 94 (1): 59–86. Bibcode:1855AnP...170...59F. doi:10.1002/andp.18551700105.

    Fick A (1855). "On liquid diffusion". The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science. 10 (63): 30–39. doi:10.1080/14786445508641925.

Philibert J (2005). "One and a Half Centuries of Diffusion: Fick, Einstein, before and beyond" (PDF). Diffusion Fundamentals. 2: 1.1–1.10. Archived from the original (PDF) on 5 February 2009.
Vázquez JL (2006). "The Porous Medium Equation". Mathematical Theory. Oxford Univ. Press.
Gorban AN, Sargsyan HP, Wahab HA (2011). "Quasichemical Models of Multicomponent Nonlinear Diffusion". Mathematical Modelling of Natural Phenomena. 6 (5): 184–262. arXiv:1012.2908. doi:10.1051/mmnp/20116509. S2CID 18961678.
Atkins P, de Paula J (2006). Physical Chemistry for the Life Science.
Conlisk AT (2013). Essentials of Micro- and Nanofluidics: With Applications to the Biological and Chemical Sciences. Cambridge University Press. p. 43. ISBN 9780521881685.
Williams FA (1985). "Appendix E". Combustion Theory. Benjamin/Cummings.
"Fickian Diffusion - an overview | ScienceDirect Topics". www.sciencedirect.com. Retrieved 11 May 2022.
Taylor R, Krishna R (1993). Multicomponent mass transfer. Wiley Series in Chemical Engineering. Vol. 2. John Wiley & Sons. ISBN 978-0-471-57417-0.
Brogioli D, Vailati A (January 2001). "Diffusive mass transfer by nonequilibrium fluctuations: Fick's law revisited". Physical Review E. 63 (1 Pt 1): 012105. arXiv:cond-mat/0006163. Bibcode:2000PhRvE..63a2105B. doi:10.1103/PhysRevE.63.012105. PMID 11304296. S2CID 1302913.
Pyle JR, Chen J (2 November 2017). "Photobleaching of YOYO-1 in super-resolution single DNA fluorescence imaging". Beilstein Journal of Nanotechnology. 8: 2296–2306. doi:10.3762/bjnano.8.229. PMC 5687005. PMID 29181286.
Langmuir I, Schaefer VJ (1937). "The Effect of Dissolved Salts on Insoluble Monolayers". Journal of the American Chemical Society. 29 (11): 2400–2414. doi:10.1021/ja01290a091.
Ward AF, Tordai L (1946). "Time-dependence of Boundary Tensions of Solutions I. The Role of Diffusion in Time-effects". Journal of Chemical Physics. 14 (7): 453–461. Bibcode:1946JChPh..14..453W. doi:10.1063/1.1724167.
Chen J (January 2022). "Simulating stochastic adsorption of diluted solute molecules at interfaces". AIP Advances. 12 (1): 015318. Bibcode:2022AIPA...12a5318C. doi:10.1063/5.0064140. PMC 8758205. PMID 35070490.
Bian X, Kim C, Karniadakis GE (August 2016). "111 years of Brownian motion". Soft Matter. 12 (30): 6331–6346. Bibcode:2016SMat...12.6331B. doi:10.1039/c6sm01153e. PMC 5476231. PMID 27396746.
Chen J (December 2022). "Why Should the Reaction Order of a Bimolecular Reaction be 2.33 Instead of 2?". The Journal of Physical Chemistry A. 126 (51): 9719–9725. doi:10.1021/acs.jpca.2c07500. PMC 9805503. PMID 36520427.
Smoluchowski M (1916). "Drei Vorträge über Diffusion, Brownsche Molekularbewegung und Koagulation von Kolloidteilchen". Zeitschrift für Physik (in German). 17: 557–571, 585–599. Bibcode:1916ZPhy...17..557S.
Nosek TM. "Section 3/3ch9/s3ch9_2". Essentials of Human Physiology. Archived from the original on 24 March 2016.

    Zhou L, Nyberg K, Rowat AC (September 2015). "Understanding diffusion theory and Fick's law through food and cooking". Advances in Physiology Education. 39 (3): 192–197. doi:10.1152/advan.00133.2014. PMID 26330037. S2CID 3921833.

Further reading

    Berg HC (1977). Random Walks in Biology. Princeton.
    Bird RB, Stewart WE, Lightfoot EN (1976). Transport Phenomena. John Wiley & Sons.
    Bokshtein BS, Mendelev MI, Srolovitz DJ, eds. (2005). Thermodynamics and Kinetics in Materials Science: A Short Course. Oxford: Oxford University Press. pp. 167–171.
    Crank J (1980). The Mathematics of Diffusion. Oxford University Press.
    Fick A (1855). "On liquid diffusion". Annalen der Physik und Chemie. 94: 59. – reprinted in Fick, Adolph (1995). "On liquid diffusion". Journal of Membrane Science. 100: 33–38. doi:10.1016/0376-7388(94)00230-v.
    Smith WF (2004). Foundations of Materials Science and Engineering (3rd ed.). McGraw-Hill.

External links

    Fick's equations, Boltzmann's transformation, etc. (with figures and animations)
    Fick's Second Law on OpenStax

Categories:

    DiffusionMathematics in medicinePhysical chemistryStatistical mechanics

    This page was last edited on 16 August 2023, at 03:41 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki



Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
History of coding theory

Source coding

    Definition
    Properties
    Principle
    Example

Channel coding

    Linear codes
        Linear block codes
        Convolutional codes

Cryptographic coding

Line coding

Other applications of coding theory

        Group testing
        Analog coding
    Neural coding
    See also
    Notes
    References

Coding theory

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
A two-dimensional visualisation of the Hamming distance, a critical measure in coding theory

Coding theory is the study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. Codes are studied by various scientific disciplines—such as information theory, electrical engineering, mathematics, linguistics, and computer science—for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.

There are four types of coding:[1]

    Data compression (or source coding)
    Error control (or channel coding)
    Cryptographic coding
    Line coding

Data compression attempts to remove unwanted redundancy from the data from a source in order to transmit it more efficiently. For example, ZIP data compression makes data files smaller, for purposes such as to reduce Internet traffic. Data compression and error correction may be studied in combination.

Error correction adds useful redundancy to the data from a source to make the transmission more robust to disturbances present on the transmission channel. The ordinary user may not be aware of many applications using error correction. A typical music compact disc (CD) uses the Reed–Solomon code to correct for scratches and dust. In this application the transmission channel is the CD itself. Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmissions, and the NASA Deep Space Network all employ channel coding techniques to get the bits through, for example the turbo code and LDPC codes.
History of coding theory

In 1948, Claude Shannon published "A Mathematical Theory of Communication", an article in two parts in the July and October issues of the Bell System Technical Journal. This work focuses on the problem of how best to encode the information a sender wants to transmit. In this fundamental work he used tools in probability theory, developed by Norbert Wiener, which were in their nascent stages of being applied to communication theory at that time. Shannon developed information entropy as a measure for the uncertainty in a message while essentially inventing the field of information theory.

The binary Golay code was developed in 1949. It is an error-correcting code capable of correcting up to three errors in each 24-bit word, and detecting a fourth.

Richard Hamming won the Turing Award in 1968 for his work at Bell Labs in numerical methods, automatic coding systems, and error-detecting and error-correcting codes. He invented the concepts known as Hamming codes, Hamming windows, Hamming numbers, and Hamming distance.

In 1972, Nasir Ahmed proposed the discrete cosine transform (DCT), which he developed with T. Natarajan and K. R. Rao in 1973.[2] The DCT is the most widely used lossy compression algorithm, the basis for multimedia formats such as JPEG, MPEG and MP3.
Source coding
Main article: Data compression

The aim of source coding is to take the source data and make it smaller.
Definition

Data can be seen as a random variable X : Ω → X {\displaystyle X:\Omega \to {\mathcal {X}}}, where x ∈ X x\in {\mathcal {X}} appears with probability P [ X = x ] \mathbb {P} [X=x].

Data are encoded by strings (words) over an alphabet Σ \Sigma .

A code is a function

    C : X → Σ ∗ {\displaystyle C:{\mathcal {X}}\to \Sigma ^{*}} (or Σ + \Sigma ^{+} if the empty string is not part of the alphabet).

C ( x ) C(x) is the code word associated with x x.

Length of the code word is written as

    l ( C ( x ) ) . {\displaystyle l(C(x)).}

Expected length of a code is

    l ( C ) = ∑ x ∈ X l ( C ( x ) ) P [ X = x ] . {\displaystyle l(C)=\sum _{x\in {\mathcal {X}}}l(C(x))\mathbb {P} [X=x].}

The concatenation of code words C ( x 1 , … , x k ) = C ( x 1 ) C ( x 2 ) ⋯ C ( x k ) {\displaystyle C(x_{1},\ldots ,x_{k})=C(x_{1})C(x_{2})\cdots C(x_{k})}.

The code word of the empty string is the empty string itself:

    C ( ϵ ) = ϵ C(\epsilon )=\epsilon 

Properties

    C : X → Σ ∗ {\displaystyle C:{\mathcal {X}}\to \Sigma ^{*}} is non-singular if injective.
    C : X ∗ → Σ ∗ {\displaystyle C:{\mathcal {X}}^{*}\to \Sigma ^{*}} is uniquely decodable if injective.
    C : X → Σ ∗ {\displaystyle C:{\mathcal {X}}\to \Sigma ^{*}} is instantaneous if C ( x 1 ) C(x_{1}) is not a prefix of C ( x 2 ) C(x_{2}) (and vice versa).

Principle

Entropy of a source is the measure of information. Basically, source codes try to reduce the redundancy present in the source, and represent the source with fewer bits that carry more information.

Data compression which explicitly tries to minimize the average length of messages according to a particular assumed probability model is called entropy encoding.

Various techniques used by source coding schemes try to achieve the limit of entropy of the source. C(x) ≥ H(x), where H(x) is entropy of source (bitrate), and C(x) is the bitrate after compression. In particular, no source coding scheme can be better than the entropy of the source.
Example

Facsimile transmission uses a simple run length code. Source coding removes all data superfluous to the need of the transmitter, decreasing the bandwidth required for transmission.
Channel coding
Main article: Error detection and correction

The purpose of channel coding theory is to find codes which transmit quickly, contain many valid code words and can correct or at least detect many errors. While not mutually exclusive, performance in these areas is a trade-off. So, different codes are optimal for different applications. The needed properties of this code mainly depend on the probability of errors happening during transmission. In a typical CD, the impairment is mainly dust or scratches.

CDs use cross-interleaved Reed–Solomon coding to spread the data out over the disk.[3]

Although not a very good code, a simple repeat code can serve as an understandable example. Suppose we take a block of data bits (representing sound) and send it three times. At the receiver we will examine the three repetitions bit by bit and take a majority vote. The twist on this is that we do not merely send the bits in order. We interleave them. The block of data bits is first divided into 4 smaller blocks. Then we cycle through the block and send one bit from the first, then the second, etc. This is done three times to spread the data out over the surface of the disk. In the context of the simple repeat code, this may not appear effective. However, there are more powerful codes known which are very effective at correcting the "burst" error of a scratch or a dust spot when this interleaving technique is used.

Other codes are more appropriate for different applications. Deep space communications are limited by the thermal noise of the receiver which is more of a continuous nature than a bursty nature. Likewise, narrowband modems are limited by the noise, present in the telephone network and also modeled better as a continuous disturbance.[citation needed] Cell phones are subject to rapid fading. The high frequencies used can cause rapid fading of the signal even if the receiver is moved a few inches. Again there are a class of channel codes that are designed to combat fading.[citation needed]
Linear codes
Main article: Linear code

The term algebraic coding theory denotes the sub-field of coding theory where the properties of codes are expressed in algebraic terms and then further researched.[citation needed]

Algebraic coding theory is basically divided into two major types of codes:[citation needed]

    Linear block codes
    Convolutional codes

It analyzes the following three properties of a code – mainly:[citation needed]

    Code word length
    Total number of valid code words
    The minimum distance between two valid code words, using mainly the Hamming distance, sometimes also other distances like the Lee distance

Linear block codes
Main article: Block code

Linear block codes have the property of linearity, i.e. the sum of any two codewords is also a code word, and they are applied to the source bits in blocks, hence the name linear block codes. There are block codes that are not linear, but it is difficult to prove that a code is a good one without this property.[4]

Linear block codes are summarized by their symbol alphabets (e.g., binary or ternary) and parameters (n,m,dmin)[5] where

    n is the length of the codeword, in symbols,
    m is the number of source symbols that will be used for encoding at once,
    dmin is the minimum hamming distance for the code.

There are many types of linear block codes, such as

    Cyclic codes (e.g., Hamming codes)
    Repetition codes
    Parity codes
    Polynomial codes (e.g., BCH codes)
    Reed–Solomon codes
    Algebraic geometric codes
    Reed–Muller codes
    Perfect codes

Block codes are tied to the sphere packing problem, which has received some attention over the years. In two dimensions, it is easy to visualize. Take a bunch of pennies flat on the table and push them together. The result is a hexagon pattern like a bee's nest. But block codes rely on more dimensions which cannot easily be visualized. The powerful (24,12) Golay code used in deep space communications uses 24 dimensions. If used as a binary code (which it usually is) the dimensions refer to the length of the codeword as defined above.

The theory of coding uses the N-dimensional sphere model. For example, how many pennies can be packed into a circle on a tabletop, or in 3 dimensions, how many marbles can be packed into a globe. Other considerations enter the choice of a code. For example, hexagon packing into the constraint of a rectangular box will leave empty space at the corners. As the dimensions get larger, the percentage of empty space grows smaller. But at certain dimensions, the packing uses all the space and these codes are the so-called "perfect" codes. The only nontrivial and useful perfect codes are the distance-3 Hamming codes with parameters satisfying (2r – 1, 2r – 1 – r, 3), and the [23,12,7] binary and [11,6,5] ternary Golay codes.[4][5]

Another code property is the number of neighbors that a single codeword may have.[6] Again, consider pennies as an example. First we pack the pennies in a rectangular grid. Each penny will have 4 near neighbors (and 4 at the corners which are farther away). In a hexagon, each penny will have 6 near neighbors. When we increase the dimensions, the number of near neighbors increases very rapidly. The result is the number of ways for noise to make the receiver choose a neighbor (hence an error) grows as well. This is a fundamental limitation of block codes, and indeed all codes. It may be harder to cause an error to a single neighbor, but the number of neighbors can be large enough so the total error probability actually suffers.[6]

Properties of linear block codes are used in many applications. For example, the syndrome-coset uniqueness property of linear block codes is used in trellis shaping,[7] one of the best-known shaping codes.
Convolutional codes
Main article: Convolutional code

The idea behind a convolutional code is to make every codeword symbol be the weighted sum of the various input message symbols. This is like convolution used in LTI systems to find the output of a system, when you know the input and impulse response.

So we generally find the output of the system convolutional encoder, which is the convolution of the input bit, against the states of the convolution encoder, registers.

Fundamentally, convolutional codes do not offer more protection against noise than an equivalent block code. In many cases, they generally offer greater simplicity of implementation over a block code of equal power. The encoder is usually a simple circuit which has state memory and some feedback logic, normally XOR gates. The decoder can be implemented in software or firmware.

The Viterbi algorithm is the optimum algorithm used to decode convolutional codes. There are simplifications to reduce the computational load. They rely on searching only the most likely paths. Although not optimum, they have generally been found to give good results in low noise environments.

Convolutional codes are used in voiceband modems (V.32, V.17, V.34) and in GSM mobile phones, as well as satellite and military communication devices.
Cryptographic coding
Main article: Cryptography

Cryptography or cryptographic coding is the practice and study of techniques for secure communication in the presence of third parties (called adversaries).[8] More generally, it is about constructing and analyzing protocols that block adversaries;[9] various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation[10] are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, and electrical engineering. Applications of cryptography include ATM cards, computer passwords, and electronic commerce.

Cryptography prior to the modern age was effectively synonymous with encryption, the conversion of information from a readable state to apparent nonsense. The originator of an encrypted message shared the decoding technique needed to recover the original information only with intended recipients, thereby precluding unwanted persons from doing the same. Since World War I and the advent of the computer, the methods used to carry out cryptology have become increasingly complex and its application more widespread.

Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.
Line coding
Main article: Line code

A line code (also called digital baseband modulation or digital baseband transmission method) is a code chosen for use within a communications system for baseband transmission purposes. Line coding is often used for digital data transport.

Line coding consists of representing the digital signal to be transported by an amplitude- and time-discrete signal that is optimally tuned for the specific properties of the physical channel (and of the receiving equipment). The waveform pattern of voltage or current used to represent the 1s and 0s of a digital data on a transmission link is called line encoding. The common types of line encoding are unipolar, polar, bipolar, and Manchester encoding.
Other applications of coding theory
	
This article or section may contain misleading parts. Please help clarify this article according to any suggestions provided on the talk page. (August 2012)

Another concern of coding theory is designing codes that help synchronization. A code may be designed so that a phase shift can be easily detected and corrected and that multiple signals can be sent on the same channel.[citation needed]

Another application of codes, used in some mobile phone systems, is code-division multiple access (CDMA). Each phone is assigned a code sequence that is approximately uncorrelated with the codes of other phones.[citation needed] When transmitting, the code word is used to modulate the data bits representing the voice message. At the receiver, a demodulation process is performed to recover the data. The properties of this class of codes allow many users (with different codes) to use the same radio channel at the same time. To the receiver, the signals of other users will appear to the demodulator only as a low-level noise.[citation needed]

Another general class of codes are the automatic repeat-request (ARQ) codes. In these codes the sender adds redundancy to each message for error checking, usually by adding check bits. If the check bits are not consistent with the rest of the message when it arrives, the receiver will ask the sender to retransmit the message. All but the simplest wide area network protocols use ARQ. Common protocols include SDLC (IBM), TCP (Internet), X.25 (International) and many others. There is an extensive field of research on this topic because of the problem of matching a rejected packet against a new packet. Is it a new one or is it a retransmission? Typically numbering schemes are used, as in TCP."RFC793". RFCS. Internet Engineering Task Force (IETF). September 1981.
Group testing

Group testing uses codes in a different way. Consider a large group of items in which a very few are different in a particular way (e.g., defective products or infected test subjects). The idea of group testing is to determine which items are "different" by using as few tests as possible. The origin of the problem has its roots in the Second World War when the United States Army Air Forces needed to test its soldiers for syphilis.[11]
Analog coding

Information is encoded analogously in the neural networks of brains, in analog signal processing, and analog electronics. Aspects of analog coding include analog error correction,[12] analog data compression[13] and analog encryption.[14]
Neural coding

Neural coding is a neuroscience-related field concerned with how sensory and other information is represented in the brain by networks of neurons. The main goal of studying neural coding is to characterize the relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among electrical activity of the neurons in the ensemble.[15] It is thought that neurons can encode both digital and analog information,[16] and that neurons follow the principles of information theory and compress information,[17] and detect and correct[18] errors in the signals that are sent throughout the brain and wider nervous system.
See also

    Telecommunication portal

    Coding gain
    Covering code
    Error correction code
    Folded Reed–Solomon code
    Group testing
    Hamming distance, Hamming weight
    Lee distance
    List of algebraic coding theory topics
    Spatial coding and MIMO in multiple antenna research
        Spatial diversity coding is spatial coding that transmits replicas of the information signal along different spatial paths, so as to increase the reliability of the data transmission.
        Spatial interference cancellation coding
        Spatial multiplex coding
    Timeline of information theory, data compression, and error correcting codes

Notes

James Irvine; David Harle (2002). "2.4.4 Types of Coding". Data Communications and Networks. John Wiley & Sons. p. 18. ISBN 9780471808725. "There are four types of coding"
Nasir Ahmed. "How I Came Up With the Discrete Cosine Transform". Digital Signal Processing, Vol. 1, Iss. 1, 1991, pp. 4-5.
Todd Campbell. "Answer Geek: Error Correction Rule CDs".
Terras, Audrey (1999). Fourier Analysis on Finite Groups and Applications. Cambridge University Press. p. 195. ISBN 978-0-521-45718-7.
Blahut, Richard E. (2003). Algebraic Codes for Data Transmission. Cambridge University Press. ISBN 978-0-521-55374-2.
Christian Schlegel; Lance Pérez (2004). Trellis and turbo coding. Wiley-IEEE. p. 73. ISBN 978-0-471-22755-7.
Forney, G.D. Jr. (March 1992). "Trellis shaping". IEEE Transactions on Information Theory. 38 (2 Pt 2): 281–300. doi:10.1109/18.119687. S2CID 37984132.
Rivest, Ronald L. (1990). "Cryptology". In J. Van Leeuwen (ed.). Handbook of Theoretical Computer Science. Vol. 1. Elsevier.
Bellare, Mihir; Rogaway, Phillip (21 September 2005). "Introduction". Introduction to Modern Cryptography. p. 10.
Menezes, A. J.; van Oorschot, P. C.; Vanstone, S. A. (1997). Handbook of Applied Cryptography. Taylor & Francis. ISBN 978-0-8493-8523-0.
Dorfman, Robert (1943). "The detection of defective members of large populations". Annals of Mathematical Statistics. 14 (4): 436–440. doi:10.1214/aoms/1177731363.
Chen, Brian; Wornell, Gregory W. (July 1998). "Analog Error-Correcting Codes Based on Chaotic Dynamical Systems" (PDF). IEEE Transactions on Communications. 46 (7): 881–890. CiteSeerX 10.1.1.30.4093. doi:10.1109/26.701312. Archived from the original (PDF) on 2001-09-27. Retrieved 2013-06-30.
Novak, Franc; Hvala, Bojan; Klavžar, Sandi (1999). "On Analog Signature Analysis". Proceedings of the conference on Design, automation and test in Europe. CiteSeerX 10.1.1.142.5853. ISBN 1-58113-121-6.
Shujun Li; Chengqing Li; Kwok-Tung Lo; Guanrong Chen (April 2008). "Cryptanalyzing an Encryption Scheme Based on Blind Source Separation" (PDF). IEEE Transactions on Circuits and Systems I. 55 (4): 1055–63. arXiv:cs/0608024. doi:10.1109/TCSI.2008.916540. S2CID 2224947.
Brown EN, Kass RE, Mitra PP (May 2004). "Multiple neural spike train data analysis: state-of-the-art and future challenges" (PDF). Nature Neuroscience. 7 (5): 456–461. doi:10.1038/nn1228. PMID 15114358. S2CID 562815.
Thorpe, S.J. (1990). "Spike arrival times: A highly efficient coding scheme for neural networks" (PDF). In Eckmiller, R.; Hartmann, G.; Hauske, G. (eds.). Parallel processing in neural systems and computers (PDF). North-Holland. pp. 91–94. ISBN 978-0-444-88390-2. Retrieved 30 June 2013.
Gedeon, T.; Parker, A.E.; Dimitrov, A.G. (Spring 2002). "Information Distortion and Neural Coding". Canadian Applied Mathematics Quarterly. 10 (1): 10. CiteSeerX 10.1.1.5.6365.

    Stiber, M. (July 2005). "Spike timing precision and neural error correction: local behavior". Neural Computation. 17 (7): 1577–1601. arXiv:q-bio/0501021. doi:10.1162/0899766053723069. PMID 15901408. S2CID 2064645.

References

    Elwyn R. Berlekamp (2014), Algebraic Coding Theory, World Scientific Publishing (revised edition), ISBN 978-9-81463-589-9.
    MacKay, David J. C. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1
    Vera Pless (1982), Introduction to the Theory of Error-Correcting Codes, John Wiley & Sons, Inc., ISBN 0-471-08684-3.
    Randy Yates, A Coding Theory Tutorial.

    vte

Industrial and applied mathematics
Authority control: National Edit this at Wikidata	

    France BnF data Germany Israel United States Japan

Categories:

    Coding theoryError detection and correction

    This page was last edited on 3 August 2023, at 09:45 (UTC).
    Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki

Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Background

Description

Derivation

Use in petroleum engineering

Use in coffee brewing

Additional forms

        Differential expression
        Quadratic law
        Correction for gases in fine media (Knudsen diffusion or Klinkenberg effect)
        Darcy's law for short time scales
        Brinkman form of Darcy's law
    Validity of Darcy's law
    See also
    References

Darcy's law

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia

Darcy's law is an equation that describes the flow of a fluid through a porous medium. The law was formulated by Henry Darcy based on results of experiments[1] on the flow of water through beds of sand, forming the basis of hydrogeology, a branch of earth sciences. It is analogous to Ohm's law in electrostatics, linearly relating the volume flow rate of the fluid to the hydraulic head difference (which is often just proportional to the pressure difference) via the hydraulic conductivity.
Background

Darcy's law was first determined experimentally by Darcy, but has since been derived from the Navier–Stokes equations via homogenization methods.[2] It is analogous to Fourier's law in the field of heat conduction, Ohm's law in the field of electrical networks, and Fick's law in diffusion theory.

One application of Darcy's law is in the analysis of water flow through an aquifer; Darcy's law along with the equation of conservation of mass simplifies to the groundwater flow equation, one of the basic relationships of hydrogeology.

Morris Muskat first[3] refined Darcy's equation for a single-phase flow by including viscosity in the single (fluid) phase equation of Darcy. It can be understood that viscous fluids have more difficulty permeating through a porous medium than less viscous fluids. This change made it suitable for researchers in the petroleum industry. Based on experimental results by his colleagues Wyckoff and Botset, Muskat and Meres also generalized Darcy's law to cover a multiphase flow of water, oil and gas in the porous medium of a petroleum reservoir. The generalized multiphase flow equations by Muskat and others provide the analytical foundation for reservoir engineering that exists to this day.
Description
Diagram showing definitions and directions for Darcy's law. A is the cross sectional area (m2) of the cylinder. Q is the flow rate (m3/s) of the fluid flowing through the area A. The flux of fluid through A is q = Q/A. L is the length of the cylinder. Δp = poutlet - pinlet = pb - pa. ∇ p \nabla p = Δp/L = hydraulic gradient applied between the points a and b.

Darcy's law, as refined by Morris Muskat, in the absence of gravitational forces and in a homogeneously permeable medium, is given by a simple proportionality relationship between the instantaneous flux q = Q / A {\displaystyle q=Q/A} (units of Q Q: m3/s, units of A A: m2, units of q q: m/s) through a porous medium, the permeability k k of the medium, the dynamic viscosity of the fluid μ \mu , and the pressure drop Δ p \Delta p over a given distance L L, in the form
q = − k μ L Δ p
{\displaystyle q=-{\frac {k}{\mu L}}\Delta p}

This equation, for single phase (fluid) flow, is the defining equation for absolute permeability (single phase permeability).

With reference to the diagram to the right, the flux q q, or discharge per unit area, is defined in units ( m / s ) {\displaystyle \mathrm {(m/s)} }, the permeability k k in units ( m 2 ) {\displaystyle \mathrm {(m^{2})} }, the cross-sectional area A A in units ( m 2 ) {\displaystyle \mathrm {(m^{2})} }, the total pressure drop Δ p = p b − p a {\displaystyle \Delta p=p_{b}-p_{a}} in units ( P a ) {\displaystyle \mathrm {(Pa)} }, the dynamic viscosity μ \mu in units ( P a ⋅ s ) {\displaystyle \mathrm {(Pa\cdot s)} }, and L L is the length of the sample in units ( m ) {\displaystyle \mathrm {(m)} }. A number of these parameters are used in alternative definitions below. A negative sign is used in the definition of the flux following the standard physics convention that fluids flow from regions of high pressure to regions of low pressure. Note that the elevation head must be taken into account if the inlet and outlet are at different elevations. If the change in pressure is negative, then the flow will be in the positive x direction. There have been several proposals for a constitutive equation for absolute permeability, and the most famous one is probably the Kozeny equation (also called Kozeny–Carman equation).

The integral form of the Darcy law is given by:
Q = k A μ L Δ p
{\displaystyle Q={\frac {kA}{\mu L}}\,{\Delta p}}
where Q (units of volume per time, e.g., m3/s) is the total discharge. By considering the relation for static fluid pressure (Stevin's law):

p = ρ g h
{\displaystyle p=\rho gh}
one can deduce the representation
Q = k A g ν L Δ h
{\displaystyle Q={\frac {kAg}{\nu L}}\,{\Delta h}}
where ν is the kinematic viscosity. The corresponding hydraulic conductivity is therefore:

    K = k ρ g μ = k g ν . {\displaystyle K={\frac {k\rho g}{\mu }}={\frac {kg}{\nu }}.}

Notice that the quantity q q or Q / A {\displaystyle Q/A}, often referred to as the Darcy flux or Darcy velocity, is not the velocity at which the fluid is travelling through the pores. The flow velocity (u) is related to the flux (q) by the porosity (φ) and takes the form

    u = q φ . {\displaystyle u={\frac {q}{\varphi }}\,.}

Darcy's law is a simple mathematical statement which neatly summarizes several familiar properties that groundwater flowing in aquifers exhibits, including:

    if there is no pressure gradient over a distance, no flow occurs (these are hydrostatic conditions),
    if there is a pressure gradient, flow will occur from high pressure towards low pressure (opposite the direction of increasing gradient — hence the negative sign in Darcy's law),
    the greater the pressure gradient (through the same formation material), the greater the discharge rate, and
    the discharge rate of fluid will often be different — through different formation materials (or even through the same material, in a different direction) — even if the same pressure gradient exists in both cases.

A graphical illustration of the use of the steady-state groundwater flow equation (based on Darcy's law and the conservation of mass) is in the construction of flownets, to quantify the amount of groundwater flowing under a dam.

Darcy's law is only valid for slow, viscous flow; however, most groundwater flow cases fall in this category. Typically any flow with a Reynolds number less than one is clearly laminar, and it would be valid to apply Darcy's law. Experimental tests have shown that flow regimes with Reynolds numbers up to 10 may still be Darcian, as in the case of groundwater flow. The Reynolds number (a dimensionless parameter) for porous media flow is typically expressed as

    R e = u d ν , {\displaystyle \mathrm {Re} ={\frac {ud}{\nu }}\,,}

where ν is the kinematic viscosity of water, u is the specific discharge (not the pore velocity — with units of length per time), d30 is a representative grain diameter for the porous media (the standard choice is d30, which is the 30% passing size from a grain size analysis using sieves — with units of length).
Derivation

For stationary, creeping, incompressible flow, i.e. D(ρui)/Dt ≈ 0, the Navier–Stokes equation simplifies to the Stokes equation, which by neglecting the bulk term is:

    μ ∇ 2 u i − ∂ i p = 0 , {\displaystyle \mu \nabla ^{2}u_{i}-\partial _{i}p=0\,,}

where μ is the viscosity, ui is the velocity in the i direction, and p is the pressure. Assuming the viscous resisting force is linear with the velocity we may write:

    − ( k − 1 ) i j μ φ u j − ∂ i p = 0 , {\displaystyle -\left(k^{-1}\right)_{ij}\mu \varphi u_{j}-\partial _{i}p=0\,,}

where φ is the porosity, and kij is the second order permeability tensor. This gives the velocity in the n direction,

    k n i ( k − 1 ) i j u j = δ n j u j = u n = − k n i φ μ ∂ i p , {\displaystyle k_{ni}\left(k^{-1}\right)_{ij}u_{j}=\delta _{nj}u_{j}=u_{n}=-{\frac {k_{ni}}{\varphi \mu }}\partial _{i}p\,,}

which gives Darcy's law for the volumetric flux density in the n direction,

    q n = − k n i μ ∂ i p . {\displaystyle q_{n}=-{\frac {k_{ni}}{\mu }}\,\partial _{i}p\,.}

In isotropic porous media the off-diagonal elements in the permeability tensor are zero, kij = 0 for i ≠ j and the diagonal elements are identical, kii = k, and the common form is obtained as below, which enables the determination of the liquid flow velocity by solving a set of equations in a given region. [4]

    q = − k μ ∇ p . {\displaystyle {\boldsymbol {q}}=-{\frac {k}{\mu }}\,{\boldsymbol {\nabla }}p\,.}

The above equation is a governing equation for single-phase fluid flow in a porous medium.
Use in petroleum engineering

Another derivation of Darcy's law is used extensively in petroleum engineering to determine the flow through permeable media — the most simple of which is for a one-dimensional, homogeneous rock formation with a single fluid phase and constant fluid viscosity.

Almost all oil reservoirs have a water zone below the oil leg, and some have also a gas cap above the oil leg. When the reservoir pressure drops due to oil production, water flows into the oil zone from below, and gas flows into the oil zone from above (if the gas cap exists), and we get a simultaneous flow and immiscible mixing of all fluid phases in the oil zone. The operator of the oil field may also inject water (and/or gas) in order to improve oil production. The petroleum industry is therefore using a generalized Darcy equation for multiphase flow that was developed by Muskat et alios. Because Darcy's name is so widespread and strongly associated with flow in porous media, the multiphase equation is denoted Darcy's law for multiphase flow or generalized Darcy equation (or law) or simply Darcy's equation (or law) or simply flow equation if the context says that the text is discussing the multiphase equation of Muskat et alios. Multiphase flow in oil and gas reservoirs is a comprehensive topic, and one of many articles about this topic is Darcy's law for multiphase flow.
Use in coffee brewing

A number of papers have utilized Darcy's law to model the physics of brewing in a moka pot, specifically how the hot water percolates through the coffee grinds under pressure, starting with a 2001 paper by Varlamov and Balestrino,[5] and continuing with a 2007 paper by Gianino,[6] a 2008 paper by Navarini et al.[7], and a 2008 paper by W. King.[8] The papers will either take the coffee permeability to be constant as a simplification or will measure change through the brewing process.
Additional forms
Differential expression

Darcy's law can be expressed very generally as:

    q = − K ∇ h {\displaystyle \mathbf {q} =-K\nabla h}

where q is the volume flux vector of the fluid at a particular point in the medium, h is the total hydraulic head, and K is the hydraulic conductivity tensor, at that point. The hydraulic conductivity can often be approximated as a scalar. (Note the analogy to Ohm's law in electrostatics. The flux vector is analogous to the current density, head is analogous to voltage, and hydraulic conductivity is analogous to electrical conductivity.)
Quadratic law

For flows in porous media with Reynolds numbers greater than about 1 to 10, inertial effects can also become significant. Sometimes an inertial term is added to the Darcy's equation, known as Forchheimer term. This term is able to account for the non-linear behavior of the pressure difference vs flow data.[9]

    ∂ p ∂ x = − μ k q − ρ k 1 q 2 , {\displaystyle {\frac {\partial p}{\partial x}}=-{\frac {\mu }{k}}q-{\frac {\rho }{k_{1}}}q^{2}\,,}

where the additional term k1 is known as inertial permeability.

The flow in the middle of a sandstone reservoir is so slow that Forchheimer's equation is usually not needed, but the gas flow into a gas production well may be high enough to justify use of Forchheimer's equation. In this case, the inflow performance calculations for the well, not the grid cell of the 3D model, is based on the Forchheimer equation. The effect of this is that an additional rate-dependent skin appears in the inflow performance formula.

Some carbonate reservoirs have many fractures, and Darcy's equation for multiphase flow is generalized in order to govern both flow in fractures and flow in the matrix (i.e. the traditional porous rock). The irregular surface of the fracture walls and high flow rate in the fractures may justify the use of Forchheimer's equation.
Correction for gases in fine media (Knudsen diffusion or Klinkenberg effect)

For gas flow in small characteristic dimensions (e.g., very fine sand, nanoporous structures etc.), the particle-wall interactions become more frequent, giving rise to additional wall friction (Knudsen friction). For a flow in this region, where both viscous and Knudsen friction are present, a new formulation needs to be used. Knudsen presented a semi-empirical model for flow in transition regime based on his experiments on small capillaries.[10][11] For a porous medium, the Knudsen equation can be given as[11]

    N = − ( k μ p a + p b 2 + D K e f f ) 1 R g T p b − p a L , {\displaystyle N=-\left({\frac {k}{\mu }}{\frac {p_{a}+p_{b}}{2}}+D_{\mathrm {K} }^{\mathrm {eff} }\right){\frac {1}{R_{\mathrm {g} }T}}{\frac {p_{\mathrm {b} }-p_{\mathrm {a} }}{L}}\,,}

where N is the molar flux, Rg is the gas constant, T is the temperature, Deff
K is the effective Knudsen diffusivity of the porous media. The model can also be derived from the first-principle-based binary friction model (BFM).[12][13] The differential equation of transition flow in porous media based on BFM is given as[12]

    ∂ p ∂ x = − R g T ( k p μ + D K ) − 1 N . {\displaystyle {\frac {\partial p}{\partial x}}=-R_{\mathrm {g} }T\left({\frac {kp}{\mu }}+D_{\mathrm {K} }\right)^{-1}N\,.}

This equation is valid for capillaries as well as porous media. The terminology of the Knudsen effect and Knudsen diffusivity is more common in mechanical and chemical engineering. In geological and petrochemical engineering, this effect is known as the Klinkenberg effect. Using the definition of molar flux, the above equation can be rewritten as

    ∂ p ∂ x = − R g T ( k p μ + D K ) − 1 p R g T q . {\displaystyle {\frac {\partial p}{\partial x}}=-R_{\mathrm {g} }T\left({\frac {kp}{\mu }}+D_{\mathrm {K} }\right)^{-1}{\dfrac {p}{R_{\mathrm {g} }T}}q\,.}

This equation can be rearranged into the following equation

    q = − k μ ( 1 + D K μ k 1 p ) ∂ p ∂ x . {\displaystyle q=-{\frac {k}{\mu }}\left(1+{\frac {D_{\mathrm {K} }\mu }{k}}{\frac {1}{p}}\right){\frac {\partial p}{\partial x}}\,.}

Comparing this equation with conventional Darcy's law, a new formulation can be given as

    q = − k e f f μ ∂ p ∂ x , {\displaystyle q=-{\frac {k^{\mathrm {eff} }}{\mu }}{\frac {\partial p}{\partial x}}\,,}

where

    k e f f = k ( 1 + D K μ k 1 p ) . {\displaystyle k^{\mathrm {eff} }=k\left(1+{\frac {D_{\mathrm {K} }\mu }{k}}{\frac {1}{p}}\right)\,.}

This is equivalent to the effective permeability formulation proposed by Klinkenberg:[14]

    k e f f = k ( 1 + b p ) . {\displaystyle k^{\mathrm {eff} }=k\left(1+{\frac {b}{p}}\right)\,.}

where b is known as the Klinkenberg parameter, which depends on the gas and the porous medium structure. This is quite evident if we compare the above formulations. The Klinkenberg parameter b is dependent on permeability, Knudsen diffusivity and viscosity (i.e., both gas and porous medium properties).
Darcy's law for short time scales

For very short time scales, a time derivative of flux may be added to Darcy's law, which results in valid solutions at very small times (in heat transfer, this is called the modified form of Fourier's law),

    τ ∂ q ∂ t + q = − k ∇ h , {\displaystyle \tau {\frac {\partial q}{\partial t}}+q=-k\nabla h\,,}

where τ is a very small time constant which causes this equation to reduce to the normal form of Darcy's law at "normal" times (> nanoseconds). The main reason for doing this is that the regular groundwater flow equation (diffusion equation) leads to singularities at constant head boundaries at very small times. This form is more mathematically rigorous but leads to a hyperbolic groundwater flow equation, which is more difficult to solve and is only useful at very small times, typically out of the realm of practical use.
Brinkman form of Darcy's law

Another extension to the traditional form of Darcy's law is the Brinkman term, which is used to account for transitional flow between boundaries (introduced by Brinkman in 1949[15]),

    − β ∇ 2 q + q = − k μ ∇ p , {\displaystyle -\beta \nabla ^{2}q+q=-{\frac {k}{\mu }}\nabla p\,,}

where β is an effective viscosity term. This correction term accounts for flow through medium where the grains of the media are porous themselves, but is difficult to use, and is typically neglected.
Validity of Darcy's law

Darcy's law is valid for laminar flow through sediments. In fine-grained sediments, the dimensions of interstices are small and thus flow is laminar. Coarse-grained sediments also behave similarly but in very coarse-grained sediments the flow may be turbulent.[16] Hence Darcy's law is not always valid in such sediments. For flow through commercial circular pipes, the flow is laminar when Reynolds number is less than 2000 and turbulent when it is more than 4000, but in some sediments, it has been found that flow is laminar when the value of Reynolds number is less than 1.[17]
See also

    The darcy, a unit of fluid permeability
    Hydrogeology
    Groundwater flow equation
    Mathematical model
    Black-oil equations

References

Darcy, H. (1856). Les fontaines publiques de la ville de Dijon. Paris: Dalmont.
Whitaker, S. (1986). "Flow in porous media I: A theoretical derivation of Darcy's law". Transport in Porous Media. 1: 3–25. doi:10.1007/BF01036523. S2CID 121904058.
Read "Memorial Tributes: Volume 14" at NAP.edu. 2011. doi:10.17226/12884. ISBN 978-0-309-15218-1.
Tailoring Porous Media For Controllable Capillary Flow Journal of Colloid and Interface Science 539 (2019) 379–387
A. Varlamov and G. Balestrino, “La fisica di un buon caffè,” Il Nuovo Saggiatore 17􏰁3-4􏰀, 59–66 􏰁2001􏰀.
Gianino, Concetto. Experimental analysis of the Italian coffee pot "moka". American Journal of Physics (2007)
"Experimental investigation of steam pressure coffee extraction in a stove-top coffee maker" L. Navarini, E. Nobile, F. Pinto, A. Scheri, F. Suggi-Liverani
King, Warren. "The physics of a stove-top espresso machine". American Journal of Physics (2008)
Bejan, A. (1984). Convection Heat Transfer. John Wiley & Sons.
Cunningham, R. E.; Williams, R. J. J. (1980). Diffusion in Gases and Porous Media. New York: Plenum Press.
Carrigy, N.; Pant, L. M.; Mitra, S. K.; Secanell, M. (2013). "Knudsen diffusivity and permeability of pemfc microporous coated gas diffusion layers for different polytetrafluoroethylene loadings". Journal of the Electrochemical Society. 160 (2): F81–89. doi:10.1149/2.036302jes.
Pant, L. M.; Mitra, S. K.; Secanell, M. (2012). "Absolute permeability and Knudsen diffusivity measurements in PEMFC gas diffusion layers and micro porous layers". Journal of Power Sources. 206: 153–160. doi:10.1016/j.jpowsour.2012.01.099.
Kerkhof, P. (1996). "A modified Maxwell–Stefan model for transport through inert membranes: The binary friction model". Chemical Engineering Journal and the Biochemical Engineering Journal. 64 (3): 319–343. doi:10.1016/S0923-0467(96)03134-X.
Klinkenberg, L. J. (1941). "The permeability of porous media to liquids and gases". Drilling and Production Practice. American Petroleum Institute. pp. 200–213.
Brinkman, H. C. (1949). "A calculation of the viscous force exerted by a flowing fluid on a dense swarm of particles". Applied Scientific Research. 1: 27–34. CiteSeerX 10.1.1.454.3769. doi:10.1007/BF02120313.
Jin, Y.; Uth, M.-F.; Kuznetsov, A. V.; Herwig, H. (2 February 2015). "Numerical investigation of the possibility of macroscopic turbulence in porous media: a direct numerical simulation study". Journal of Fluid Mechanics. 766: 76–103. Bibcode:2015JFM...766...76J. doi:10.1017/jfm.2015.9. S2CID 119946306.

    Arora, K. R. (1989). Soil Mechanics and Foundation Engineering. Standard Publishers.

    vte

Hydrogeology
Physical aquifer properties	

    hydraulic head hydraulic conductivity storativity permeability porosity water content

Governing equations	

    Darcy's law Groundwater flow equation Theis equation Thiem equation Hooghoudt equation

icon Geology portal
Categories:

    WaterCivil engineeringSoil mechanicsSoil physicsHydrologyTransport phenomena

    This page was last edited on 13 June 2023, at 17:56 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Geology

Earth's interior

Atmospheric science

Earth's magnetic field

Hydrology

Ecology

Physical geography

Methodology

Earth's spheres

    Earth science breakup

See also

References

        Sources
    Further reading
    External links

Earth science

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
The rocky side of a mountain creek in Costa Rica

Earth science or geoscience includes all fields of natural science related to the planet Earth.[1] This is a branch of science dealing with the physical, chemical, and biological complex constitutions and synergistic linkages of Earth's four spheres: the biosphere, hydrosphere, atmosphere, and geosphere (or lithosphere). Earth science can be considered to be a branch of planetary science, but with a much older history.

There are reductionist and holistic approaches to Earth sciences. It is also the study of Earth and its neighbors in space. Some Earth scientists use their knowledge of the planet to locate and develop energy and mineral resources. Others study the impact of human activity on Earth's environment, and design methods to protect the planet. Some use their knowledge about Earth processes such as volcanoes, earthquakes, and hurricanes to help protect people from these dangerous events.

Earth sciences can include the study of geology, the lithosphere, and the large-scale structure of Earth's interior, as well as the atmosphere, hydrosphere, and biosphere. Typically, Earth scientists use tools from geology, chronology, physics, chemistry, geography, biology, and mathematics to build a quantitative understanding of how Earth works and evolves. For example, meteorologists study the weather and watch for dangerous storms. Hydrologists examine water and warn of floods. Seismologists study earthquakes and try to understand where they will strike. Geologists study rocks and help to locate useful minerals. Earth scientists often work in the field—perhaps climbing mountains, exploring the seabed, crawling through caves, or wading in swamps. They measure and collect samples (such as rocks or river water), then record their findings on charts and maps.
Geology
Main article: Geology
Layers of sedimentary rock in Makhtesh Ramon

Geology is the study of the lithosphere, or Earth's surface, including the crust and rocks. It includes the physical characteristics and processes that occur in the lithosphere as well as how they are affected by geothermal energy. It incorporates aspects of chemistry, physics, and biology as elements of geology interact. Historical geology is the application of geology to interpret Earth history and how it has changed over time. Geochemistry studies the chemical components and processes of the Earth. Geophysics studies the physical properties of the Earth. Paleontology studies fossilized biological material in the lithosphere. Planetary geology studies geology as it pertains to extraterrestrial bodies. Geomorphology studies the origin of landscapes. Structural geology studies the deformation of rocks to produce mountains and lowlands. Resource geology studies how energy resources can be obtained from minerals. Environmental geology studies how pollution and contaminants affect soil and rock.[2] Mineralogy is the study of minerals. It includes the study of mineral formation, crystal structure, hazards associated with minerals, and the physical and chemical properties of minerals.[3] Petrology is the study of rocks, including the formation and composition of rocks. Petrography is a branch of petrology that studies the typology and classification of rocks.[4]
Earth's interior
	
This section may contain material unrelated or insufficiently related to the topic of the article. Please help improve this section or discuss this issue on the talk page. (August 2022) (Learn how and when to remove this template message)
A volcanic eruption is the release of stored energy from below Earth's surface.[5]
Main article: Structure of Earth

Plate tectonics, mountain ranges, volcanoes, and earthquakes are geological phenomena that can be explained in terms of physical and chemical processes in the Earth's crust.[6] Beneath the Earth's crust lies the mantle which is heated by the radioactive decay of heavy elements. The mantle is not quite solid and consists of magma which is in a state of semi-perpetual convection. This convection process causes the lithospheric plates to move, albeit slowly. The resulting process is known as plate tectonics.[7][8][9][10] Areas of the crust where new crust is created are called divergent boundaries, those where it is brought back into the Earth are convergent boundaries and those where plates slide past each other, but no new lithospheric material is created or destroyed, are referred to as transform (or conservative) boundaries[8][10][11] Earthquakes result from the movement of the lithospheric plates, and they often occur near convergent boundaries where parts of the crust are forced into the earth as part of subduction.[12]

Plate tectonics might be thought of as the process by which the Earth is resurfaced. As the result of seafloor spreading, new crust and lithosphere is created by the flow of magma from the mantle to the near surface, through fissures, where it cools and solidifies. Through subduction, oceanic crust and lithosphere returns to the convecting mantle.[8][10][13] Volcanoes result primarily from the melting of subducted crust material. Crust material that is forced into the asthenosphere melts, and some portion of the melted material becomes light enough to rise to the surface—giving birth to volcanoes.[8][12]
Atmospheric science
	
This section may contain material unrelated or insufficiently related to the topic of the article. Please help improve this section or discuss this issue on the talk page. (August 2022) (Learn how and when to remove this template message)
Main article: Atmospheric science
The magnetosphere shields the surface of Earth from the charged particles of the solar wind.
(image not to scale.)

Atmospheric science initially developed in the late-19th century as a means to forecast the weather through meteorology, the study of weather. Atmospheric chemistry was developed in the 20th century to measure air pollution and expanded in the 1970s in response to acid rain. Climatology studies the climate and climate change.[14]

The troposphere, stratosphere, mesosphere, thermosphere, and exosphere are the five layers which make up Earth's atmosphere. 75% of the mass in the atmosphere is located within the troposphere, the lowest layer. In all, the atmosphere is made up of about 78.0% nitrogen, 20.9% oxygen, and 0.92% argon, and small amounts of other gases including CO2 and water vapor.[15] Water vapor and CO2 cause the Earth's atmosphere to catch and hold the Sun's energy through the greenhouse effect.[16] This makes Earth's surface warm enough for liquid water and life. In addition to trapping heat, the atmosphere also protects living organisms by shielding the Earth's surface from cosmic rays.[17] The magnetic field—created by the internal motions of the core—produces the magnetosphere which protects Earth's atmosphere from the solar wind.[18] As the Earth is 4.5 billion years old,[19][20] it would have lost its atmosphere by now if there were no protective magnetosphere.
Earth's magnetic field
This section is an excerpt from Earth's magnetic field.[edit]
Computer simulation of Earth's field in a period of normal polarity between reversals.[21] The lines represent magnetic field lines, blue when the field points towards the center and yellow when away. The rotation axis of Earth is centered and vertical. The dense clusters of lines are within Earth's core.[22]

Earth's magnetic field, also known as the geomagnetic field, is the magnetic field that extends from Earth's interior out into space, where it interacts with the solar wind, a stream of charged particles emanating from the Sun. The magnetic field is generated by electric currents due to the motion of convection currents of a mixture of molten iron and nickel in Earth's outer core: these convection currents are caused by heat escaping from the core, a natural process called a geodynamo.

The magnitude of Earth's magnetic field at its surface ranges from 25 to 65 μT (0.25 to 0.65 G).[23] As an approximation, it is represented by a field of a magnetic dipole currently tilted at an angle of about 11° with respect to Earth's rotational axis, as if there were an enormous bar magnet placed at that angle through the center of Earth. The North geomagnetic pole actually represents the South pole of Earth's magnetic field, and conversely the South geomagnetic pole corresponds to the north pole of Earth's magnetic field (because opposite magnetic poles attract and the north end of a magnet, like a compass needle, points toward Earth's South magnetic field, i.e., the North geomagnetic pole near the Geographic North Pole). As of 2015, the North geomagnetic pole was located on Ellesmere Island, Nunavut, Canada.

While the North and South magnetic poles are usually located near the geographic poles, they slowly and continuously move over geological time scales, but sufficiently slowly for ordinary compasses to remain useful for navigation. However, at irregular intervals averaging several hundred thousand years, Earth's field reverses and the North and South Magnetic Poles respectively, abruptly switch places. These reversals of the geomagnetic poles leave a record in rocks that are of value to paleomagnetists in calculating geomagnetic fields in the past. Such information in turn is helpful in studying the motions of continents and ocean floors in the process of plate tectonics.
The magnetosphere is the region above the ionosphere that is defined by the extent of Earth's magnetic field in space. It extends several tens of thousands of kilometres into space, protecting Earth from the charged particles of the solar wind and cosmic rays that would otherwise strip away the upper atmosphere, including the ozone layer that protects Earth from harmful ultraviolet radiation.
Hydrology
Main article: Hydrology
Movement of water through the water cycle

Hydrology is the study of the hydrosphere and the movement of water on Earth. It emphasizes the study of how humans use and interact with freshwater supplies. Study of water's movement is closely related to geomorphology and other branches of Earth science. Applied hydrology involves engineering to maintain aquatic environments and distribute water supplies. Subdisciplines of hydrology include oceanography, hydrogeology, ecohydrology, and glaciology. Oceanography is the study of oceans.[24] Hydrogeology is the study of groundwater. It includes the mapping of groundwater supplies and the analysis of groundwater contaminants. Applied hydrogeology seeks to prevent contamination of groundwater and mineral springs and make it available as drinking water. The earliest exploitation of groundwater resources dates back to 3000 BC, and hydrogeology as a science was developed by hydrologists beginning in the 17th century.[25] Ecohydrology is the study of ecological systems in the hydrosphere. It can be divided into the physical study of aquatic ecosystems and the biological study of aquatic organisms. Ecohydrology includes the effects that organisms and aquatic ecosystems have on one another as well as how these ecoystems are affected by humans.[26] Glaciology is the study of the cryosphere, including glaciers and coverage of the Earth by ice and snow. Concerns of glaciology include access to glacial freshwater, mitigation of glacial hazards, obtaining resources that exist beneath frozen land, and addressing the effects of climate change on the cryosphere.[27]
Ecology
Main article: Ecology

Ecology is the study of the biosphere. This includes the study of nature and of how living things interact with the Earth and one another. It considers how living things use resources such as oxygen, water, and nutrients from the Earth to sustain themselves. It also considers how humans and other living creatures cause changes to nature.[28]
Physical geography
Main article: Physical geography

Physical geography is the study of Earth's systems and how they interact with one another as part of a single self-contained system. It incorporates astronomy, mathematical geography, meteorology, climatology, geology, geomorphology, biology, biogeography, pedology, and soils geography. Physical geography is distinct from human geography, which studies the human populations on Earth, though it does include human effects on the environment.[29]
Methodology
	
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (August 2022) (Learn how and when to remove this template message)

Methodologies vary depending on the nature of the subjects being studied. Studies typically fall into one of three categories: observational, experimental, or theoretical. Earth scientists often conduct sophisticated computer analysis or visit an interesting location to study earth phenomena (e.g. Antarctica or hot spot island chains).

A foundational idea in Earth science is the notion of uniformitarianism, which states that "ancient geologic features are interpreted by understanding active processes that are readily observed." In other words, any geologic processes at work in the present have operated in the same ways throughout geologic time. This enables those who study Earth history to apply knowledge of how the Earth's processes operate in the present to gain insight into how the planet has evolved and changed throughout long history.
Earth's spheres
Nature timeline
This box:

    viewtalkedit

−13 —
–
−12 —
–
−11 —
–
−10 —
–
−9 —
–
−8 —
–
−7 —
–
−6 —
–
−5 —
–
−4 —
–
−3 —
–
−2 —
–
−1 —
–
0 —
	
Dark Ages
Reionization
Matter-dominated
era
Accelerated expansion
Water on Earth
Single-celled life
Photosynthesis
Multicellular
life
Vertebrates
	
←	
Earliest Universe
←	
Earliest stars
←	
Earliest galaxy
←	
Earliest quasar / black hole
←	
Omega Centauri
←	
Andromeda Galaxy
←	
Milky Way spirals
←	
NGC 188 star cluster
←	
Alpha Centauri
←	
Earth / Solar System
←	
Earliest known life
←	
Earliest oxygen
←	
Atmospheric oxygen
←	
Sexual reproduction
←	
Earliest fungi
←	
Earliest animals / plants
←	
Cambrian explosion
←	
Earliest mammals
←	
Earliest apes / humans
L
i
f
e
(billion years ago)
	
This article is in list format but may read better as prose. You can help by converting this article, if appropriate. Editing help is available. (August 2022)

Earth science generally recognizes four spheres, the lithosphere, the hydrosphere, the atmosphere, and the biosphere;[30] these correspond to rocks, water, air and life. Also included by some are the cryosphere (corresponding to ice) as a distinct portion of the hydrosphere and the pedosphere (corresponding to soil) as an active and intermixed sphere. The following fields of science are generally categorized within the Earth sciences:

    Geology describes the rocky parts of the Earth's crust (or lithosphere) and its historic development. Major subdisciplines are mineralogy and petrology, geomorphology, paleontology, stratigraphy, structural geology, engineering geology, and sedimentology.[31][32]
    Physical geography focuses on geography as an Earth science. Physical geography is the study of Earth's seasons, climate, atmosphere, soil, streams, landforms, and oceans. Physical geography can be divided into several branches or related fields, as follows: geomorphology, biogeography, environmental geography, palaeogeography, climatology, meteorology, coastal geography, hydrology, ecology, glaciology.[citation needed]
    Geophysics and geodesy investigate the shape of the Earth, its reaction to forces and its magnetic and gravity fields. Geophysicists explore the Earth's core and mantle as well as the tectonic and seismic activity of the lithosphere.[32][33][34] Geophysics is commonly used to supplement the work of geologists in developing a comprehensive understanding of crustal geology, particularly in mineral and petroleum exploration. Seismologists use geophysics to understand plate tectonic movement, as well as predict seismic activity.
    Geochemistry is defined as the study of the processes that control the abundance, composition, and distribution of chemical compounds and isotopes in geologic environments. Geochemists use the tools and principles of chemistry to study the composition, structure, processes, and other physical aspects of the Earth. Major subdisciplines are aqueous geochemistry, cosmochemistry, isotope geochemistry and biogeochemistry.
    Soil science covers the outermost layer of the Earth's crust that is subject to soil formation processes (or pedosphere).[35] Major subdivisions in this field of study include edaphology and pedology.[36]
    Ecology covers the interactions between organisms and their environment. This field of study differentiates the study of Earth from the study of other planets in the Solar System, Earth being its only planet teeming with life.
    Hydrology, oceanography and limnology are studies which focus on the movement, distribution, and quality of the water and involves all the components of the hydrologic cycle on the Earth and its atmosphere (or hydrosphere). "Sub-disciplines of hydrology include hydrometeorology, surface water hydrology, hydrogeology, watershed science, forest hydrology, and water chemistry."[37]
    Glaciology covers the icy parts of the Earth (or cryosphere).
    Atmospheric sciences cover the gaseous parts of the Earth (or atmosphere) between the surface and the exosphere (about 1000 km). Major subdisciplines include meteorology, climatology, atmospheric chemistry, and atmospheric physics.

Earth science breakup
Main article: Outline of Earth sciences

Atmosphere

    Atmospheric chemistry
    Geography
        Climatology
        Meteorology
    Hydrometeorology
    Paleoclimatology

Biosphere

    Biogeochemistry
    Biogeography
    Ecology
        Landscape ecology
    Geoarchaeology
    Geomicrobiology
    Paleontology
        Palynology
        Micropaleontology

Hydrosphere

    Hydrology
        Hydrogeology
    Limnology (freshwater science)
    Oceanography (marine science)
        Chemical oceanography
        Physical oceanography
        Biological oceanography (marine biology)
        Geological oceanography (marine geology)
            Paleoceanography

Lithosphere (geosphere)

    Geology
        Economic geology
        Engineering geology
        Environmental geology
        Forensic geology
        Historical geology
            Quaternary geology
        Planetary geology and planetary geography
        Sedimentology
        Stratigraphy
        Structural geology
    Geography
        Human geography
        Physical geography
    Geochemistry
    Geomorphology
    Geophysics
        Geochronology
        Geodynamics (see also Tectonics)
        Geomagnetism
        Gravimetry (also part of Geodesy)
        Seismology
    Glaciology
    Hydrogeology
    Mineralogy
        Crystallography
        Gemology
    Petrology
        Petrophysics
    Speleology
    Volcanology

Pedosphere

    Geography
    Soil science
        Edaphology
        Pedology

Systems

    Earth system science
    Environmental science
    Geography
        Human geography
        Physical geography
    Gaia hypothesis
    Systems ecology
    Systems geology

Others

    Geography
        Cartography
        Geoinformatics (GIScience)
        Geostatistics
        Geodesy and Surveying
        Remote Sensing
        Hydrography
    Nanogeoscience

See also

    Earth sciences portaliconEnvironment portaliconEcology portalWorld portal

    American Geosciences Institute
    Earth sciences graphics software
    Glossary of geology terms
    List of Earth scientists
    List of geoscience organizations
    List of unsolved problems in geoscience
    Making North America (2015 PBS film)
    National Association of Geoscience Teachers
    Solid-earth science
    Science tourism
    Structure of the Earth

References

"Earth sciences | Definition, Topics, & Facts | Britannica". www.britannica.com. Retrieved 2023-08-19.
Smith & Pun 2006, pp. 14–16.
Haldar 2020, p. 109.
Haldar 2020, p. 145.
Encyclopedia of Volcanoes, Academic Press, London, 2000
"Earth's Energy Budget". ou.edu. Archived from the original on 2008-08-27. Retrieved 2007-06-20.
Simison 2007, paragraph 7
Adams & Lambert 2006, pp. 94–95, 100, 102
Smith & Pun 2006, pp. 13–17, 218, G-6
Oldroyd 2006, pp. 101, 103, 104
Smith & Pun 2006, p. 331
Smith & Pun 2006, pp. 325–26, 329
Smith & Pun 2006, p. 327
Wallace, John M.; Hobbs, Peter V. (2006). Atmospheric Science: An Introductory Survey (2nd ed.). Elsevier Science. pp. 1–3. ISBN 9780080499536.
Adams & Lambert 2006, pp. 107–08
American Heritage, p. 770
Parker, Eugene (March 2006), Shielding Space (PDF), Scientific American, archived from the original (PDF) on 2016-01-01, retrieved 2017-05-24
Adams & Lambert 2006, pp. 21–22
Smith & Pun 2006, p. 183
"How Did Scientists Calculate the Age of Earth?". education.nationalgeographic.org. Retrieved 2023-08-19.
Glatzmaier, Gary A.; Roberts, Paul H. (1995). "A three-dimensional self-consistent computer simulation of a geomagnetic field reversal". Nature. 377 (6546): 203–209. Bibcode:1995Natur.377..203G. doi:10.1038/377203a0. S2CID 4265765.
Glatzmaier, Gary. "The Geodynamo". University of California Santa Cruz. Retrieved 20 October 2013.
Finlay, C. C.; Maus, S.; Beggan, C. D.; Bondar, T. N.; Chambodut, A.; Chernova, T. A.; Chulliat, A.; Golovkov, V. P.; Hamilton, B.; Hamoudi, M.; Holme, R.; Hulot, G.; Kuang, W.; Langlais, B.; Lesur, V.; Lowes, F. J.; Lühr, H.; Macmillan, S.; Mandea, M.; McLean, S.; Manoj, C.; Menvielle, M.; Michaelis, I.; Olsen, N.; Rauberg, J.; Rother, M.; Sabaka, T. J.; Tangborn, A.; Tøffner-Clausen, L.; Thébault, E.; Thomson, A. W. P.; Wardinski, I.; Wei, Z.; Zvereva, T. I. (December 2010). "International Geomagnetic Reference Field: the eleventh generation". Geophysical Journal International. 183 (3): 1216–1230. Bibcode:2010GeoJI.183.1216F. doi:10.1111/j.1365-246X.2010.04804.x.
Davie, Tim; Quinn, Nevil Wyndham (2019). Fundamentals of Hydrology (3rd ed.). Routledge. pp. 1–2. ISBN 9780203798942.
Hölting, Bernward; Coldewey, Wilhelm G. (2019). "Introduction". Hydrogeology (8th ed.). Springer. pp. 1–3. doi:10.1007/978-3-662-56375-5. ISBN 9783662563755. Archived from the original on 2022-08-16. Retrieved 2022-08-16.
Wood, Paul J.; Hannah, David M.; Sadler, Jonathan P. (2007). "Ecohydrology and Hydroecology: An Introduction". Hydroecology and Ecohydrology: Past, Present and Future. Wiley. pp. 1–6. ISBN 9780470010174.
Knight, Peter (1999). Glaciers. Taylor & Francis. p. 1. ISBN 9780748740000.
Ricklefs, Robert E.; Miller, Gary L. (2000). Ecology (4th ed.). W. H. Freeman. pp. 3–4. ISBN 9780716728290.
Petersen, James F.; Sack, Dorothy; Gabler, Robert E. (2014). Fundamentals of Physical Geography. Cengage Learning. pp. 2–3. ISBN 9781285969718.
Earth's Spheres Archived August 31, 2007, at the Wayback Machine. ©1997–2000. Wheeling Jesuit University/NASA Classroom of the Future. Retrieved November 11, 2007.
Adams & Lambert 2006, p. 20
Smith & Pun 2006, p. 5
"WordNet Search – 3.1". princeton.edu. Archived from the original on 2019-05-15. Retrieved 2010-10-13.
"NOAA National Ocean Service Education: Global Positioning Tutorial". noaa.gov. Archived from the original on 2005-05-08. Retrieved 2007-11-17.
Elissa Levine, 2001, The Pedosphere As A Hub broken link?
Gardiner, Duane T. "Lecture 1 Chapter 1 Why Study Soils?". ENV320: Soil Science Lecture Notes. Texas A&M University-Kingsville. Archived from the original on 2018-02-09. Retrieved 2019-01-07.

    Craig, Kendall. "Hydrology of the Watershed". Archived from the original on 2017-01-11. Retrieved 2017-04-04.

Sources

    Adams, Simon; Lambert, David (2006). Earth Science: An illustrated guide to science. New York: Chelsea House. ISBN 978-0-8160-6164-8.
    Haldar, S. K. (2020). Introduction to Mineralogy and Petrology (2nd ed.). Elsevier Science. ISBN 9780323851367.
    American Heritage dictionary of the English language (4th ed.). Boston: Houghton Mifflin Company. 1992. ISBN 978-0-395-82517-4.
    Simison, W. Brian (2007-02-05). "The mechanism behind plate tectonics". Archived from the original on 2007-11-12. Retrieved 2007-11-17.
    Smith, Gary A.; Pun, Aurora (2006). How Does the Earth Work? Physical Geology and the Process of Science. Upper Saddle River, NJ: Pearson Prentice Hall. ISBN 978-0-13-034129-7.
    Oldroyd, David (2006). Earth Cycles: A historical perspective. Westport, CT: Greenwood Press. ISBN 978-0-313-33229-6.

Further reading

    Allaby M., 2008. Dictionary of Earth Sciences, Oxford University Press, ISBN 978-0-19-921194-4
    Korvin G., 1998. Fractal Models in the Earth Sciences, Elsvier, ISBN 978-0-444-88907-2
    "Earth's Energy Budget". Oklahoma Climatological Survey. 1996–2004. Archived from the original on 2007-11-17. Retrieved 2007-11-17.
    Miller, George A.; Christiane Fellbaum; and Randee Tengi; and Pamela Wakefield; and Rajesh Poddar; and Helen Langone; Benjamin Haskell (2006). "WordNet Search 3.0". WordNet a lexical database for the English language. Princeton, NJ: Princeton University/Cognitive Science Laboratory. Archived from the original on 2011-01-01. Retrieved 2007-11-10.
    "NOAA National Ocean Service Education: Geodesy". National Oceanic and Atmospheric Administration. 2005-03-08. Archived from the original on 2005-05-08. Retrieved 2007-11-17.
    Reed, Christina (2008). Earth Science: Decade by Decade. New York: Facts on File. ISBN 978-0-8160-5533-3.
    Tarbuck E. J., Lutgens F. K., and Tasa D., 2002. Earth Science, Prentice Hall, ISBN 978-0-13-035390-0

External links
Wikimedia Commons has media related to Earth sciences.

    Earth Science Picture of the Day, a service of Universities Space Research Association, sponsored by NASA Goddard Space Flight Center.
    Geoethics in Planetary and Space Exploration.
    Geology Buzz: Earth Science

    vte

Earth science

    vte

Natural science

    vte

Earth

    vte

Elements of nature
Authority control Edit this at Wikidata
Categories:

    Earth sciencesEarthPlanetary scienceScience-related lists

    This page was last edited on 22 August 2023, at 07:33 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
History

Scope

Microscopic origins

Hydraulic analogy

Circuit analysis

    Resistive circuits
    Reactive circuits with time-varying signals
    Linear approximations

Temperature effects

Relation to heat conductions

Other versions

        Magnetic effects
        Conductive fluids
    See also
    References
    External links and further reading

Ohm's law

    Article
    Talk

    Read
    View source
    View history

Tools

Page semi-protected
From Wikipedia, the free encyclopedia
This article is about the law related to electricity. For other uses, see Ohm's acoustic law.
V, I, and R, the parameters of Ohm's law
Articles about
Electromagnetism
Solenoid

    Electricity Magnetism Optics History Textbooks

Electrostatics
Magnetostatics
Electrodynamics
Electrical network

    Alternating current Capacitance Direct current Electric current Electrolysis Current density Joule heating Electromotive force Impedance Inductance Ohm's law Parallel circuit Resistance Resonant cavities Series circuit Voltage Waveguides

Magnetic circuit
Covariant formulation
Scientists

    vte

Ohm's law states that the current through a conductor between two points is directly proportional to the voltage across the two points. Introducing the constant of proportionality, the resistance,[1] one arrives at the three mathematical equations used to describe this relationship:[2]

V = I R or I = V R or R = V I
{\displaystyle V=IR\quad {\text{or}}\quad I={\frac {V}{R}}\quad {\text{or}}\quad R={\frac {V}{I}}}

where I is the current through the conductor, V is the voltage measured across the conductor and R is the resistance of the conductor. More specifically, Ohm's law states that the R in this relation is constant, independent of the current.[3] If the resistance is not constant, the previous equation cannot be called Ohm's law, but it can still be used as a definition of static/DC resistance.[4] Ohm's law is an empirical relation which accurately describes the conductivity of the vast majority of electrically conductive materials over many orders of magnitude of current. However some materials do not obey Ohm's law; these are called non-ohmic.

The law was named after the German physicist Georg Ohm, who, in a treatise published in 1827, described measurements of applied voltage and current through simple electrical circuits containing various lengths of wire. Ohm explained his experimental results by a slightly more complex equation than the modern form above (see § History below).

In physics, the term Ohm's law is also used to refer to various generalizations of the law; for example the vector form of the law used in electromagnetics and material science:

J = σ E ,
{\displaystyle \mathbf {J} =\sigma \mathbf {E} ,}

where J is the current density at a given location in a resistive material, E is the electric field at that location, and σ (sigma) is a material-dependent parameter called the conductivity. This reformulation of Ohm's law is due to Gustav Kirchhoff.[5]
History
Georg Ohm

In January 1781, before Georg Ohm's work, Henry Cavendish experimented with Leyden jars and glass tubes of varying diameter and length filled with salt solution. He measured the current by noting how strong a shock he felt as he completed the circuit with his body. Cavendish wrote that the "velocity" (current) varied directly as the "degree of electrification" (voltage). He did not communicate his results to other scientists at the time,[6] and his results were unknown until Maxwell published them in 1879.[7]

Francis Ronalds delineated "intensity" (voltage) and "quantity" (current) for the dry pile—a high voltage source—in 1814 using a gold-leaf electrometer. He found for a dry pile that the relationship between the two parameters was not proportional under certain meteorological conditions.[8][9]

Ohm did his work on resistance in the years 1825 and 1826, and published his results in 1827 as the book Die galvanische Kette, mathematisch bearbeitet ("The galvanic circuit investigated mathematically").[10] He drew considerable inspiration from Fourier's work on heat conduction in the theoretical explanation of his work. For experiments, he initially used voltaic piles, but later used a thermocouple as this provided a more stable voltage source in terms of internal resistance and constant voltage. He used a galvanometer to measure current, and knew that the voltage between the thermocouple terminals was proportional to the junction temperature. He then added test wires of varying length, diameter, and material to complete the circuit. He found that his data could be modeled through the equation
x = a b + ℓ ,
{\displaystyle x={\frac {a}{b+\ell }},}
where x was the reading from the galvanometer, ℓ was the length of the test conductor, a depended on the thermocouple junction temperature, and b was a constant of the entire setup. From this, Ohm determined his law of proportionality and published his results.

Internal resistance model

In modern notation we would write,
I = E r + R ,
{\displaystyle I={\frac {\mathcal {E}}{r+R}},}
where E {\mathcal {E}} is the open-circuit emf of the thermocouple, r r is the internal resistance of the thermocouple and R R is the resistance of the test wire. In terms of the length of the wire this becomes,
I = E r + R ℓ ,
{\displaystyle I={\frac {\mathcal {E}}{r+{\mathcal {R}}\ell }},}
where R \mathcal R is the resistance of the test wire per unit length. Thus, Ohm's coefficients are,
a = E R , b = r R .
{\displaystyle a={\frac {\mathcal {E}}{\mathcal {R}}},\quad b={\frac {\mathcal {r}}{\mathcal {R}}}.}

Ohm's law in Georg Ohm's lab book.

Ohm's law was probably the most important of the early quantitative descriptions of the physics of electricity. We consider it almost obvious today. When Ohm first published his work, this was not the case; critics reacted to his treatment of the subject with hostility. They called his work a "web of naked fancies"[11] and the Minister of Education proclaimed that "a professor who preached such heresies was unworthy to teach science."[12] The prevailing scientific philosophy in Germany at the time asserted that experiments need not be performed to develop an understanding of nature because nature is so well ordered, and that scientific truths may be deduced through reasoning alone.[13] Also, Ohm's brother Martin, a mathematician, was battling the German educational system. These factors hindered the acceptance of Ohm's work, and his work did not become widely accepted until the 1840s. However, Ohm received recognition for his contributions to science well before he died.

In the 1850s, Ohm's law was widely known and considered proved. Alternatives such as "Barlow's law", were discredited, in terms of real applications to telegraph system design, as discussed by Samuel F. B. Morse in 1855.[14]

The electron was discovered in 1897 by J. J. Thomson, and it was quickly realized that it is the particle (charge carrier) that carries electric currents in electric circuits. In 1900 the first (classical) model of electrical conduction, the Drude model, was proposed by Paul Drude, which finally gave a scientific explanation for Ohm's law. In this model, a solid conductor consists of a stationary lattice of atoms (ions), with conduction electrons moving randomly in it. A voltage across a conductor causes an electric field, which accelerates the electrons in the direction of the electric field, causing a drift of electrons which is the electric current. However the electrons collide with atoms which causes them to scatter and randomizes their motion, thus converting kinetic energy to heat (thermal energy). Using statistical distributions, it can be shown that the average drift velocity of the electrons, and thus the current, is proportional to the electric field, and thus the voltage, over a wide range of voltages.

The development of quantum mechanics in the 1920s modified this picture somewhat, but in modern theories the average drift velocity of electrons can still be shown to be proportional to the electric field, thus deriving Ohm's law. In 1927 Arnold Sommerfeld applied the quantum Fermi-Dirac distribution of electron energies to the Drude model, resulting in the free electron model. A year later, Felix Bloch showed that electrons move in waves (Bloch electrons) through a solid crystal lattice, so scattering off the lattice atoms as postulated in the Drude model is not a major process; the electrons scatter off impurity atoms and defects in the material. The final successor, the modern quantum band theory of solids, showed that the electrons in a solid cannot take on any energy as assumed in the Drude model but are restricted to energy bands, with gaps between them of energies that electrons are forbidden to have. The size of the band gap is a characteristic of a particular substance which has a great deal to do with its electrical resistivity, explaining why some substances are electrical conductors, some semiconductors, and some insulators.

While the old term for electrical conductance, the mho (the inverse of the resistance unit ohm), is still used, a new name, the siemens, was adopted in 1971, honoring Ernst Werner von Siemens. The siemens is preferred in formal papers.

In the 1920s, it was discovered that the current through a practical resistor actually has statistical fluctuations, which depend on temperature, even when voltage and resistance are exactly constant; this fluctuation, now known as Johnson–Nyquist noise, is due to the discrete nature of charge. This thermal effect implies that measurements of current and voltage that are taken over sufficiently short periods of time will yield ratios of V/I that fluctuate from the value of R implied by the time average or ensemble average of the measured current; Ohm's law remains correct for the average current, in the case of ordinary resistive materials.

Ohm's work long preceded Maxwell's equations and any understanding of frequency-dependent effects in AC circuits. Modern developments in electromagnetic theory and circuit theory do not contradict Ohm's law when they are evaluated within the appropriate limits.
Scope

Ohm's law is an empirical law, a generalization from many experiments that have shown that current is approximately proportional to electric field for most materials. It is less fundamental than Maxwell's equations and is not always obeyed. Any given material will break down under a strong-enough electric field, and some materials of interest in electrical engineering are "non-ohmic" under weak fields.[15][16]

Ohm's law has been observed on a wide range of length scales. In the early 20th century, it was thought that Ohm's law would fail at the atomic scale, but experiments have not borne out this expectation. As of 2012, researchers have demonstrated that Ohm's law works for silicon wires as small as four atoms wide and one atom high.[17]
Microscopic origins
Drude Model electrons (shown here in blue) constantly bounce among heavier, stationary crystal ions (shown in red).
Main article: Drude model

The dependence of the current density on the applied electric field is essentially quantum mechanical in nature; (see Classical and quantum conductivity.) A qualitative description leading to Ohm's law can be based upon classical mechanics using the Drude model developed by Paul Drude in 1900.[18][19]

The Drude model treats electrons (or other charge carriers) like pinballs bouncing among the ions that make up the structure of the material. Electrons will be accelerated in the opposite direction to the electric field by the average electric field at their location. With each collision, though, the electron is deflected in a random direction with a velocity that is much larger than the velocity gained by the electric field. The net result is that electrons take a zigzag path due to the collisions, but generally drift in a direction opposing the electric field.

The drift velocity then determines the electric current density and its relationship to E and is independent of the collisions. Drude calculated the average drift velocity from p = −eEτ where p is the average momentum, −e is the charge of the electron and τ is the average time between the collisions. Since both the momentum and the current density are proportional to the drift velocity, the current density becomes proportional to the applied electric field; this leads to Ohm's law.
Hydraulic analogy

A hydraulic analogy is sometimes used to describe Ohm's law. Water pressure, measured by pascals (or PSI), is the analog of voltage because establishing a water pressure difference between two points along a (horizontal) pipe causes water to flow. The water volume flow rate, as in liters per second, is the analog of current, as in coulombs per second. Finally, flow restrictors—such as apertures placed in pipes between points where the water pressure is measured—are the analog of resistors. We say that the rate of water flow through an aperture restrictor is proportional to the difference in water pressure across the restrictor. Similarly, the rate of flow of electrical charge, that is, the electric current, through an electrical resistor is proportional to the difference in voltage measured across the resistor. More generally, the hydraulic head may be taken as the analog of voltage, and Ohm's law is then analogous to Darcy's law which relates hydraulic head to the volume flow rate via the hydraulic conductivity.

Flow and pressure variables can be calculated in fluid flow network with the use of the hydraulic ohm analogy.[20][21] The method can be applied to both steady and transient flow situations. In the linear laminar flow region, Poiseuille's law describes the hydraulic resistance of a pipe, but in the turbulent flow region the pressure–flow relations become nonlinear.

The hydraulic analogy to Ohm's law has been used, for example, to approximate blood flow through the circulatory system.[22]
Circuit analysis
Covering the unknown in the Ohm's law image mnemonic gives the formula in terms of the remaining parameters
Ohm's law wheel with international unit symbols

In circuit analysis, three equivalent expressions of Ohm's law are used interchangeably:

I = V R or V = I R or R = V I .
{\displaystyle I={\frac {V}{R}}\quad {\text{or}}\quad V=IR\quad {\text{or}}\quad R={\frac {V}{I}}.}

Each equation is quoted by some sources as the defining relationship of Ohm's law,[2][23][24] or all three are quoted,[25] or derived from a proportional form,[26] or even just the two that do not correspond to Ohm's original statement may sometimes be given.[27][28]

The interchangeability of the equation may be represented by a triangle, where V (voltage) is placed on the top section, the I (current) is placed to the left section, and the R (resistance) is placed to the right. The divider between the top and bottom sections indicates division (hence the division bar).

Resistive circuits

Resistors are circuit elements that impede the passage of electric charge in agreement with Ohm's law, and are designed to have a specific resistance value R. In schematic diagrams, a resistor is shown as a long rectangle or zig-zag symbol. An element (resistor or conductor) that behaves according to Ohm's law over some operating range is referred to as an ohmic device (or an ohmic resistor) because Ohm's law and a single value for the resistance suffice to describe the behavior of the device over that range.

Ohm's law holds for circuits containing only resistive elements (no capacitances or inductances) for all forms of driving voltage or current, regardless of whether the driving voltage or current is constant (DC) or time-varying such as AC. At any instant of time Ohm's law is valid for such circuits.

Resistors which are in series or in parallel may be grouped together into a single "equivalent resistance" in order to apply Ohm's law in analyzing the circuit.
Reactive circuits with time-varying signals

When reactive elements such as capacitors, inductors, or transmission lines are involved in a circuit to which AC or time-varying voltage or current is applied, the relationship between voltage and current becomes the solution to a differential equation, so Ohm's law (as defined above) does not directly apply since that form contains only resistances having value R, not complex impedances which may contain capacitance (C) or inductance (L).

Equations for time-invariant AC circuits take the same form as Ohm's law. However, the variables are generalized to complex numbers and the current and voltage waveforms are complex exponentials.[29]

In this approach, a voltage or current waveform takes the form Aest, where t is time, s is a complex parameter, and A is a complex scalar. In any linear time-invariant system, all of the currents and voltages can be expressed with the same s parameter as the input to the system, allowing the time-varying complex exponential term to be canceled out and the system described algebraically in terms of the complex scalars in the current and voltage waveforms.

The complex generalization of resistance is impedance, usually denoted Z; it can be shown that for an inductor,
Z = s L
{\displaystyle Z=sL}
and for a capacitor,
Z = 1 s C .
{\displaystyle Z={\frac {1}{sC}}.}

We can now write,
V = Z I
{\displaystyle V=Z\,I}
where V and I are the complex scalars in the voltage and current respectively and Z is the complex impedance.

This form of Ohm's law, with Z taking the place of R, generalizes the simpler form. When Z is complex, only the real part is responsible for dissipating heat.

In a general AC circuit, Z varies strongly with the frequency parameter s, and so also will the relationship between voltage and current.

For the common case of a steady sinusoid, the s parameter is taken to be j ω j\omega , corresponding to a complex sinusoid A e   j ω t Ae^{{\mbox{ }}j\omega t}. The real parts of such complex current and voltage waveforms describe the actual sinusoidal currents and voltages in a circuit, which can be in different phases due to the different complex scalars.
Linear approximations
See also: Small-signal modeling and Network analysis (electrical circuits) § Small signal equivalent circuit

Ohm's law is one of the basic equations used in the analysis of electrical circuits. It applies to both metal conductors and circuit components (resistors) specifically made for this behaviour. Both are ubiquitous in electrical engineering. Materials and components that obey Ohm's law are described as "ohmic"[30] which means they produce the same value for resistance (R = V/I) regardless of the value of V or I which is applied and whether the applied voltage or current is DC (direct current) of either positive or negative polarity or AC (alternating current).

In a true ohmic device, the same value of resistance will be calculated from R = V/I regardless of the value of the applied voltage V. That is, the ratio of V/I is constant, and when current is plotted as a function of voltage the curve is linear (a straight line). If voltage is forced to some value V, then that voltage V divided by measured current I will equal R. Or if the current is forced to some value I, then the measured voltage V divided by that current I is also R. Since the plot of I versus V is a straight line, then it is also true that for any set of two different voltages V1 and V2 applied across a given device of resistance R, producing currents I1 = V1/R and I2 = V2/R, that the ratio (V1 − V2)/(I1 − I2) is also a constant equal to R. The operator "delta" (Δ) is used to represent a difference in a quantity, so we can write ΔV = V1 − V2 and ΔI = I1 − I2. Summarizing, for any truly ohmic device having resistance R, V/I = ΔV/ΔI = R for any applied voltage or current or for the difference between any set of applied voltages or currents.
The I–V curves of four devices: Two resistors, a diode, and a battery. The two resistors follow Ohm's law: The plot is a straight line through the origin. The other two devices do not follow Ohm's law.

There are, however, components of electrical circuits which do not obey Ohm's law; that is, their relationship between current and voltage (their I–V curve) is nonlinear (or non-ohmic). An example is the p–n junction diode (curve at right). As seen in the figure, the current does not increase linearly with applied voltage for a diode. One can determine a value of current (I) for a given value of applied voltage (V) from the curve, but not from Ohm's law, since the value of "resistance" is not constant as a function of applied voltage. Further, the current only increases significantly if the applied voltage is positive, not negative. The ratio V/I for some point along the nonlinear curve is sometimes called the static, or chordal, or DC, resistance,[31][32] but as seen in the figure the value of total V over total I varies depending on the particular point along the nonlinear curve which is chosen. This means the "DC resistance" V/I at some point on the curve is not the same as what would be determined by applying an AC signal having peak amplitude ΔV volts or ΔI amps centered at that same point along the curve and measuring ΔV/ΔI. However, in some diode applications, the AC signal applied to the device is small and it is possible to analyze the circuit in terms of the dynamic, small-signal, or incremental resistance, defined as the one over the slope of the V–I curve at the average value (DC operating point) of the voltage (that is, one over the derivative of current with respect to voltage). For sufficiently small signals, the dynamic resistance allows the Ohm's law small signal resistance to be calculated as approximately one over the slope of a line drawn tangentially to the V–I curve at the DC operating point.[33]
Temperature effects

Ohm's law has sometimes been stated as, "for a conductor in a given state, the electromotive force is proportional to the current produced." That is, that the resistance, the ratio of the applied electromotive force (or voltage) to the current, "does not vary with the current strength ." The qualifier "in a given state" is usually interpreted as meaning "at a constant temperature," since the resistivity of materials is usually temperature dependent. Because the conduction of current is related to Joule heating of the conducting body, according to Joule's first law, the temperature of a conducting body may change when it carries a current. The dependence of resistance on temperature therefore makes resistance depend upon the current in a typical experimental setup, making the law in this form difficult to directly verify. Maxwell and others worked out several methods to test the law experimentally in 1876, controlling for heating effects.[34]
Relation to heat conductions
See also: Conduction (heat)

Ohm's principle predicts the flow of electrical charge (i.e. current) in electrical conductors when subjected to the influence of voltage differences; Jean-Baptiste-Joseph Fourier's principle predicts the flow of heat in heat conductors when subjected to the influence of temperature differences.

The same equation describes both phenomena, the equation's variables taking on different meanings in the two cases. Specifically, solving a heat conduction (Fourier) problem with temperature (the driving "force") and flux of heat (the rate of flow of the driven "quantity", i.e. heat energy) variables also solves an analogous electrical conduction (Ohm) problem having electric potential (the driving "force") and electric current (the rate of flow of the driven "quantity", i.e. charge) variables.

The basis of Fourier's work was his clear conception and definition of thermal conductivity. He assumed that, all else being the same, the flux of heat is strictly proportional to the gradient of temperature. Although undoubtedly true for small temperature gradients, strictly proportional behavior will be lost when real materials (e.g. ones having a thermal conductivity that is a function of temperature) are subjected to large temperature gradients.

A similar assumption is made in the statement of Ohm's law: other things being alike, the strength of the current at each point is proportional to the gradient of electric potential. The accuracy of the assumption that flow is proportional to the gradient is more readily tested, using modern measurement methods, for the electrical case than for the heat case.
Other versions

Ohm's law, in the form above, is an extremely useful equation in the field of electrical/electronic engineering because it describes how voltage, current and resistance are interrelated on a "macroscopic" level, that is, commonly, as circuit elements in an electrical circuit. Physicists who study the electrical properties of matter at the microscopic level use a closely related and more general vector equation, sometimes also referred to as Ohm's law, having variables that are closely related to the V, I, and R scalar variables of Ohm's law, but which are each functions of position within the conductor. Physicists often use this continuum form of Ohm's Law:[35]

E = ρ J
{\displaystyle \mathbf {E} =\rho \mathbf {J} }

where "E" is the electric field vector with units of volts per meter (analogous to "V" of Ohm's law which has units of volts), "J" is the current density vector with units of amperes per unit area (analogous to "I" of Ohm's law which has units of amperes), and "ρ" (Greek "rho") is the resistivity with units of ohm·meters (analogous to "R" of Ohm's law which has units of ohms). The above equation is sometimes written[36] as J = σE where "σ" (Greek "sigma") is the conductivity which is the reciprocal of ρ.
Current flowing through a uniform cylindrical conductor (such as a round wire) with a uniform field applied.

The voltage between two points is defined as:[37]
Δ V = − ∫ E ⋅ d ℓ
{\displaystyle {\Delta V}=-\int {\mathbf {E} \cdot d{\boldsymbol {\ell }}}}
with d ℓ {\displaystyle d{\boldsymbol {\ell }}} the element of path along the integration of electric field vector E. If the applied E field is uniform and oriented along the length of the conductor as shown in the figure, then defining the voltage V in the usual convention of being opposite in direction to the field (see figure), and with the understanding that the voltage V is measured differentially across the length of the conductor allowing us to drop the Δ symbol, the above vector equation reduces to the scalar equation:

V = E ℓ     or     E = V ℓ .
{\displaystyle V={E}{\ell }\ \ {\text{or}}\ \ E={\frac {V}{\ell }}.}

Since the E field is uniform in the direction of wire length, for a conductor having uniformly consistent resistivity ρ, the current density J will also be uniform in any cross-sectional area and oriented in the direction of wire length, so we may write:[38]
J = I a .
{\displaystyle J={\frac {I}{a}}.}

Substituting the above 2 results (for E and J respectively) into the continuum form shown at the beginning of this section:
V ℓ = I a ρ or V = I ρ ℓ a .
{\displaystyle {\frac {V}{\ell }}={\frac {I}{a}}\rho \qquad {\text{or}}\qquad V=I\rho {\frac {\ell }{a}}.}

The electrical resistance of a uniform conductor is given in terms of resistivity by:[38]
R = ρ ℓ a
{\displaystyle {R}=\rho {\frac {\ell }{a}}}
where ℓ is the length of the conductor in SI units of meters, a is the cross-sectional area (for a round wire a = πr2 if r is radius) in units of meters squared, and ρ is the resistivity in units of ohm·meters.

After substitution of R from the above equation into the equation preceding it, the continuum form of Ohm's law for a uniform field (and uniform current density) oriented along the length of the conductor reduces to the more familiar form:
V = I R .
{\displaystyle V=IR.}

A perfect crystal lattice, with low enough thermal motion and no deviations from periodic structure, would have no resistivity,[39] but a real metal has crystallographic defects, impurities, multiple isotopes, and thermal motion of the atoms. Electrons scatter from all of these, resulting in resistance to their flow.

The more complex generalized forms of Ohm's law are important to condensed matter physics, which studies the properties of matter and, in particular, its electronic structure. In broad terms, they fall under the topic of constitutive equations and the theory of transport coefficients.
Magnetic effects

If an external B-field is present and the conductor is not at rest but moving at velocity v, then an extra term must be added to account for the current induced by the Lorentz force on the charge carriers.
J = σ ( E + v × B )
{\displaystyle \mathbf {J} =\sigma (\mathbf {E} +\mathbf {v} \times \mathbf {B} )}

In the rest frame of the moving conductor this term drops out because v = 0. There is no contradiction because the electric field in the rest frame differs from the E-field in the lab frame: E′ = E + v × B. Electric and magnetic fields are relative, see Lorentz transformation.

If the current J is alternating because the applied voltage or E-field varies in time, then reactance must be added to resistance to account for self-inductance, see electrical impedance. The reactance may be strong if the frequency is high or the conductor is coiled.
Conductive fluids

In a conductive fluid, such as a plasma, there is a similar effect. Consider a fluid moving with the velocity v \mathbf {v} in a magnetic field B \mathbf {B} . The relative motion induces an electric field E \mathbf {E} which exerts electric force on the charged particles giving rise to an electric current J \mathbf {J} . The equation of motion for the electron gas, with a number density n e n_{e}, is written as
m e n e d v e d t = − n e e E + n e m e ν ( v i − v e ) − e n e v e × B ,
{\displaystyle m_{e}n_{e}{d\mathbf {v} _{e} \over dt}=-n_{e}e\mathbf {E} +n_{e}m_{e}\nu (\mathbf {v} _{i}-\mathbf {v} _{e})-en_{e}\mathbf {v} _{e}\times \mathbf {B} ,}

where e e, m e m_{e} and v e {\displaystyle \mathbf {v} _{e}} are the charge, mass and velocity of the electrons, respectively. Also, ν \nu is the frequency of collisions of the electrons with ions which have a velocity field v i \mathbf {v} _{i}. Since, the electron has a very small mass compared with that of ions, we can ignore the left hand side of the above equation to write
σ ( E + v × B ) = J ,
{\displaystyle \sigma (\mathbf {E} +\mathbf {v} \times \mathbf {B} )=\mathbf {J} ,}

where we have used the definition of the current density, and also put σ = n e e 2 ν m e {\displaystyle \sigma ={n_{e}e^{2} \over \nu m_{e}}} which is the electrical conductivity. This equation can also be equivalently written as
E + v × B = ρ J ,
{\displaystyle \mathbf {E} +\mathbf {v} \times \mathbf {B} =\rho \mathbf {J} ,}
where ρ = σ − 1 {\displaystyle \rho =\sigma ^{-1}} is the electrical resistivity. It is also common to write η \eta instead of ρ \rho which can be confusing since it is the same notation used for the magnetic diffusivity defined as η = 1 / μ 0 σ {\displaystyle \eta =1/\mu _{0}\sigma }.

See also

    iconElectronics portal

    Fick's law of diffusion
    Hopkinson's law ("Ohm's law for magnetics")
    Maximum power transfer theorem
    Norton's theorem
    Electric power
    Sheet resistance
    Superposition theorem
    Thermal noise
    Thévenin's theorem

References

Consoliver, Earl L. & Mitchell, Grover I. (1920). Automotive Ignition Systems. McGraw-Hill. p. 4.
Millikan, Robert A.; Bishop, E. S. (1917). Elements of Electricity. American Technical Society. p. 54.
Heaviside, Oliver (1894). Electrical Papers. Vol. 1. Macmillan and Co. p. 283. ISBN 978-0-8218-2840-3.
Young, Hugh; Freedman, Roger (2008). Sears and Zemansky's University Physics: With Modern Physics. Vol. 2 (12 ed.). Pearson. p. 853. ISBN 978-0-321-50121-9.
Darrigol, Olivier (8 June 2000). Electrodynamics from Ampère to Einstein. Clarendon Press. p. 70. ISBN 9780198505945..
Fleming, John Ambrose (1911). "Electricity" . In Chisholm, Hugh (ed.). Encyclopædia Britannica. Vol. 9 (11th ed.). Cambridge University Press. p. 182.
Bordeau, Sanford P. (1982). Volts to Hertz-- the Rise of Electricity: From the Compass to the Radio Through the Works of Sixteen Great Men of Science Whose Names are Used in Measuring Electricity and Magnetism. Burgess Publishing Company. pp. 86–107. ISBN 9780808749080.
Ronalds, B. F. (2016). Sir Francis Ronalds: Father of the Electric Telegraph. London: Imperial College Press. ISBN 978-1-78326-917-4.
Ronalds, B. F. (July 2016). "Francis Ronalds (1788–1873): The First Electrical Engineer?". Proceedings of the IEEE. 104 (7): 1489–1498. doi:10.1109/JPROC.2016.2571358. S2CID 20662894.
Ohm, G. S. (1827). Die galvanische Kette, mathematisch bearbeitet (PDF). Berlin: T. H. Riemann. Archived from the original (PDF) on 2009-03-26.
Davies, Brian (1980). "A web of naked fancies?". Physics Education. 15 (1): 57–61. Bibcode:1980PhyEd..15...57D. doi:10.1088/0031-9120/15/1/314. S2CID 250832899.
Hart, Ivor Blashka (1923). Makers of Science. London: Oxford University Press. p. 243. OL 6662681M..
Schnädelbach, Herbert (14 June 1984). Philosophy in Germany 1831-1933. Cambridge University Press. pp. 78–79. ISBN 9780521296465.
Taliaferro Preston (1855). Shaffner's Telegraph Companion: Devoted to the Science and Art of the Morse Telegraph. Vol. 2. Pudney & Russell.
Purcell, Edward M. (1985), Electricity and magnetism, Berkeley Physics Course, vol. 2 (2nd ed.), McGraw-Hill, p. 129, ISBN 978-0-07-004908-6
Griffiths, David J. (1999), Introduction to electrodynamics (3rd ed.), Prentice Hall, p. 289, ISBN 978-0-13-805326-0
Weber, B.; Mahapatra, S.; Ryu, H.; Lee, S.; Fuhrer, A.; Reusch, T. C. G.; Thompson, D. L.; Lee, W. C. T.; Klimeck, G.; Hollenberg, L. C. L.; Simmons, M. Y. (2012). "Ohm's Law Survives to the Atomic Scale". Science. 335 (6064): 64–67. Bibcode:2012Sci...335...64W. doi:10.1126/science.1214319. PMID 22223802. S2CID 10873901.
Drude, Paul (1900). "Zur Elektronentheorie der Metalle". Annalen der Physik. 306 (3): 566–613. Bibcode:1900AnP...306..566D. doi:10.1002/andp.19003060312.[dead link]
Drude, Paul (1900). "Zur Elektronentheorie der Metalle; II. Teil. Galvanomagnetische und thermomagnetische Effecte". Annalen der Physik. 308 (11): 369–402. Bibcode:1900AnP...308..369D. doi:10.1002/andp.19003081102.[dead link]
A. Akers; M. Gassman & R. Smith (2006). Hydraulic Power System Analysis. New York: Taylor & Francis. Chapter 13. ISBN 978-0-8247-9956-4.
A. Esposito, "A Simplified Method for Analyzing Circuits by Analogy", Machine Design, October 1969, pp. 173–177.
Guyton, Arthur; Hall, John (2006). "Chapter 14: Overview of the Circulation; Medical Physics of Pressure, Flow, and Resistance". In Gruliow, Rebecca (ed.). Textbook of Medical Physiology (11th ed.). Philadelphia, Pennsylvania: Elsevier Inc. p. 164. ISBN 978-0-7216-0240-0.
Nilsson, James William & Riedel, Susan A. (2008). Electric circuits. Prentice Hall. p. 29. ISBN 978-0-13-198925-2.
Halpern, Alvin M. & Erlbach, Erich (1998). Schaum's outline of theory and problems of beginning physics II. McGraw-Hill Professional. p. 140. ISBN 978-0-07-025707-8.
Patrick, Dale R. & Fardo, Stephen W. (1999). Understanding DC circuits. Newnes. p. 96. ISBN 978-0-7506-7110-1.
O'Conor Sloane, Thomas (1909). Elementary electrical calculations. D. Van Nostrand Co. p. 41. "R= Ohm's law proportional."
Cumming, Linnaeus (1902). Electricity treated experimentally for the use of schools and students. Longman's Green and Co. p. 220. "V=IR Ohm's law."
Stein, Benjamin (1997). Building technology (2nd ed.). John Wiley and Sons. p. 169. ISBN 978-0-471-59319-5.
Prasad, Rajendra (2006). Fundamentals of Electrical Engineering. Prentice-Hall of India. ISBN 978-81-203-2729-0.
Hughes, E, Electrical Technology, pp10, Longmans, 1969.
Brown, Forbes T. (2006). Engineering System Dynamics. CRC Press. p. 43. ISBN 978-0-8493-9648-9.
Kaiser, Kenneth L. (2004). Electromagnetic Compatibility Handbook. CRC Press. pp. 13–52. ISBN 978-0-8493-2087-3.
Horowitz, Paul; Hill, Winfield (1989). The Art of Electronics (2nd ed.). Cambridge University Press. p. 13. ISBN 978-0-521-37095-0.
Normal Lockyer, ed. (September 21, 1876). "Reports". Nature. Macmillan Journals Ltd. 14 (360): 451–459 [452]. Bibcode:1876Natur..14..451.. doi:10.1038/014451a0.
Lerner, Lawrence S. (1977). Physics for scientists and engineers. Jones & Bartlett. p. 736. ISBN 978-0-7637-0460-5.
Seymour J, Physical Electronics, Pitman, 1972, pp. 53–54
Lerner L, Physics for scientists and engineers, Jones & Bartlett, 1997, pp. 685–686
Lerner L, Physics for scientists and engineers, Jones & Bartlett, 1997, pp. 732–733

    Seymour J, Physical Electronics, pp. 48–49, Pitman, 1972

External links and further reading
Wikimedia Commons has media related to Ohm's law.

    Ohm's Law chapter from Lessons In Electric Circuits Vol 1 DC book and series.
    John C. Shedd and Mayo D. Hershey,"The History of Ohm's Law", Popular Science, December 1913, pp. 599–614, Bonnier Corporation ISSN 0161-7370, gives the history of Ohm's investigations, prior work, Ohm's false equation in the first paper, illustration of Ohm's experimental apparatus.
    Schagrin, Morton L. (1963). "Resistance to Ohm's Law". American Journal of Physics. 31 (7): 536–547. Bibcode:1963AmJPh..31..536S. doi:10.1119/1.1969620. S2CID 120421759. Explores the conceptual change underlying Ohm's experimental work.
    Kenneth L. Caneva, "Ohm, Georg Simon." Complete Dictionary of Scientific Biography. 2008
    s:Scientific Memoirs/2/The Galvanic Circuit investigated Mathematically, a translation of Ohm's original paper.

Authority control: National Edit this at Wikidata	

    Germany Israel United States

Categories:

    Electronic engineeringCircuit theoremsEmpirical lawsEponymsElectrical resistance and conductanceVoltageGeorg Ohm

    This page was last edited on 28 August 2023, at 23:59 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
History of the theory

Fundamental forces

Classical electrodynamics

Extension to nonlinear phenomena

Quantities and units

Applications

See also

References

Further reading

        Web sources
        Textbooks
        General coverage
    External links

Electromagnetism

    Article
    Talk

    Read
    View source
    View history

Tools

Page semi-protected
From Wikipedia, the free encyclopedia
(Redirected from Electromagnetics)
For a more accessible and less technical introduction to this topic, see Introduction to electromagnetism.
"Electromagnetic" redirects here. The term may also refer to the use of an electromagnet.
"Electromagnetics" redirects here. For the academic journal, see Electromagnetics (journal).
"Electromagnetic force" redirects here. For the force exerted on particles by electromagnetic fields, see Lorentz force.
Electromagnetic interactions are responsible for the glowing filaments in this plasma globe
Articles about
Electromagnetism
Solenoid

    Electricity Magnetism Optics History Textbooks

Electrostatics
Magnetostatics
Electrodynamics
Electrical network
Magnetic circuit
Covariant formulation
Scientists

    vte

In physics, electromagnetism is an interaction that occurs between particles with electric charge via electromagnetic fields. The electromagnetic force is one of the four fundamental forces of nature. It is the dominant force in the interactions of atoms and molecules. Electromagnetism can be thought of as a combination of electrostatics and magnetism, two distinct but closely intertwined phenomena. Electromagnetic forces occur between any two charged particles, causing an attraction between particles with opposite charges and repulsion between particles with the same charge, while magnetism is an interaction that occurs exclusively between charged particles in relative motion. These two effects combine to create electromagnetic fields in the vicinity of charged particles, which can accelerate other charged particles via the Lorentz force. At high energy, the weak force and electromagnetic force are unified as a single electroweak force.

The electromagnetic force is responsible for many of the chemical and physical phenomena observed in daily life. The electrostatic attraction between atomic nuclei and their electrons holds atoms together. Electric forces also allow different atoms to combine into molecules, including the macromolecules such as proteins that form the basis of life. Meanwhile, magnetic interactions between the spin and angular momentum magnetic moments of electrons also play a role in chemical reactivity; such relationships are studied in spin chemistry. Electromagnetism also plays a crucial role in modern technology: electrical energy production, transformation and distribution; light, heat, and sound production and detection; fiber optic and wireless communication; sensors; computation; electrolysis; electroplating; and mechanical motors and actuators.

Electromagnetism has been studied since ancient times. Many ancient civilizations, including the Greeks and the Mayans created wide-ranging theories to explain lightning, static electricity, and the attraction between magnetized pieces of iron ore. However, it wasn't until the late 18th century that scientists began to develop a mathematical basis for understanding the nature of electromagnetic interactions. In the 18th and 19th centuries, prominent scientists and mathematicians such as Coulomb, Gauss and Faraday developed namesake laws which helped to explain the formation and interaction of electromagnetic fields. This process culminated in the 1860s with the discovery of Maxwell's equations, a set of four partial differential equations which provide a complete description of classical electromagnetic fields. Besides providing a sound mathematical basis for the relationships between electricity and magnetism that scientists had been exploring for centuries, Maxwell's equations also predicted the existence of self-sustaining electromagnetic waves. Maxwell postulated that such waves make up visible light, which was later shown to be true. Indeed, gamma-rays, x-rays, ultraviolet, visible, infrared radiation, microwaves and radio waves were all determined to be electromagnetic radiation differing only in their range of frequencies.

In the modern era, scientists have continued to refine the theorem of electromagnetism to take into account the effects of modern physics, including quantum mechanics and relativity. Indeed, the theoretical implications of electromagnetism, particularly the establishment of the speed of light based on properties of the "medium" of propagation (permeability and permittivity), helped inspire Einstein's theory of special relativity in 1905. Meanwhile, the field of quantum electrodynamics (QED) has modified Maxwell's equations to be consistent with the quantized nature of matter. In QED, the electromagnetic field is expressed in terms of discrete particles known as photons, which are also the physical quanta of light. Today, there exist many problems in electromagnetism that remain unsolved, such as the existence of magnetic monopoles and the mechanism by which some organisms can sense electric and magnetic fields.
History of the theory
See also: History of electromagnetic theory
Cover of A Treatise on Electricity and Magnetism

Originally, electricity and magnetism were considered to be two separate forces. This view changed with the publication of James Clerk Maxwell's 1873 A Treatise on Electricity and Magnetism[1] in which the interactions of positive and negative charges were shown to be mediated by one force. There are four main effects resulting from these interactions, all of which have been clearly demonstrated by experiments:

    Electric charges attract or repel one another with a force inversely proportional to the square of the distance between them: unlike charges attract, like ones repel.[2]
    Magnetic poles (or states of polarization at individual points) attract or repel one another in a manner similar to positive and negative charges and always exist as pairs: every north pole is yoked to a south pole.[3]
    An electric current inside a wire creates a corresponding circumferential magnetic field outside the wire. Its direction (clockwise or counter-clockwise) depends on the direction of the current in the wire.[4]
    A current is induced in a loop of wire when it is moved toward or away from a magnetic field, or a magnet is moved towards or away from it; the direction of current depends on that of the movement.[4]

In April 1820, Hans Christian Ørsted observed that an electrical current in a wire caused a nearby compass needle to move. At the time of discovery, Ørsted did not suggest any satisfactory explanation of the phenomenon, nor did he try to represent the phenomenon in a mathematical framework. However, three months later he began more intensive investigations.[5][6] Soon thereafter he published his findings, proving that an electric current produces a magnetic field as it flows through a wire. The CGS unit of magnetic induction (oersted) is named in honor of his contributions to the field of electromagnetism.[7]

His findings resulted in intensive research throughout the scientific community in electrodynamics. They influenced French physicist André-Marie Ampère's developments of a single mathematical form to represent the magnetic forces between current-carrying conductors. Ørsted's discovery also represented a major step toward a unified concept of energy.

This unification, which was observed by Michael Faraday, extended by James Clerk Maxwell, and partially reformulated by Oliver Heaviside and Heinrich Hertz, is one of the key accomplishments of 19th-century mathematical physics.[8] It has had far-reaching consequences, one of which was the understanding of the nature of light. Unlike what was proposed by the electromagnetic theory of that time, light and other electromagnetic waves are at present seen as taking the form of quantized, self-propagating oscillatory electromagnetic field disturbances called photons. Different frequencies of oscillation give rise to the different forms of electromagnetic radiation, from radio waves at the lowest frequencies, to visible light at intermediate frequencies, to gamma rays at the highest frequencies.

Ørsted was not the only person to examine the relationship between electricity and magnetism. In 1802, Gian Domenico Romagnosi, an Italian legal scholar, deflected a magnetic needle using a Voltaic pile. The factual setup of the experiment is not completely clear, nor if current flowed across the needle or not. An account of the discovery was published in 1802 in an Italian newspaper, but it was largely overlooked by the contemporary scientific community, because Romagnosi seemingly did not belong to this community.[9]

An earlier (1735), and often neglected, connection between electricity and magnetism was reported by a Dr. Cookson.[10] The account stated:

    A tradesman at Wakefield in Yorkshire, having put up a great number of knives and forks in a large box ... and having placed the box in the corner of a large room, there happened a sudden storm of thunder, lightning, &c. ... The owner emptying the box on a counter where some nails lay, the persons who took up the knives, that lay on the nails, observed that the knives took up the nails. On this the whole number was tried, and found to do the same, and that, to such a degree as to take up large nails, packing needles, and other iron things of considerable weight ...

E. T. Whittaker suggested in 1910 that this particular event was responsible for lightning to be "credited with the power of magnetizing steel; and it was doubtless this which led Franklin in 1751 to attempt to magnetize a sewing-needle by means of the discharge of Leyden jars."[11]
Fundamental forces
Representation of the electric field vector of a wave of circularly polarized electromagnetic radiation.

The electromagnetic force is one of the four known fundamental forces and the second strongest (after the strong nuclear force), operating with infinite range;[12] The other fundamental forces are:

    the strong nuclear force, which binds quarks to form nucleons, and binds nucleons to form nuclei; it is the strongest of the four known fundamental forces, but operates only at short range;[12]
    the weak nuclear force, which binds to all known particles in the Standard Model, and causes certain forms of radioactive decay; it is the second weakest of the four fundamental forces and, like the strong nuclear force, operates only at short range (in particle physics, the electroweak interaction is the unified description of two of the four known fundamental interactions of nature: electro­magnetism and the weak interaction);[12]
    the gravitational force is the only one of the four fundamental forces that is not part of the Standard Model of particle physics; while by far the weakest of the four fundamental forces, the gravitational force, along with the electro­magnetic force, operates at infinite range.[12]

All other forces (e.g., friction, contact forces) are derived from these four fundamental forces and they are known as non-fundamental forces.[13]

Roughly speaking, all the forces involved in interactions between atoms can be explained by the electromagnetic force acting between the electrically charged atomic nuclei and electrons of the atoms. Electromagnetic forces also explain how these particles carry momentum by their movement. This includes the forces we experience in "pushing" or "pulling" ordinary material objects, which result from the intermolecular forces that act between the individual molecules in our bodies and those in the objects. The electromagnetic force is also involved in all forms of chemical phenomena.

A necessary part of understanding the intra-atomic and intermolecular forces is the effective force generated by the momentum of the electrons' movement, such that as electrons move between interacting atoms they carry momentum with them. As a collection of electrons becomes more confined, their minimum momentum necessarily increases due to the Pauli exclusion principle. The behaviour of matter at the molecular scale including its density is determined by the balance between the electromagnetic force and the force generated by the exchange of momentum carried by the electrons themselves.[14]
Classical electrodynamics
Main article: Classical electrodynamics

In 1600, William Gilbert proposed, in his De Magnete, that electricity and magnetism, while both capable of causing attraction and repulsion of objects, were distinct effects.[15] Mariners had noticed that lightning strikes had the ability to disturb a compass needle. The link between lightning and electricity was not confirmed until Benjamin Franklin's proposed experiments in 1752 were conducted on 10 May 1752 by Thomas-François Dalibard of France using a 40-foot-tall (12 m) iron rod instead of a kite and he successfully extracted electrical sparks from a cloud.[16][17]

One of the first to discover and publish a link between man-made electric current and magnetism was Gian Romagnosi, who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle. However, the effect did not become widely known until 1820, when Ørsted performed a similar experiment.[18] Ørsted's work influenced Ampère to conduct further experiments, which eventually gave rise to a new area of physics: electrodynamics. By determining a force law for the interaction between elements of electric current, Ampère placed the subject on a solid mathematical foundation.[19]

A theory of electromagnetism, known as classical electromagnetism, was developed by several physicists during the period between 1820 and 1873, when James Clerk Maxwell's treatise was published, which unified previous developments into a single theory, proposing that light was an electromagnetic wave propagating in the luminiferous ether.[20] In classical electromagnetism, the behavior of the electromagnetic field is described by a set of equations known as Maxwell's equations, and the electromagnetic force is given by the Lorentz force law.[21]

One of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics, but it is compatible with special relativity. According to Maxwell's equations, the speed of light in vacuum is a universal constant that is dependent only on the electrical permittivity and magnetic permeability of free space. This violates Galilean invariance, a long-standing cornerstone of classical mechanics. One way to reconcile the two theories (electromagnetism and classical mechanics) is to assume the existence of a luminiferous aether through which the light propagates. However, subsequent experimental efforts failed to detect the presence of the aether. After important contributions of Hendrik Lorentz and Henri Poincaré, in 1905, Albert Einstein solved the problem with the introduction of special relativity, which replaced classical kinematics with a new theory of kinematics compatible with classical electromagnetism. (For more information, see History of special relativity.)

In addition, relativity theory implies that in moving frames of reference, a magnetic field transforms to a field with a nonzero electric component and conversely, a moving electric field transforms to a nonzero magnetic component, thus firmly showing that the phenomena are two sides of the same coin. Hence the term "electromagnetism". (For more information, see Classical electromagnetism and special relativity and Covariant formulation of classical electromagnetism.)
Extension to nonlinear phenomena

The Maxwell equations are linear, in that a change in the sources (the charges and currents) results in a proportional change of the fields. Nonlinear dynamics can occur when electromagnetic fields couple to matter that follows nonlinear dynamical laws.[22] This is studied, for example, in the subject of magnetohydrodynamics, which combines Maxwell theory with the Navier–Stokes equations.[23]
Quantities and units
See also: List of physical quantities and List of electromagnetism equations

Here is a list of common units related to electromagnetism:[24]

    ampere (electric current)
    coulomb (electric charge)
    farad (capacitance)
    henry (inductance)
    ohm (resistance)
    siemens (conductance)
    tesla (magnetic flux density)
    volt (electric potential)
    watt (power)
    weber (magnetic flux)

In the electromagnetic CGS system, electric current is a fundamental quantity defined via Ampère's law and takes the permeability as a dimensionless quantity (relative permeability) whose value in vacuum is unity.[25] As a consequence, the square of the speed of light appears explicitly in some of the equations interrelating quantities in this system.
SI electromagnetism units

    vte

Symbol[26] 	Name of quantity 	Unit name 	Symbol 	Base units
E 	energy 	joule 	J = C⋅V = W⋅s 	kg⋅m2⋅s−2
Q 	electric charge 	coulomb 	C 	A⋅s
I 	electric current 	ampere 	A = C/s = W/V 	A
J 	electric current density 	ampere per square metre 	A/m2 	A⋅m−2
U, ΔV; Δφ; E {\mathcal {E}}, ξ {\displaystyle {\xi }} 	potential difference; voltage; electromotive force 	volt 	V = J/C 	kg⋅m2⋅s−3⋅A−1
R; Z; X 	electric resistance; impedance; reactance 	ohm 	Ω = V/A 	kg⋅m2⋅s−3⋅A−2
ρ 	resistivity 	ohm metre 	Ω⋅m 	kg⋅m3⋅s−3⋅A−2
P 	electric power 	watt 	W = V⋅A 	kg⋅m2⋅s−3
C 	capacitance 	farad 	F = C/V 	kg−1⋅m−2⋅A2⋅s4
ΦE 	electric flux 	volt metre 	V⋅m 	kg⋅m3⋅s−3⋅A−1
E 	electric field strength 	volt per metre 	V/m = N/C 	kg⋅m⋅A−1⋅s−3
D 	electric displacement field 	coulomb per square metre 	C/m2 	A⋅s⋅m−2
ε 	permittivity 	farad per metre 	F/m 	kg−1⋅m−3⋅A2⋅s4
χe 	electric susceptibility 	(dimensionless) 	1 	1
G; Y; B 	conductance; admittance; susceptance 	siemens 	S = Ω−1 	kg−1⋅m−2⋅s3⋅A2
κ, γ, σ 	conductivity 	siemens per metre 	S/m 	kg−1⋅m−3⋅s3⋅A2
B 	magnetic flux density, magnetic induction 	tesla 	T = Wb/m2 = N⋅A−1⋅m−1 	kg⋅s−2⋅A−1
Φ, ΦM, ΦB 	magnetic flux 	weber 	Wb = V⋅s 	kg⋅m2⋅s−2⋅A−1
H 	magnetic field strength 	ampere per metre 	A/m 	A⋅m−1
L, M 	inductance 	henry 	H = Wb/A = V⋅s/A 	kg⋅m2⋅s−2⋅A−2
μ 	permeability 	henry per metre 	H/m 	kg⋅m⋅s−2⋅A−2
χ 	magnetic susceptibility 	(dimensionless) 	1 	1
µ 	magnetic dipole moment 	ampere square meter 	A⋅m2 = J⋅T−1 	A⋅m2
σ 	mass magnetization 	ampere square meter per kilogram 	A⋅m2/kg 	A⋅m2⋅kg−1

Formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on what system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit "sub-systems", including Gaussian, "ESU", "EMU", and Heaviside–Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase "CGS units" is often used to refer specifically to CGS-Gaussian units.[27]
Applications

The study of electromagnetism informs electric circuits and semiconductor devices' construction.
See also

    Abraham–Lorentz force
    Aeromagnetic surveys
    Computational electromagnetics
    Double-slit experiment
    Electromagnet
    Electromagnetic induction
    Electromagnetic wave equation
    Electromagnetic scattering
    Electromechanics
    Geophysics
    Introduction to electromagnetism
    Magnetostatics
    Magnetoquasistatic field
    Optics
    Relativistic electromagnetism
    Wheeler–Feynman absorber theory

References

"A Treatise on Electricity and Magnetism". Nature. 7 (182): 478–480. 24 April 1873. Bibcode:1873Natur...7..478.. doi:10.1038/007478a0. ISSN 0028-0836. S2CID 10178476.
"Why Do Like Charges Repel And Opposite Charges Attract?". Science ABC. 2019-02-06. Retrieved 2022-08-22.
"What Makes Magnets Repel?". Sciencing. Retrieved 2022-08-22.
Jim Lucas Contributions from Ashley Hamer (2022-02-18). "What Is Faraday's Law of Induction?". livescience.com. Retrieved 2022-08-22.
"History of the Electric Telegraph". Scientific American. 17 (425supp): 6784–6786. 1884-02-23. doi:10.1038/scientificamerican02231884-6784supp. ISSN 0036-8733.
Volta and the history of electricity. Fabio Bevilacqua, Enrico A. Giannetto. Milano: U. Hoepli. 2003. ISBN 88-203-3284-1. OCLC 1261807533.
Roche, John J. (1998). The mathematics of measurement : a critical history. London: Athlone Press. ISBN 0-485-11473-9. OCLC 40499222.
Darrigol, Olivier (2000). Electrodynamics from Ampère to Einstein. New York: Oxford University Press. ISBN 0198505949.
Martins, Roberto de Andrade. "Romagnosi and Volta's Pile: Early Difficulties in the Interpretation of Voltaic Electricity" (PDF). In Fabio Bevilacqua; Lucio Fregonese (eds.). Nuova Voltiana: Studies on Volta and his Times. Vol. 3. Università degli Studi di Pavia. pp. 81–102. Archived from the original (PDF) on 2013-05-30. Retrieved 2010-12-02.
VIII. An account of an extraordinary effect of lightning in communicating magnetism. Communicated by Pierce Dod, M.D. F.R.S. from Dr. Cookson of Wakefield in Yorkshire. Phil. Trans. 1735 39, 74-75, published 1 January 1735
Whittaker, E.T. (1910). A History of the Theories of Aether and Electricity from the Age of Descartes to the Close of the Nineteenth Century. Longmans, Green and Company.
Rehm, Jeremy; published, Ben Biggs (2021-12-23). "The four fundamental forces of nature". Space.com. Retrieved 2022-08-22.
Browne, "Physics for Engineering and Science", p. 160: "Gravity is one of the fundamental forces of nature. The other forces such as friction, tension, and the normal force are derived from the electric force, another of the fundamental forces. Gravity is a rather weak force... The electric force between two protons is much stronger than the gravitational force between them."
Purcell, "Electricity and Magnetism, 3rd Edition", p. 546: Ch 11 Section 6, "Electron Spin and Magnetic Moment."
Malin, Stuart; Barraclough, David (2000). "Gilbert's De Magnete: An early study of magnetism and electricity". Eos, Transactions American Geophysical Union. 81 (21): 233. Bibcode:2000EOSTr..81..233M. doi:10.1029/00EO00163. ISSN 0096-3941.
"Lightning! | Museum of Science, Boston".
Tucker, Tom (2003). Bolt of fate : Benjamin Franklin and his electric kite hoax (1st ed.). New York: PublicAffairs. ISBN 1-891620-70-3. OCLC 51763922.
Stern, Dr. David P.; Peredo, Mauricio (2001-11-25). "Magnetic Fields – History". NASA Goddard Space Flight Center. Retrieved 2009-11-27.
"Andre-Marie Ampère". ETHW. 2016-01-13. Retrieved 2022-08-22.
Purcell, p. 436. Chapter 9.3, "Maxwell's description of the electromagnetic field was essentially complete."
Purcell: p. 278: Chapter 6.1, "Definition of the Magnetic Field." Lorentz force and force equation.
Jufriansah, Adi; Hermanto, Arief; Toifur, Moh.; Prasetyo, Erwin (2020-05-18). "Theoretical study of Maxwell's equations in nonlinear optics". AIP Conference Proceedings. 2234 (1): 040013. Bibcode:2020AIPC.2234d0013J. doi:10.1063/5.0008179. ISSN 0094-243X. S2CID 219451710.
Hunt, Julian C. R. (1967-07-27). Some aspects of magnetohydrodynamics (Thesis thesis). University of Cambridge. doi:10.17863/cam.14141.
"Essentials of the SI: Base & derived units". physics.nist.gov. Retrieved 2022-08-22.
"Tables of Physical and Chemical Constants, and some Mathematical Functions". Nature. 107 (2687): 264. April 1921. Bibcode:1921Natur.107R.264.. doi:10.1038/107264c0. ISSN 1476-4687.
International Union of Pure and Applied Chemistry (1993). Quantities, Units and Symbols in Physical Chemistry, 2nd edition, Oxford: Blackwell Science. ISBN 0-632-03583-8. pp. 14–15. Electronic version.

    "Conversion of formulae and quantities between unit systems" (PDF). www.stanford.edu. Retrieved 29 January 2022.

Further reading
Library resources about
Electromagnetism

    Resources in your library

Web sources

    Nave, R. "Electricity and magnetism". HyperPhysics. Georgia State University. Retrieved 2013-11-12.
    Khutoryansky, E. "Electromagnetism – Maxwell's Laws". YouTube. Retrieved 2014-12-28.

Textbooks

    G.A.G. Bennet (1974). Electricity and Modern Physics (2nd ed.). Edward Arnold (UK). ISBN 978-0-7131-2459-0.
    Browne, Michael (2008). Physics for Engineering and Science (2nd ed.). McGraw-Hill/Schaum. ISBN 978-0-07-161399-6.
    Dibner, Bern (2012). Oersted and the discovery of electromagnetism. Literary Licensing, LLC. ISBN 978-1-258-33555-7.
    Durney, Carl H.; Johnson, Curtis C. (1969). Introduction to modern electromagnetics. McGraw-Hill. ISBN 978-0-07-018388-9.
    Feynman, Richard P. (1970). The Feynman Lectures on Physics Vol II. Addison Wesley Longman. ISBN 978-0-201-02115-8.
    Fleisch, Daniel (2008). A Student's Guide to Maxwell's Equations. Cambridge, UK: Cambridge University Press. ISBN 978-0-521-70147-1.
    I.S. Grant; W.R. Phillips; Manchester Physics (2008). Electromagnetism (2nd ed.). John Wiley & Sons. ISBN 978-0-471-92712-9.
    Griffiths, David J. (1998). Introduction to Electrodynamics (3rd ed.). Prentice Hall. ISBN 978-0-13-805326-0.
    Jackson, John D. (1998). Classical Electrodynamics (3rd ed.). Wiley. ISBN 978-0-471-30932-1.
    Moliton, André (2007). Basic electromagnetism and materials. ISBN 978-0-387-30284-3. {{cite book}}: |work= ignored (help)
    Purcell, Edward M. (1985). Electricity and Magnetism Berkeley, Physics Course Volume 2 (2nd ed.). McGraw-Hill. ISBN 978-0-07-004908-6.
    Purcell, Edward M and Morin, David. (2013). Electricity and Magnetism, 820p (3rd ed.). Cambridge University Press, New York. ISBN 978-1-107-01402-2.
    Rao, Nannapaneni N. (1994). Elements of engineering electromagnetics (4th ed.). Prentice Hall. ISBN 978-0-13-948746-0.
    Rothwell, Edward J.; Cloud, Michael J. (2001). Electromagnetics. CRC Press. ISBN 978-0-8493-1397-4.
    Tipler, Paul (1998). Physics for Scientists and Engineers: Vol. 2: Light, Electricity and Magnetism (4th ed.). W.H. Freeman. ISBN 978-1-57259-492-0.
    Wangsness, Roald K.; Cloud, Michael J. (1986). Electromagnetic Fields (2nd ed.). Wiley. ISBN 978-0-471-81186-2.

General coverage

    A. Beiser (1987). Concepts of Modern Physics (4th ed.). McGraw-Hill (International). ISBN 978-0-07-100144-1.
    L.H. Greenberg (1978). Physics with Modern Applications. Holt-Saunders International W.B. Saunders and Co. ISBN 978-0-7216-4247-5.
    R.G. Lerner; G.L. Trigg (2005). Encyclopaedia of Physics (2nd ed.). VHC Publishers, Hans Warlimont, Springer. pp. 12–13. ISBN 978-0-07-025734-4.
    J.B. Marion; W.F. Hornyak (1984). Principles of Physics. Holt-Saunders International Saunders College. ISBN 978-4-8337-0195-2.
    H.J. Pain (1983). The Physics of Vibrations and Waves (3rd ed.). John Wiley & Sons. ISBN 978-0-471-90182-2.
    C.B. Parker (1994). McGraw Hill Encyclopaedia of Physics (2nd ed.). McGraw Hill. ISBN 978-0-07-051400-3.
    R. Penrose (2007). The Road to Reality. Vintage books. ISBN 978-0-679-77631-4.
    P.A. Tipler; G. Mosca (2008). Physics for Scientists and Engineers: With Modern Physics (6th ed.). W.H. Freeman and Co. ISBN 978-1-4292-0265-7.
    P.M. Whelan; M.J. Hodgeson (1978). Essential Principles of Physics (2nd ed.). John Murray. ISBN 978-0-7195-3382-2.

External links
Wikiquote has quotations related to Electromagnetism.

    Magnetic Field Strength Converter
    Electromagnetic Force – from Eric Weisstein's World of Physics

    vte

The fundamental interactions of physics

    vte

Major branches of physics

    vte

Magnetic states
Authority control Edit this at Wikidata
Categories:

    ElectromagnetismElectrodynamicsFundamental interactions

    This page was last edited on 9 August 2023, at 01:07 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Introduction

Conductors and resistors

Ohm's law

Relation to resistivity and conductivity

Measurement

Typical values

Static and differential resistance

AC circuits

    Impedance and admittance
    Frequency dependence

Energy dissipation and Joule heating

Dependence on other conditions

        Temperature dependence
        Strain dependence
        Light illumination dependence
    Superconductivity
    See also
    Footnotes
    References
    External links

Electrical resistance and conductance

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
This article is about specific applications of conductivity and resistivity in electrical elements. For other types of conductivity, see Conductivity. For electrical conductivity in general, see Electrical resistivity and conductivity.
"Resistive" redirects here. For the term used when referring to touchscreens, see Resistive touchscreen.
Articles about
Electromagnetism
Solenoid

    Electricity Magnetism Optics History Textbooks

Electrostatics
Magnetostatics
Electrodynamics
Electrical network

    Alternating current Capacitance Direct current Electric current Electrolysis Current density Joule heating Electromotive force Impedance Inductance Ohm's law Parallel circuit Resistance Resonant cavities Series circuit Voltage Waveguides

Magnetic circuit
Covariant formulation
Scientists

    vte

The electrical resistance of an object is a measure of its opposition to the flow of electric current. Its reciprocal quantity is electrical conductance, measuring the ease with which an electric current passes. Electrical resistance shares some conceptual parallels with mechanical friction. The SI unit of electrical resistance is the ohm (Ω), while electrical conductance is measured in siemens (S) (formerly called the 'mho' and then represented by ℧).

The resistance of an object depends in large part on the material it is made of. Objects made of electrical insulators like rubber tend to have very high resistance and low conductance, while objects made of electrical conductors like metals tend to have very low resistance and high conductance. This relationship is quantified by resistivity or conductivity. The nature of a material is not the only factor in resistance and conductance, however; it also depends on the size and shape of an object because these properties are extensive rather than intensive. For example, a wire's resistance is higher if it is long and thin, and lower if it is short and thick. All objects resist electrical current, except for superconductors, which have a resistance of zero.

The resistance R of an object is defined as the ratio of voltage V across it to current I through it, while the conductance G is the reciprocal:
R = V I , G = I V = 1 R
{\displaystyle R={\frac {V}{I}},\qquad G={\frac {I}{V}}={\frac {1}{R}}}

For a wide variety of materials and conditions, V and I are directly proportional to each other, and therefore R and G are constants (although they will depend on the size and shape of the object, the material it is made of, and other factors like temperature or strain). This proportionality is called Ohm's law, and materials that satisfy it are called ohmic materials.

In other cases, such as a transformer, diode or battery, V and I are not directly proportional. The ratio V/I is sometimes still useful, and is referred to as a chordal resistance or static resistance,[1][2] since it corresponds to the inverse slope of a chord between the origin and an I–V curve. In other situations, the derivative d V d I {\textstyle {\frac {\mathrm {d} V}{\mathrm {d} I}}} may be most useful; this is called the differential resistance.
Introduction
analogy of resistance
The hydraulic analogy compares electric current flowing through circuits to water flowing through pipes. When a pipe (left) is filled with hair (right), it takes a larger pressure to achieve the same flow of water. Pushing electric current through a large resistance is like pushing water through a pipe clogged with hair: It requires a larger push (electromotive force) to drive the same flow (electric current).

In the hydraulic analogy, current flowing through a wire (or resistor) is like water flowing through a pipe, and the voltage drop across the wire is like the pressure drop that pushes water through the pipe. Conductance is proportional to how much flow occurs for a given pressure, and resistance is proportional to how much pressure is required to achieve a given flow.

The voltage drop (i.e., difference between voltages on one side of the resistor and the other), not the voltage itself, provides the driving force pushing current through a resistor. In hydraulics, it is similar: the pressure difference between two sides of a pipe, not the pressure itself, determines the flow through it. For example, there may be a large water pressure above the pipe, which tries to push water down through the pipe. But there may be an equally large water pressure below the pipe, which tries to push water back up through the pipe. If these pressures are equal, no water flows. (In the image at right, the water pressure below the pipe is zero.)

The resistance and conductance of a wire, resistor, or other element is mostly determined by two properties:

    geometry (shape), and
    material

Geometry is important because it is more difficult to push water through a long, narrow pipe than a wide, short pipe. In the same way, a long, thin copper wire has higher resistance (lower conductance) than a short, thick copper wire.

Materials are important as well. A pipe filled with hair restricts the flow of water more than a clean pipe of the same shape and size. Similarly, electrons can flow freely and easily through a copper wire, but cannot flow as easily through a steel wire of the same shape and size, and they essentially cannot flow at all through an insulator like rubber, regardless of its shape. The difference between copper, steel, and rubber is related to their microscopic structure and electron configuration, and is quantified by a property called resistivity.

In addition to geometry and material, there are various other factors that influence resistance and conductance, such as temperature; see below.
Conductors and resistors
A 75 Ω resistor, as identified by its electronic color code (violet–green–black–gold–red). An ohmmeter could be used to verify this value.

Substances in which electricity can flow are called conductors. A piece of conducting material of a particular resistance meant for use in a circuit is called a resistor. Conductors are made of high-conductivity materials such as metals, in particular copper and aluminium. Resistors, on the other hand, are made of a wide variety of materials depending on factors such as the desired resistance, amount of energy that it needs to dissipate, precision, and costs.
Ohm's law
Main article: Ohm's law
The current–voltage characteristics of four devices: Two resistors, a diode, and a battery. The horizontal axis is voltage drop, the vertical axis is current. Ohm's law is satisfied when the graph is a straight line through the origin. Therefore, the two resistors are ohmic, but the diode and battery are not.

For many materials, the current I through the material is proportional to the voltage V applied across it:
I ∝ V
{\displaystyle I\propto V}
over a wide range of voltages and currents. Therefore, the resistance and conductance of objects or electronic components made of these materials is constant. This relationship is called Ohm's law, and materials which obey it are called ohmic materials. Examples of ohmic components are wires and resistors. The current–voltage graph of an ohmic device consists of a straight line through the origin with positive slope.

Other components and materials used in electronics do not obey Ohm's law; the current is not proportional to the voltage, so the resistance varies with the voltage and current through them. These are called nonlinear or non-ohmic. Examples include diodes and fluorescent lamps. The current-voltage curve of a nonohmic device is a curved line.
Relation to resistivity and conductivity
Main article: Electrical resistivity and conductivity
A piece of resistive material with electrical contacts on both ends.

The resistance of a given object depends primarily on two factors: what material it is made of, and its shape. For a given material, the resistance is inversely proportional to the cross-sectional area; for example, a thick copper wire has lower resistance than an otherwise-identical thin copper wire. Also, for a given material, the resistance is proportional to the length; for example, a long copper wire has higher resistance than an otherwise-identical short copper wire. The resistance R and conductance G of a conductor of uniform cross section, therefore, can be computed as

R = ρ ℓ A , G = σ A ℓ .
{\displaystyle {\begin{aligned}R&=\rho {\frac {\ell }{A}},\\[5pt]G&=\sigma {\frac {A}{\ell }}\,.\end{aligned}}}

where ℓ \ell is the length of the conductor, measured in metres (m), A is the cross-sectional area of the conductor measured in square metres (m2), σ (sigma) is the electrical conductivity measured in siemens per meter (S·m−1), and ρ (rho) is the electrical resistivity (also called specific electrical resistance) of the material, measured in ohm-metres (Ω·m). The resistivity and conductivity are proportionality constants, and therefore depend only on the material the wire is made of, not the geometry of the wire. Resistivity and conductivity are reciprocals: ρ = 1 / σ \rho=1/\sigma. Resistivity is a measure of the material's ability to oppose electric current.

This formula is not exact, as it assumes the current density is totally uniform in the conductor, which is not always true in practical situations. However, this formula still provides a good approximation for long thin conductors such as wires.

Another situation for which this formula is not exact is with alternating current (AC), because the skin effect inhibits current flow near the center of the conductor. For this reason, the geometrical cross-section is different from the effective cross-section in which current actually flows, so resistance is higher than expected. Similarly, if two conductors near each other carry AC current, their resistances increase due to the proximity effect. At commercial power frequency, these effects are significant for large conductors carrying large currents, such as busbars in an electrical substation,[3] or large power cables carrying more than a few hundred amperes.

The resistivity of different materials varies by an enormous amount: For example, the conductivity of teflon is about 1030 times lower than the conductivity of copper. Loosely speaking, this is because metals have large numbers of "delocalized" electrons that are not stuck in any one place, so they are free to move across large distances. In an insulator, such as Teflon, each electron is tightly bound to a single molecule so a great force is required to pull it away. Semiconductors lie between these two extremes. More details can be found in the article: Electrical resistivity and conductivity. For the case of electrolyte solutions, see the article: Conductivity (electrolytic).

Resistivity varies with temperature. In semiconductors, resistivity also changes when exposed to light. See below.
Measurement
Main article: Ohmmeter
photograph of an ohmmeter
An ohmmeter

An instrument for measuring resistance is called an ohmmeter. Simple ohmmeters cannot measure low resistances accurately because the resistance of their measuring leads causes a voltage drop that interferes with the measurement, so more accurate devices use four-terminal sensing.
Typical values
See also: Electrical resistivities of the elements (data page) and Electrical resistivity and conductivity
Typical resistance values for selected objects Component 	Resistance (Ω)
1 meter of copper wire with 1 mm diameter 	0.02[a]
1 km overhead power line (typical) 	0.03[5]
AA battery (typical internal resistance) 	0.1[b]
Incandescent light bulb filament (typical) 	200–1000[c]
Human body 	1000–100,000[d]
Static and differential resistance
See also: Small-signal model
Differential versus chordal resistance
The current–voltage curve of a non-ohmic device (purple). The static resistance at point A is the inverse slope of line B through the origin. The differential resistance at A is the inverse slope of tangent line C.
Negative differential resistance
The current–voltage curve of a component with negative differential resistance, an unusual phenomenon where the current–voltage curve is non-monotonic.

Many electrical elements, such as diodes and batteries do not satisfy Ohm's law. These are called non-ohmic or non-linear, and their current–voltage curves are not straight lines through the origin.

Resistance and conductance can still be defined for non-ohmic elements. However, unlike ohmic resistance, non-linear resistance is not constant but varies with the voltage or current through the device; i.e., its operating point. There are two types of resistance:[1][2]

Static resistance

Also called chordal or DC resistance
    This corresponds to the usual definition of resistance; the voltage divided by the current
    R s t a t i c = U I .
    {\displaystyle R_{\mathrm {static} }={\frac {U}{I}}\,.}
    It is the slope of the line (chord) from the origin through the point on the curve. Static resistance determines the power dissipation in an electrical component. Points on the current–voltage curve located in the 2nd or 4th quadrants, for which the slope of the chordal line is negative, have negative static resistance. Passive devices, which have no source of energy, cannot have negative static resistance. However active devices such as transistors or op-amps can synthesize negative static resistance with feedback, and it is used in some circuits such as gyrators. 
Differential resistance

Also called dynamic, incremental, or small-signal resistance
    Differential resistance is the derivative of the voltage with respect to the current; the slope of the current–voltage curve at a point
    R d i f f = d U d I .
    {\displaystyle R_{\mathrm {diff} }={\frac {{\mathrm {d} }U}{{\mathrm {d} }I}}\,.}
    If the current–voltage curve is nonmonotonic (with peaks and troughs), the curve has a negative slope in some regions—so in these regions the device has negative differential resistance. Devices with negative differential resistance can amplify a signal applied to them, and are used to make amplifiers and oscillators. These include tunnel diodes, Gunn diodes, IMPATT diodes, magnetron tubes, and unijunction transistors. 

AC circuits
Impedance and admittance
Main articles: Electrical impedance and Admittance
The voltage (red) and current (blue) versus time (horizontal axis) for a capacitor (top) and inductor (bottom). Since the amplitude of the current and voltage sinusoids are the same, the absolute value of impedance is 1 for both the capacitor and the inductor (in whatever units the graph is using). On the other hand, the phase difference between current and voltage is −90° for the capacitor; therefore, the complex phase of the impedance of the capacitor is −90°. Similarly, the phase difference between current and voltage is +90° for the inductor; therefore, the complex phase of the impedance of the inductor is +90°.

When an alternating current flows through a circuit, the relation between current and voltage across a circuit element is characterized not only by the ratio of their magnitudes, but also the difference in their phases. For example, in an ideal resistor, the moment when the voltage reaches its maximum, the current also reaches its maximum (current and voltage are oscillating in phase). But for a capacitor or inductor, the maximum current flow occurs as the voltage passes through zero and vice versa (current and voltage are oscillating 90° out of phase, see image below). Complex numbers are used to keep track of both the phase and magnitude of current and voltage:

u ( t ) = R e ⁡ ( U 0 ⋅ e j ω t ) i ( t ) = R e ⁡ ( I 0 ⋅ e j ( ω t + φ ) ) Z = U   I   Y =   1   Z =   I   U
{\displaystyle {\begin{array}{cl}u(t)&=\operatorname {\mathcal {R_{e}}} \left(U_{0}\cdot e^{j\omega t}\right)\\i(t)&=\operatorname {\mathcal {R_{e}}} \left(I_{0}\cdot e^{j(\omega t+\varphi )}\right)\\Z&={\frac {U}{\ I\ }}\\Y&={\frac {\ 1\ }{Z}}={\frac {\ I\ }{U}}\end{array}}}

where:

    t is time;
    u(t) and i(t) are the voltage and current as a function of time, respectively;
    U0 and I0 indicate the amplitude of the voltage and current, respectively;
    ω \omega is the angular frequency of the AC current;
    φ \varphi is the displacement angle;
    U and I are the complex-valued voltage and current, respectively;
    Z and Y are the complex impedance and admittance, respectively;
    R e {\displaystyle {\mathcal {R_{e}}}} indicates the real part of a complex number; and
    j ≡ − 1   {\displaystyle j\equiv {\sqrt {-1\ }}} is the imaginary unit.

The impedance and admittance may be expressed as complex numbers that can be broken into real and imaginary parts:
Z = R + j X Y = G + j B   .
{\displaystyle {\begin{aligned}Z&=R+jX\\Y&=G+jB~.\end{aligned}}}

where R is resistance, G is conductance, X is reactance, and B is susceptance. These lead to the complex number identities
R = G   G 2 + B 2     , X = − B     G 2 + B 2     , G = R   R 2 + X 2     , B = − X     R 2 + X 2     ,
{\displaystyle {\begin{aligned}R&={\frac {G}{\ G^{2}+B^{2}\ }}\ ,\qquad &X={\frac {-B~}{\ G^{2}+B^{2}\ }}\ ,\\G&={\frac {R}{\ R^{2}+X^{2}\ }}\ ,\qquad &B={\frac {-X~}{\ R^{2}+X^{2}\ }}\ ,\end{aligned}}}
which are true in all cases, whereas   R = 1 / G   {\displaystyle \ R=1/G\ } is only true in the special cases of either DC or reactance-free current.

The complex angle   θ = arg ⁡ ( Z ) = − arg ⁡ ( Y )   {\displaystyle \ \theta =\arg(Z)=-\arg(Y)\ } is the phase difference between the voltage and current passing through a component with impedance Z. For capacitors and inductors, this angle is exactly -90° or +90°, respectively, and X and B are nonzero. Ideal resistors have an angle of 0°, since X is zero (and hence B also), and Z and Y reduce to R and G respectively. In general, AC systems are designed to keep the phase angle close to 0° as much as possible, since it reduces the reactive power, which does no useful work at a load. In a simple case with an inductive load (causing the phase to increase), a capacitor may be added for compensation at one frequency, since the capacitor's phase shift is negative, bringing the total impedance phase closer to 0° again.

Y is the reciprocal of Z (   Z = 1 / Y   {\displaystyle \ Z=1/Y\ }) for all circuits, just as R = 1 / G {\displaystyle R=1/G} for DC circuits containing only resistors, or AC circuits for which either the reactance or susceptance happens to be zero (X or B = 0, respectively) (if one is zero, then for realistic systems both must be zero).
Frequency dependence

A key feature of AC circuits is that the resistance and conductance can be frequency-dependent, a phenomenon known as the universal dielectric response.[8] One reason, mentioned above is the skin effect (and the related proximity effect). Another reason is that the resistivity itself may depend on frequency (see Drude model, deep-level traps, resonant frequency, Kramers–Kronig relations, etc.)
Energy dissipation and Joule heating
Main article: Joule heating
Running current through a material with resistance creates heat, in a phenomenon called Joule heating. In this picture, a cartridge heater, warmed by Joule heating, is glowing red hot.

Resistors (and other elements with resistance) oppose the flow of electric current; therefore, electrical energy is required to push current through the resistance. This electrical energy is dissipated, heating the resistor in the process. This is called Joule heating (after James Prescott Joule), also called ohmic heating or resistive heating.

The dissipation of electrical energy is often undesired, particularly in the case of transmission losses in power lines. High voltage transmission helps reduce the losses by reducing the current for a given power.

On the other hand, Joule heating is sometimes useful, for example in electric stoves and other electric heaters (also called resistive heaters). As another example, incandescent lamps rely on Joule heating: the filament is heated to such a high temperature that it glows "white hot" with thermal radiation (also called incandescence).

The formula for Joule heating is:
P = I 2 R
{\displaystyle P=I^{2}R}
where P is the power (energy per unit time) converted from electrical energy to thermal energy, R is the resistance, and I is the current through the resistor.

Dependence on other conditions
Temperature dependence
Main article: Electrical resistivity and conductivity § Temperature dependence

Near room temperature, the resistivity of metals typically increases as temperature is increased, while the resistivity of semiconductors typically decreases as temperature is increased. The resistivity of insulators and electrolytes may increase or decrease depending on the system. For the detailed behavior and explanation, see Electrical resistivity and conductivity.

As a consequence, the resistance of wires, resistors, and other components often change with temperature. This effect may be undesired, causing an electronic circuit to malfunction at extreme temperatures. In some cases, however, the effect is put to good use. When temperature-dependent resistance of a component is used purposefully, the component is called a resistance thermometer or thermistor. (A resistance thermometer is made of metal, usually platinum, while a thermistor is made of ceramic or polymer.)

Resistance thermometers and thermistors are generally used in two ways. First, they can be used as thermometers: by measuring the resistance, the temperature of the environment can be inferred. Second, they can be used in conjunction with Joule heating (also called self-heating): if a large current is running through the resistor, the resistor's temperature rises and therefore its resistance changes. Therefore, these components can be used in a circuit-protection role similar to fuses, or for feedback in circuits, or for many other purposes. In general, self-heating can turn a resistor into a nonlinear and hysteretic circuit element. For more details see Thermistor#Self-heating effects.

If the temperature T does not vary too much, a linear approximation is typically used:
R ( T ) = R 0 [ 1 + α ( T − T 0 ) ]
{\displaystyle R(T)=R_{0}[1+\alpha (T-T_{0})]}
where α \alpha is called the temperature coefficient of resistance, T 0 T_{0} is a fixed reference temperature (usually room temperature), and R 0 R_{0} is the resistance at temperature T 0 T_{0}. The parameter α \alpha is an empirical parameter fitted from measurement data. Because the linear approximation is only an approximation, α \alpha is different for different reference temperatures. For this reason it is usual to specify the temperature that α \alpha was measured at with a suffix, such as α 15 \alpha _{15}, and the relationship only holds in a range of temperatures around the reference.[9]

The temperature coefficient α \alpha is typically +3×10−3 K−1 to +6×10−3 K−1 for metals near room temperature. It is usually negative for semiconductors and insulators, with highly variable magnitude.[e]
Strain dependence
Main article: Strain gauge

Just as the resistance of a conductor depends upon temperature, the resistance of a conductor depends upon strain.[10] By placing a conductor under tension (a form of stress that leads to strain in the form of stretching of the conductor), the length of the section of conductor under tension increases and its cross-sectional area decreases. Both these effects contribute to increasing the resistance of the strained section of conductor. Under compression (strain in the opposite direction), the resistance of the strained section of conductor decreases. See the discussion on strain gauges for details about devices constructed to take advantage of this effect.
Light illumination dependence
Main articles: Photoresistor and Photoconductivity

Some resistors, particularly those made from semiconductors, exhibit photoconductivity, meaning that their resistance changes when light is shining on them. Therefore, they are called photoresistors (or light dependent resistors). These are a common type of light detector.
Superconductivity
Main article: Superconductivity

Superconductors are materials that have exactly zero resistance and infinite conductance, because they can have V = 0 and I ≠ 0. This also means there is no joule heating, or in other words no dissipation of electrical energy. Therefore, if superconductive wire is made into a closed loop, current flows around the loop forever. Superconductors require cooling to temperatures near 4 K with liquid helium for most metallic superconductors like niobium–tin alloys, or cooling to temperatures near 77 K with liquid nitrogen for the expensive, brittle and delicate ceramic high temperature superconductors. Nevertheless, there are many technological applications of superconductivity, including superconducting magnets.
See also

    iconElectronics portal

    Conductance quantum
        Von Klitzing constant (its reciprocal)
    Electrical measurements
    Contact resistance
    Electrical resistivity and conductivity for more information about the physical mechanisms for conduction in materials.
    Johnson–Nyquist noise
    Quantum Hall effect, a standard for high-accuracy resistance measurements.
    Resistor
    RKM code
    Series and parallel circuits
    Sheet resistance
    SI electromagnetism units
    Thermal resistance
    Voltage divider
    Voltage drop

Footnotes

The resistivity of copper is about 1.7×10−8 Ω⋅m.[4]
For a fresh Energizer E91 AA alkaline battery, the internal resistance varies from 0.9 Ω at −40 °C, to 0.1 Ω at +40 °C.[6]
A 60 W light bulb (in the USA, with 120 V mains electricity) draws RMS current 60 W/120 V = 500 mA, so its resistance is 120 V/500 mA = 240 Ω. The resistance of a 60 W light bulb in Europe (230 V mains) is 900 Ω. The resistance of a filament is temperature-dependent; these values are for when the filament is already heated up and the light is already glowing.
100 kΩ for dry skin contact, 1 kΩ for wet or broken skin contact. High voltage breaks down the skin, lowering resistance to 500 Ω. Other factors and conditions are relevant as well. For more details, see the electric shock article, and NIOSH 98-131.[7]

    See Electrical resistivity and conductivity for a table. The temperature coefficient of resistivity is similar but not identical to the temperature coefficient of resistance. The small difference is due to thermal expansion changing the dimensions of the resistor.

References

Brown, Forbes T. (2006). Engineering System Dynamics: A Unified Graph-Centered Approach (2nd ed.). Boca Raton, Florida: CRC Press. p. 43. ISBN 978-0-8493-9648-9.
Kaiser, Kenneth L. (2004). Electromagnetic Compatibility Handbook. Boca Raton, Florida: CRC Press. pp. 13–52. ISBN 978-0-8493-2087-3.
Fink & Beaty (1923). "Standard Handbook for Electrical Engineers". Nature (11th ed.). 111 (2788): 17–19. Bibcode:1923Natur.111..458R. doi:10.1038/111458a0. hdl:2027/mdp.39015065357108. S2CID 26358546.
Cutnell, John D.; Johnson, Kenneth W. (1992). Physics (2nd ed.). New York: Wiley. p. 559. ISBN 978-0-471-52919-4.
McDonald, John D. (2016). Electric Power Substations Engineering (2nd ed.). Boca Raton, Florida: CRC Press. pp. 363ff. ISBN 978-1-4200-0731-2.
Battery internal resistance (PDF) (Report). Energizer Corp.
"Worker Deaths by Electrocution" (PDF). National Institute for Occupational Safety and Health. Publication No. 98-131. Retrieved 2 November 2014.
Zhai, Chongpu; Gan, Yixiang; Hanaor, Dorian; Proust, Gwénaëlle (2018). "Stress-dependent electrical transport and its universal scaling in granular materials". Extreme Mechanics Letters. 22: 83–88. arXiv:1712.05938. doi:10.1016/j.eml.2018.05.005. S2CID 51912472.
Ward, M.R. (1971). Electrical Engineering Science. McGraw-Hill. pp. 36–40.

    Meyer, Sebastian; et al. (2022), "Characterization of the deformation state of magnesium by electrical resistance", Volume 215, Scripta Materialia, vol. 215, p. 114712, doi:10.1016/j.scriptamat.2022.114712, S2CID 247959452

External links
Wikimedia Commons has media related to Electrical resistance and conductance.

    "Resistance calculator". Vehicular Electronics Laboratory. Clemson University. Archived from the original on 11 July 2010.
    "Electron conductance models using maximal entropy random walks". wolfram.com. Wolfram Demonstrantions Project.

Categories:

    Electrical resistance and conductanceElectricityPhysical quantitiesElectromagnetism

    This page was last edited on 7 March 2023, at 07:54 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Code of Conduct
    Mobile view
    Developers
    Statistics
    Cookie statement

    Wikimedia Foundation
    Powered by MediaWiki


Main menu

Wikipedia The Free Encyclopedia

    Create account
    Log in

Personal tools

Contents
(Top)
Resistance and conductance

Conductor materials

Wire size

Conductor ampacity

Isotropy

See also

References

Further reading

        Pioneering and historical books
        Reference books
    External links

Electrical conductor

    Article
    Talk

    Read
    Edit
    View history

Tools

From Wikipedia, the free encyclopedia
Overhead conductors carry electric power from generating stations to customers.
Articles about
Electromagnetism
Solenoid

    Electricity Magnetism Optics History Textbooks

Electrostatics

    Electric charge Coulomb's law Conductor Charge density Permittivity Electric dipole moment Electric field Electric potential Electric flux / potential energy Electrostatic discharge Gauss's law Induction Insulator Polarization density Static electricity Triboelectricity

Magnetostatics
Electrodynamics
Electrical network
Magnetic circuit
Covariant formulation
Scientists

    vte

In physics and electrical engineering, a conductor is an object or type of material that allows the flow of charge (electric current) in one or more directions. Materials made of metal are common electrical conductors. The flow of negatively charged electrons generates electric current, positively charged holes, and positive or negative ions in some cases.

In order for current to flow within a closed electrical circuit, one charged particle does not need to travel from the component producing the current (the current source) to those consuming it (the loads). Instead, the charged particle simply needs to nudge its neighbor a finite amount, who will nudge its neighbor, and on and on until a particle is nudged into the consumer, thus powering it. Essentially what is occurring is a long chain of momentum transfer between mobile charge carriers; the Drude model of conduction describes this process more rigorously. This momentum transfer model makes metal an ideal choice for a conductor; metals, characteristically, possess a delocalized sea of electrons which gives the electrons enough mobility to collide and thus affect a momentum transfer.

As discussed above, electrons are the primary mover in metals; however, other devices such as the cationic electrolyte(s) of a battery, or the mobile protons of the proton conductor of a fuel cell rely on positive charge carriers. Insulators are non-conducting materials with few mobile charges that support only insignificant electric currents.
Resistance and conductance
A piece of resistive material with electrical contacts on both ends.
Main article: Electrical resistance and conductance

The resistance of a given conductor depends on the material it is made of, and on its dimensions. For a given material, the resistance is inversely proportional to the cross-sectional area.[1] For example, a thick copper wire has lower resistance than an otherwise-identical thin copper wire. Also, for a given material, the resistance is proportional to the length; for example, a long copper wire has higher resistance than an otherwise-identical short copper wire. The resistance R and conductance G of a conductor of uniform cross section, therefore, can be computed as[1]

    R = ρ ℓ A , G = σ A ℓ . {\displaystyle {\begin{aligned}R&=\rho {\frac {\ell }{A}},\\[6pt]G&=\sigma {\frac {A}{\ell }}.\end{aligned}}}

where ℓ \ell is the length of the conductor, measured in metres [m], A is the cross-section area of the conductor measured in square metres [m2], σ (sigma) is the electrical conductivity measured in siemens per meter (S·m−1), and ρ (rho) is the electrical resistivity (also called specific electrical resistance) of the material, measured in ohm-metres (Ω·m). The resistivity and conductivity are proportionality constants, and therefore depend only on the material the wire is made of, not the geometry of the wire. Resistivity and conductivity are reciprocals: ρ = 1 / σ \rho=1/\sigma. Resistivity is a measure of the material's ability to oppose electric current.

This formula is not exact: It assumes the current density is totally uniform in the conductor, which is not always true in practical situation. However, this formula still provides a good approximation for long thin conductors such as wires.

Another situation this formula is not exact for is with alternating current (AC), because the skin effect inhibits current flow near the center of the conductor. Then, the geometrical cross-section is different from the effective cross-section in which current actually flows, so the resistance is higher than expected. Similarly, if two conductors are near each other carrying AC current, their resistances increase due to the proximity effect. At commercial power frequency, these effects are significant for large conductors carrying large currents, such as busbars in an electrical substation,[2] or large power cables carrying more than a few hundred amperes.

Aside from the geometry of the wire, temperature also has a significant effect on the efficacy of conductors. Temperature affects conductors in two main ways, the first is that materials may expand under the application of heat. The amount that the material will expand is governed by the thermal expansion coefficient specific to the material. Such an expansion (or contraction) will change the geometry of the conductor and therefore its characteristic resistance. However, this effect is generally small, on the order of 10−6. An increase in temperature will also increase the number of phonons generated within the material. A phonon is essentially a lattice vibration, or rather a small, harmonic kinetic movement of the atoms of the material. Much like the shaking of a pinball machine, phonons serve to disrupt the path of electrons, causing them to scatter. This electron scattering will decrease the number of electron collisions and therefore will decrease the total amount of current transferred.
Conductor materials
Main article: Electrical resistivity and conductivity
Further information: Copper conductor and Aluminum building wiring
Material 	ρ [Ω·m] at 20°C 	σ [S/m] at 20°C
Silver, Ag 	1.59 × 10−8 	6.30 × 107
Copper, Cu 	1.68 × 10−8 	5.96 × 107
Aluminum, Al 	2.82 × 10−8 	3.50 × 107

Conduction materials include metals, electrolytes, superconductors, semiconductors, plasmas and some nonmetallic conductors such as graphite and conductive polymers.

Copper has a high conductivity. Annealed copper is the international standard to which all other electrical conductors are compared; the International Annealed Copper Standard conductivity is 58 MS/m, although ultra-pure copper can slightly exceed 101% IACS. The main grade of copper used for electrical applications, such as building wire, motor windings, cables and busbars, is electrolytic-tough pitch (ETP) copper (CW004A or ASTM designation C100140). If high conductivity copper must be welded or brazed or used in a reducing atmosphere, then oxygen-free high conductivity copper (CW008A or ASTM designation C10100) may be used.[3] Because of its ease of connection by soldering or clamping, copper is still the most common choice for most light-gauge wires.

Silver is 6% more conductive than copper, but due to cost it is not practical in most cases. However, it is used in specialized equipment, such as satellites, and as a thin plating to mitigate skin effect losses at high frequencies. Famously, 14,700 short tons (13,300 t) of silver on loan from the United States Treasury were used in the making of the calutron magnets during World War II due to wartime shortages of copper. [4]

Aluminum wire is the most common metal in electric power transmission and distribution. Although only 61% of the conductivity of copper by cross-sectional area, its lower density makes it twice as conductive by mass. As aluminum is roughly one-third the cost of copper by weight, the economic advantages are considerable when large conductors are required.

The disadvantages of aluminum wiring lie in its mechanical and chemical properties. It readily forms an insulating oxide, making connections heat up. Its larger coefficient of thermal expansion than the brass materials used for connectors causes connections to loosen. Aluminum can also "creep", slowly deforming under load, which also loosens connections. These effects can be mitigated with suitably designed connectors and extra care in installation, but they have made aluminum building wiring unpopular past the service drop.

Organic compounds such as octane, which has 8 carbon atoms and 18 hydrogen atoms, cannot conduct electricity. Oils are hydrocarbons, since carbon has the property of tetracovalency and forms covalent bonds with other elements such as hydrogen, since it does not lose or gain electrons, thus does not form ions. Covalent bonds are simply the sharing of electrons. Hence, there is no separation of ions when electricity is passed through it. Liquids made of compounds with only covalent bonds cannot conduct electricity. Certain organic ionic liquids, by contrast, can conduct an electric current.

While pure water is not an electrical conductor, even a small portion of ionic impurities, such as salt, can rapidly transform it into a conductor.
Wire size

Wires are measured by their cross sectional area. In many countries, the size is expressed in square millimetres. In North America, conductors are measured by American wire gauge for smaller ones, and circular mils for larger ones.
Conductor ampacity

The ampacity of a conductor, that is, the amount of current it can carry, is related to its electrical resistance: a lower-resistance conductor can carry a larger value of current. The resistance, in turn, is determined by the material the conductor is made from (as described above) and the conductor's size. For a given material, conductors with a larger cross-sectional area have less resistance than conductors with a smaller cross-sectional area.

For bare conductors, the ultimate limit is the point at which power lost to resistance causes the conductor to melt. Aside from fuses, most conductors in the real world are operated far below this limit, however. For example, household wiring is usually insulated with PVC insulation that is only rated to operate to about 60 °C, therefore, the current in such wires must be limited so that it never heats the copper conductor above 60 °C, causing a risk of fire. Other, more expensive insulation such as Teflon or fiberglass may allow operation at much higher temperatures.
Isotropy

If an electric field is applied to a material, and the resulting induced electric current is in the same direction, the material is said to be an isotropic electrical conductor. If the resulting electric current is in a different direction from the applied electric field, the material is said to be an anisotropic electrical conductor.
See also
Classification of materials based on permittivity εr″/εr′ 	Current conduction 	Field propagation
0 		perfect dielectric
lossless medium
≪ 1 	low-conductivity material
poor conductor 	low-loss medium
good dielectric
≈ 1 	lossy conducting material 	lossy propagation medium
≫ 1 	high-conductivity material
good conductor 	high-loss medium
poor dielectric
∞ 	perfect conductor 	

    Bundle conductor
    Charge transfer complex
    Electrical cable
    Electrical resistivity and conductivity
    Fourth rail
    Overhead line
    Stephen Gray, first to identify electrical conductors and insulators
    Superconductivity
    Third rail

References

"Wire Sizes and Resistance" (PDF). Retrieved 2018-01-14.
Fink and Beaty, Standard Handbook for Electrical Engineers 11th Edition, pages 17–19
"High conductivity coppers (electrical)". Copper Development Association (U.K.). Archived from the original on 2013-07-20. Retrieved 2013-06-01.

    "From Treasury Vault to the Manhattan Project" (PDF). American Scientist. Retrieved 2022-10-27.

Further reading
Pioneering and historical books

    William Henry Preece. On Electrical Conductors. 1883.
    Oliver Heaviside. Electrical Papers. Macmillan, 1894.

Reference books

    Annual Book of ASTM Standards: Electrical Conductors. American Society for Testing and Materials. (every year)
    IET Wiring Regulations. Institution for Engineering and Technology. wiringregulations.net

External links

    BBC: Key Stage 2 Bitesize: Electrical Conductors
    The discovery of conductors and insulators by Gray, Dufay and Franklin.

Wikimedia Commons has media related to Electrical conductors.
Authority control: National Edit this at Wikidata	

    France BnF data Germany Israel United States Japan

Categories:

    ElectricityElectrical conductors

    This page was last edited on 28 August 2023, at 00:10 (UTC).
  site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
